[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science With Python",
    "section": "",
    "text": "Preface\nWelcome to my DataCamp Data Science Portfolio repository!\nThis repository serves as a showcase of my journey through various DataCamp courses in the field of data science. Here, you’ll find a collection of projects, practice notebooks, and code snippets that I’ve completed and worked on during my learning process.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Data Science With Python",
    "section": "Contents",
    "text": "Contents\n\nPractice Notebooks:\nHere, you’ll find notebooks where I’ve practiced various data science concepts, techniques, and algorithms taught in DataCamp courses. Feel free to explore and learn alongside me!\n\nIntroduction to Python\nIntermediate Python\nData Manipulation with pandas\nJoining Data with pandas\nIntroduction to Statistics in Python\nIntroduction to Data Visualization with Matplotlib\nIntroduction to Data Visualization with Seaborn\nPython Data Science Toolbox (Part 1)\nPython Data Science Toolbox (Part 2)\nIntermediate Data Visualization with Seaborn\n[Exploratory Data Analysis in Python]\n[Working with Categorical Data in Python]\n[Data Communication Concepts]\n[Introduction to Importing Data in Python]\n[Cleaning Data in Python]\n[Working with Dates and Times in Python]\n[Writing Functions in Python]\n[Introduction to Regression with statsmodels in Python]\n[Sampling in Python]\n[Hypothesis Testing in Python]\n[Supervised Learning with scikit-learn]\n[Unsupervised Learning in Python]\n[Machine Learning with Tree-Based Models in Python]\n[Intermediate Importing Data in Python]\n[Preprocessing for Machine Learning in Python]\n[Developing Python Packages]\n[Machine Learning for Business]\n[Introduction to SQL]\n[Intermediate SQL]\n[Joining Data in SQL]\n[Introduction to Git]\n\n\n\nCourse Projects:\nThis section contains projects completed as part of DataCamp courses. Each project includes a detailed analysis, code implementation, and insights gained from the data.\n\nInvestigating Netflix Movies\nA Visual History of Nobel Prize Winners\nProject-Exploring NYC Public School Test Result Scores\nHypothesis Testing in Healthcare\nPredictive Modeling for Agriculture\nAnalyzing Crime in Los Angeles\nCustomer Analytics- Preparing Data for Modeling",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html",
    "href": "1_Introduction_to_Python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Explore Datasets\nUse the arrays imported in the first cell to explore the data and practice your skills! - Print out the weight of the first ten baseball players. - What is the median weight of all baseball players in the data? - Print out the names of all players with a height greater than 80 (heights are in inches). - Who is taller on average? Baseball players or soccer players? Keep in mind that baseball heights are stored in inches! - The values in soccer_shooting are decimals. Convert them to whole numbers (e.g., 0.98 becomes 98). - Do taller players get higher ratings? Calculate the correlation between soccer_ratings and soccer_heights to find out! - What is the average rating for attacking players ('A')?\n# Create a variable savings\n\nsavings=100\n# Print out savings\nprint(savings)\n\n100\n# Create the variables monthly_savings and num_months\n\nmonthly_savings = 10\nnum_months = 4\n\n# Multiply monthly_savings and num_months\nnew_savings = monthly_savings * num_months\n\n# Add new_savings to your savings\ntotal_savings = savings+ new_savings \n\n# Print total_savings\nprint(total_savings)\n\n140\n# Create a variable half\nhalf = 0.5\n\n# Create a variable intro\nintro = \"Hello! How are you?\"\n\n# Create a variable is_good\nis_good = True\nmonthly_savings = 10\nnum_months = 12\nintro = \"Hello! How are you?\"\n\n# Calculate year_savings using monthly_savings and num_months\nyear_savings = monthly_savings * num_months\n\n# Print the type of year_savings\nprint(type(year_savings))\n\n# Assign sum of intro and intro to doubleintro\ndoubleintro = intro + intro\n\n\n# Print out doubleintro\nprint(doubleintro)\n\n&lt;class 'int'&gt;\nHello! How are you?Hello! How are you?\n# Definition of savings and total_savings\nsavings = 100\ntotal_savings = 150\n\n# Fix the printout\nprint(\"I started with $\" + str(savings) + \" and now have $\" + str(total_savings) + \". Awesome!\")\n\n# Definition of pi_string\npi_string = \"3.1415926\"\n\n# Convert pi_string into float: pi_float\npi_float = float(pi_string)\n\nI started with $100 and now have $150. Awesome!",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#create-a-list",
    "href": "1_Introduction_to_Python.html#create-a-list",
    "title": "Introduction to Python",
    "section": "Create a list",
    "text": "Create a list\nAs opposed to int, bool etc., a list is a compound data type; you can group values together:\na = \"is\" b = \"nice\" my_list = [\"my\", \"list\", a, b]\nAfter measuring the height of your family, you decide to collect some information on the house you’re living in. The areas of the different parts of your house are stored in separate variables for now, as shown in the script.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a list, areas, that contains the area of the hallway (hall), kitchen (kit), living room (liv), bedroom (bed) and bathroom (bath), in this order. Use the predefined variables. - Print areas with the print() function.\n\n\n\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# Create list areas\nareas = [hall, kit, liv, bed, bath]\n\n# Print areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#create-list-with-different-types",
    "href": "1_Introduction_to_Python.html#create-list-with-different-types",
    "title": "Introduction to Python",
    "section": "Create list with different types",
    "text": "Create list with different types\nA list can contain any Python type. Although it’s not really common, a list can also contain a mix of Python types including strings, floats, booleans, etc.\nThe printout of the previous exercise wasn’t really satisfying. It’s just a list of numbers representing the areas, but you can’t tell which area corresponds to which part of your house.\nThe code in the editor is the start of a solution. For some of the areas, the name of the corresponding room is already placed in front. Pay attention here! “bathroom” is a string, while bath is a variable that represents the float 9.50 you specified earlier.\n\n\n\n\n\n\nInstructions\n\n\n\n-Finish the code that creates the areas list. Build the list so that the list first contains the name of each room as a string and then its area. In other words, add the strings “hallway”, “kitchen” and “bedroom” at the appropriate locations. - Print areas again; is the printout more informative this time?\n\n\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# Adapt list areas\nareas = [\"hallway\", hall, \"kitchen\", kit, \"living room\", liv, \"bedroom\",bed, \"bathroom\", bath]\n\n# Print areas\nprint(areas)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0, 'bedroom', 10.75, 'bathroom', 9.5]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#list-of-lists",
    "href": "1_Introduction_to_Python.html#list-of-lists",
    "title": "Introduction to Python",
    "section": "List of lists",
    "text": "List of lists\nAs a data scientist, you’ll often be dealing with a lot of data, and it will make sense to group some of this data.\nInstead of creating a flat list containing strings and floats, representing the names and areas of the rooms in your house, you can create a list of lists. The script in the editor can already give you an idea.\nDon’t get confused here: “hallway” is a string, while hall is a variable that represents the float 11.25 you specified earlier.\n\n\n\n\n\n\nInstructions\n\n\n\n\nFinish the list of lists so that it also contains the bedroom and bathroom data. Make sure you enter these in order! - Print out house; does this way of structuring your data make more sense? - Print out the type of house. Are you still dealing with a list?\n\n\n\n\n# area variables (in square meters)\nhall = 11.25\nkit = 18.0\nliv = 20.0\nbed = 10.75\nbath = 9.50\n\n# house information as list of lists\nhouse = [[\"hallway\", hall],\n         [\"kitchen\", kit],\n         [\"living room\", liv],\n         [\"bedroom\", bed],\n         [\"bathroom\",bath]\n         ]\n\n# Print out house\nprint(house)\n\n# Print out the type of house\nprint(type(house))\n\n[['hallway', 11.25], ['kitchen', 18.0], ['living room', 20.0], ['bedroom', 10.75], ['bathroom', 9.5]]\n&lt;class 'list'&gt;",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#subset-and-conquer",
    "href": "1_Introduction_to_Python.html#subset-and-conquer",
    "title": "Introduction to Python",
    "section": "Subset and conquer",
    "text": "Subset and conquer\nSubsetting Python lists is a piece of cake. Take the code sample below, which creates a list x and then selects “b” from it. Remember that this is the second element, so it has index 1. You can also use negative indexing.\nx = [\"a\", \"b\", \"c\", \"d\"] x[1] x[-3] # same result!\nRemember the areas list from before, containing both strings and floats? Its definition is already in the script. Can you add the correct code to do some Python subsetting?\n\n\n\n\n\n\nInstructions\n\n\n\n\nPrint out the second element from the areas list (it has the value 11.25). - Subset and print out the last element of areas, being 9.50. Using a negative index makes sense here! - Select the number representing the area of the living room (20.0) and print it out.\n\n\n\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Print out second element from areas\nprint(areas[1])\n\n# Print out last element from areas\nprint(areas[-1])\n\n# Print out the area of the living room\nprint(areas[5])\n\n11.25\n9.5\n20.0",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#subset-and-calculate",
    "href": "1_Introduction_to_Python.html#subset-and-calculate",
    "title": "Introduction to Python",
    "section": "Subset and calculate",
    "text": "Subset and calculate\nAfter you’ve extracted values from a list, you can use them to perform additional calculations. Take this example, where the second and fourth element of a list x are extracted. The strings that result are pasted together using the + operator:\nx = [\"a\", \"b\", \"c\", \"d\"] print(x[1] + x[3])\n\n\n\n\n\n\nInstructions\n\n\n\n\nUsing a combination of list subsetting and variable assignment, create a new variable, eat_sleep_area, that contains the sum of the area of the kitchen and the area of the bedroom. - Print the new variable eat_sleep_area.\n\n\n\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Sum of kitchen and bedroom area: eat_sleep_area\neat_sleep_area = areas[3] + areas[7]\n\n# Print the variable eat_sleep_area\nprint(eat_sleep_area)\n\n28.75",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#slicing-and-dicing",
    "href": "1_Introduction_to_Python.html#slicing-and-dicing",
    "title": "Introduction to Python",
    "section": "Slicing and dicing",
    "text": "Slicing and dicing\nSelecting single values from a list is just one part of the story. It’s also possible to slice your list, which means selecting multiple elements from your list. Use the following syntax:\nmy_list[start:end]\nThe start index will be included, while the end index is not.\nThe code sample below shows an example. A list with “b” and “c”, corresponding to indexes 1 and 2, are selected from a list x:\n`x = [“a”, “b”, “c”, “d”] x[1:3]’\nThe elements with index 1 and 2 are included, while the element with index 3 is not.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse slicing to create a list, downstairs, that contains the first 6 elements of areas. - Do a similar thing to create a new variable, upstairs, that contains the last 4 elements of areas. - Print both downstairs and upstairs using print().\n\n\n\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Use slicing to create downstairs\n\ndownstairs = areas[0:6]\n# Use slicing to create upstairs\nupstairs = areas[6:10]\n\n# Print out downstairs and upstairs\nprint(downstairs)\nprint(upstairs)\n\n['hallway', 11.25, 'kitchen', 18.0, 'living room', 20.0]\n['bedroom', 10.75, 'bathroom', 9.5]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#slicing-and-dicing-2",
    "href": "1_Introduction_to_Python.html#slicing-and-dicing-2",
    "title": "Introduction to Python",
    "section": "Slicing and dicing (2)",
    "text": "Slicing and dicing (2)\nIn the video, Hugo first discussed the syntax where you specify both where to begin and end the slice of your list:\nmy_list[begin:end]\nHowever, it’s also possible not to specify these indexes. If you don’t specify the begin index, Python figures out that you want to start your slice at the beginning of your list. If you don’t specify the end index, the slice will go all the way to the last element of your list. To experiment with this, try the following commands in the IPython Shell:\nx = [\"a\", \"b\", \"c\", \"d\"] x[:2] x[2:] x[:]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#subsetting-lists-of-lists",
    "href": "1_Introduction_to_Python.html#subsetting-lists-of-lists",
    "title": "Introduction to Python",
    "section": "Subsetting lists of lists",
    "text": "Subsetting lists of lists\nYou saw before that a Python list can contain practically anything; even other lists! To subset lists of lists, you can use the same technique as before: square brackets. Try out the commands in the following code sample in the IPython Shell:\nx = [[\"a\", \"b\", \"c\"],      [\"d\", \"e\", \"f\"],      [\"g\", \"h\", \"i\"]] x[2][0] x[2][:2]\nx[2] results in a list, that you can subset again by adding additional square brackets. What will house[-1][1] return?\n\nhouse[-1][1]\n\n9.5",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#replace-list-elements",
    "href": "1_Introduction_to_Python.html#replace-list-elements",
    "title": "Introduction to Python",
    "section": "Replace list elements",
    "text": "Replace list elements\nReplacing list elements is pretty easy. Simply subset the list and assign new values to the subset. You can select single elements or you can change entire list slices at once.\nUse the IPython Shell to experiment with the commands below. Can you tell what’s happening and why?\nx = [\"a\", \"b\", \"c\", \"d\"] x[1] = \"r\" x[2:] = [\"s\", \"t\"]\nFor this and the following exercises, you’ll continue working on the areas list that contains the names and areas of different rooms in a house.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUpdate the area of the bathroom area to be 10.50 square meters instead of 9.50. - Make the areas list more trendy! Change “living room” to “chill zone”.\n\n\n\n\n# Create the areas list\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"living room\", 20.0, \"bedroom\", 10.75, \"bathroom\", 9.50]\n\n# Correct the bathroom area\nareas[-1] = 10.50\n\n# Change \"living room\" to \"chill zone\"\nareas[4] = \"chill zone\"",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#extend-a-list",
    "href": "1_Introduction_to_Python.html#extend-a-list",
    "title": "Introduction to Python",
    "section": "Extend a list",
    "text": "Extend a list\nIf you can change elements in a list, you sure want to be able to add elements to it, right? You can use the + operator:\nx = [\"a\", \"b\", \"c\", \"d\"] y = x + [\"e\", \"f\"]\nYou just won the lottery, awesome! You decide to build a poolhouse and a garage. Can you add the information to the areas list?\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the + operator to paste the list [“poolhouse”, 24.5] to the end of the areas list. Store the resulting list as areas_1. - Further extend areas_1 by adding data on your garage. Add the string “garage” and float 15.45. Name the resulting list areas_2.\n\n\n\n\n# Create the areas list and make some changes\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0, \"chill zone\", 20.0,\n         \"bedroom\", 10.75, \"bathroom\", 10.50]\n\n# Add poolhouse data to areas, new list is areas_1\nareas_1 = areas + [\"poolhouse\", 24.5]\n\n# Add garage data to areas_1, new list is areas_2\nareas_2 = areas_1 + [\"garage\", 15.45]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#delete-list-elements",
    "href": "1_Introduction_to_Python.html#delete-list-elements",
    "title": "Introduction to Python",
    "section": "Delete list elements",
    "text": "Delete list elements\nFinally, you can also remove elements from your list. You can do this with the del statement:\nx = [\"a\", \"b\", \"c\", \"d\"] del(x[1])\nPay attention here: as soon as you remove an element from a list, the indexes of the elements that come after the deleted element all change!\nThe updated and extended version of areas that you’ve built in the previous exercises is coded below. You can copy and paste this into the IPython Shell to play around with the result.\nareas = [\"hallway\", 11.25, \"kitchen\", 18.0,         \"chill zone\", 20.0, \"bedroom\", 10.75,          \"bathroom\", 10.50, \"poolhouse\", 24.5,          \"garage\", 15.45]\nThere was a mistake! The amount you won with the lottery is not that big after all and it looks like the poolhouse isn’t going to happen. You decide to remove the corresponding string and float from the areas list.\nThe ; sign is used to place commands on the same line. The following two code chunks are equivalent:\n` Same line command1; command2\nSeparate lines command1 command2`\nWhich of the code chunks will do the job for us? del(areas[-4:-2])",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#inner-workings-of-lists",
    "href": "1_Introduction_to_Python.html#inner-workings-of-lists",
    "title": "Introduction to Python",
    "section": "Inner workings of lists",
    "text": "Inner workings of lists\nAt the end of the video, Hugo explained how Python lists work behind the scenes. In this exercise you’ll get some hands-on experience with this.\nThe Python code in the script already creates a list with the name areas and a copy named areas_copy. Next, the first element in the areas_copy list is changed and the areas list is printed out. If you hit Run Code you’ll see that, although you’ve changed areas_copy, the change also takes effect in the areas list. That’s because areas and areas_copy point to the same list.\nIf you want to prevent changes in areas_copy from also taking effect in areas, you’ll have to do a more explicit copy of the areas list. You can do this with list() or by using [:].\n\n\n\n\n\n\nInstructions\n\n\n\n\nChange the second command, that creates the variable areas_copy, such that areas_copy is an explicit copy of areas. After your edit, changes made to areas_copy shouldn’t affect areas. Submit the answer to check this.\n\n\n\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Create areas_copy\nareas_copy = list(areas)\n\n# Change areas_copy\nareas_copy[0] = 5.0\n\n# Print areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#familiar-functions",
    "href": "1_Introduction_to_Python.html#familiar-functions",
    "title": "Introduction to Python",
    "section": "Familiar functions",
    "text": "Familiar functions\nOut of the box, Python offers a bunch of built-in functions to make your life as a data scientist easier. You already know two such functions: print() and type(). You’ve also used the functions str(), int(), bool() and float() to switch between data types. These are built-in functions as well.\nCalling a function is easy. To get the type of 3.0 and store the output as a new variable, result, you can use the following:\nresult = type(3.0)\nThe general recipe for calling functions and saving the result to a variable is thus:\noutput = function_name(input)",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#multiple-arguments",
    "href": "1_Introduction_to_Python.html#multiple-arguments",
    "title": "Introduction to Python",
    "section": "Multiple arguments",
    "text": "Multiple arguments\nIn the previous exercise, you identified optional arguments by viewing the documentation with help(). You’ll now apply this to change the behavior of the sorted() function.\nHave a look at the documentation of sorted() by typing help(sorted) in the IPython Shell.\nYou’ll see that sorted() takes three arguments: iterable, key, and reverse.\nkey=None means that if you don’t specify the key argument, it will be None. reverse=False means that if you don’t specify the reverse argument, it will be False, by default.\nIn this exercise, you’ll only have to specify iterable and reverse, not key. The first input you pass to sorted() will be matched to the iterable argument, but what about the second input? To tell Python you want to specify reverse without changing anything about key, you can use = to assign it a new value:\nsorted(____, reverse=____)\nTwo lists have been created for you. Can you paste them together and sort them in descending order?\nNote: For now, we can understand an iterable as being any collection of objects, e.g., a List.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse + to merge the contents of first and second into a new list: full. - Call sorted() on full and specify the reverse argument to be True. Save the sorted list as full_sorted. - Finish off by printing out full_sorted.\n\n\n\n\n# Create lists first and second\nfirst = [11.25, 18.0, 20.0]\nsecond = [10.75, 9.50]\n\n# Paste together first and second: full\nfull = first + second\n\n# Sort full in descending order: full_sorted\nfull_sorted = sorted(full,reverse=True)\n\n# Print out full_sorted\nprint(full_sorted)\n\n[20.0, 18.0, 11.25, 10.75, 9.5]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#string-methods",
    "href": "1_Introduction_to_Python.html#string-methods",
    "title": "Introduction to Python",
    "section": "String Methods",
    "text": "String Methods\nStrings come with a bunch of methods. Follow the instructions closely to discover some of them. If you want to discover them in more detail, you can always type help(str) in the IPython Shell.\nA string place has already been created for you to experiment with.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the upper() method on place and store the result in place_up. Use the syntax for calling methods that you learned in the previous video. - Print out place and place_up. Did both change? - Print out the number of o’s on the variable place by calling count() on place and passing the letter ‘o’ as an input to the method. We’re talking about the variable place, not the word “place”!\n\n\n\n\n# string to experiment with: place\nplace = \"poolhouse\"\n\n# Use upper() on place: place_up\nplace_up = place.upper()\n\n# Print out place and place_up\n\nprint(place)\nprint(place_up)\n# Print out the number of o's in place\nprint(place.count(\"o\"))\n\npoolhouse\nPOOLHOUSE\n3",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#list-methods",
    "href": "1_Introduction_to_Python.html#list-methods",
    "title": "Introduction to Python",
    "section": "List Methods",
    "text": "List Methods\nStrings are not the only Python types that have methods associated with them. Lists, floats, integers and booleans are also types that come packaged with a bunch of useful methods. In this exercise, you’ll be experimenting with:\n\nindex(), to get the index of the first element of a list that matches its input and\ncount(), to get the number of times an element appears in a list.\n\nYou’ll be working on the list with the area of different parts of a house: areas. Instructions - Use the index() method to get the index of the element in areas that is equal to 20.0. Print out this index. - Call count() on areas to find out how many times 9.50 appears in the list. Again, simply print out this number.\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Print out the index of the element 20.0\n\nprint(areas.index(20.0))\n# Print out how often 9.50 appears in areas\n\nprint(areas.count(9.50))\n\n2\n1",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#list-methods-2",
    "href": "1_Introduction_to_Python.html#list-methods-2",
    "title": "Introduction to Python",
    "section": "List Methods (2)",
    "text": "List Methods (2)\nMost list methods will change the list they’re called on. Examples are:\n\nappend(), that adds an element to the list it is called on,\nremove(), that removes the first element of a list that matches the input, and\nreverse(), that reverses the order of the elements in the list it is called on.\n\nYou’ll be working on the list with the area of different parts of the house: areas.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse append() twice to add the size of the poolhouse and the garage again: 24.5 and 15.45, respectively. Make sure to add them in this order. - Print out areas - Use the reverse() method to reverse the order of the elements in areas. - Print out areas once more.\n\n\n\n\n# Create list areas\nareas = [11.25, 18.0, 20.0, 10.75, 9.50]\n\n# Use append twice to add poolhouse and garage size\n\nareas.append(24.5)\nareas.append(15.45)\n\n# Print out areas\nprint(areas)\n\n# Reverse the orders of the elements in areas\n\nareas = areas.reverse()\n# Print out areas\nprint(areas)\n\n[11.25, 18.0, 20.0, 10.75, 9.5, 24.5, 15.45]\nNone",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#import-package",
    "href": "1_Introduction_to_Python.html#import-package",
    "title": "Introduction to Python",
    "section": "Import package",
    "text": "Import package\nAs a data scientist, some notions of geometry never hurt. Let’s refresh some of the basics.\nFor a fancy clustering algorithm, you want to find the circumference, , and area, , of a circle. When the radius of the circle is r, you can calculate and\nas: C = 2pir _A = pir^2\nIn Python, the symbol for exponentiation is . This operator raises the number to its left to the power of the number to its right. For example 34 is 3 to the power of 4 and will give 81.\nTo use the constant pi, you’ll need the math package. A variable r is already coded in the script. Fill in the code to calculate C and A and see how the print() functions create some nice printouts.\n\n\n\n\n\n\nInstructions\n\n\n\n\nImport the math package. Now you can access the constant pi with math.pi. - Calculate the circumference of the circle and store it in C. - Calculate the area of the circle and store it in A.\n\n\n\n\n# Import the math package\nimport math\n\n# Definition of radius\nr = 0.43\n\n# Calculate C\nC = 2*math.pi*r\n\n# Calculate A\nA = math.pi * r*r\n\n# Build printout\nprint(\"Circumference: \" + str(C))\nprint(\"Area: \" + str(A))\n\nCircumference: 2.701769682087222\nArea: 0.5808804816487527",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#selective-import",
    "href": "1_Introduction_to_Python.html#selective-import",
    "title": "Introduction to Python",
    "section": "Selective import",
    "text": "Selective import\nGeneral imports, like import math, make all functionality from the math package available to you. However, if you decide to only use a specific part of a package, you can always make your import more selective:\nfrom math import pi\nLet’s say the Moon’s orbit around planet Earth is a perfect circle, with a radius r (in km) that is defined in the script.\n\n\n\n\n\n\nInstructions\n\n\n\n\nPerform a selective import from the math package where you only import the radians function. - Calculate the distance travelled by the Moon over 12 degrees of its orbit. Assign the result to dist. You can calculate this as r * phi, where r is the radius and phi is the angle in radians. To convert an angle in degrees to an angle in radians, use the radians() function, which you just imported. - Print out dist.\n\n\n\n\n# Import radians function of math package\nfrom math import radians\n\n# Definition of radius\nr = 192500\n\n# Travel distance of Moon over 12 degrees. Store in dist.\ndist = r*radians(12)\n\n# Print out dist\nprint(dist)\n\n40317.10572106901",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#your-first-numpy-array",
    "href": "1_Introduction_to_Python.html#your-first-numpy-array",
    "title": "Introduction to Python",
    "section": "Your First NumPy Array",
    "text": "Your First NumPy Array\nIn this chapter, we’re going to dive into the world of baseball. Along the way, you’ll get comfortable with the basics of numpy, a powerful package to do data science.\nA list baseball has already been defined in the Python script, representing the height of some baseball players in centimeters. Can you add some code here and there to create a numpy array from it?\n\n\n\n\n\n\nInstructions\n\n\n\n\nImport the numpy package as np, so that you can refer to numpy with np. - Use np.array() to create a numpy array from baseball. Name this array np_baseball. - Print out the type of np_baseball to check that you got it right.\n\n\n\n\n# Import the numpy package as np\nimport numpy as np\n\n# Create list baseball\nbaseball = [180, 215, 210, 210, 188, 176, 209, 200]\n\n# Create a numpy array from baseball: np_baseball\n\nnp_baseball = np.array(baseball)\n# Print out type of np_baseball\nprint(type(np_baseball))\n\n&lt;class 'numpy.ndarray'&gt;",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#baseball-players-height",
    "href": "1_Introduction_to_Python.html#baseball-players-height",
    "title": "Introduction to Python",
    "section": "Baseball players’ height",
    "text": "Baseball players’ height\nYou are a huge baseball fan. You decide to call the MLB (Major League Baseball) and ask around for some more statistics on the height of the main players. They pass along data on more than a thousand players, which is stored as a regular Python list: height_in. The height is expressed in inches. Can you make a numpy array out of it and convert the units to meters?\nheight_in is already available and the numpy package is loaded, so you can start straight away (Source: stat.ucla.edu).\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a numpy array from height_in. Name this new array np_height_in. - Print np_height_in.\n\nMultiply np_height_in with 0.0254 to convert all height measurements from inches to meters. Store the new values in a new array, np_height_m. - Print out np_height_m and check if the output makes sense.\n\n\n\n\nheight_in = baseball_heights.astype(int)\nheight_in[0:10]\nweight_lb = baseball_weights.astype(int)\nweight_lb\n\narray([180, 215, 210, ..., 205, 190, 195])\n\n\n\n# Import numpy\nimport numpy as np\n\n# Create a numpy array from height_in: np_height_in\nnp_height_in = np.array(height_in)\n\n# Print out np_height_in\nprint(np_height_in)\n\n# Convert np_height_in to m: np_height_m\n\nnp_height_m = 0.0254 * np_height_in\n# Print np_height_m\nprint(np_height_m)\n\n[74 74 72 ... 75 75 73]\n[1.8796 1.8796 1.8288 ... 1.905  1.905  1.8542]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#baseball-players-bmi",
    "href": "1_Introduction_to_Python.html#baseball-players-bmi",
    "title": "Introduction to Python",
    "section": "Baseball player’s BMI",
    "text": "Baseball player’s BMI\nThe MLB also offers to let you analyze their weight data. Again, both are available as regular Python lists: height_in and weight_lb. height_in is in inches and weight_lb is in pounds.\nIt’s now possible to calculate the BMI of each baseball player. Python code to convert height_in to a numpy array with the correct units is already available in the workspace. Follow the instructions step by step and finish the game! height_in and weight_lb are available as regular lists.\n\n\n\n\n\n\nNote\n\n\n\n# Instructions - Create a numpy array from the weight_lb list with the correct units. Multiply by 0.453592 to go from pounds to kilograms. Store the resulting numpy array as np_weight_kg. - Use np_height_m and np_weight_kg to calculate the BMI of each player. Use the following equation: - - BMI = weight(kg)/height(m)^2 - Save the resulting numpy array as bmi. - Print out bmi.\n\n\n\n# Import numpy\nimport numpy as np\n\n# Create array from height_in with metric units: np_height_m\nnp_height_m = np.array(height_in) * 0.0254\n\n# Create array from weight_lb with metric units: np_weight_kg\nnp_weight_kg = 0.453592 * np.array(weight_lb)\n\n# Calculate the BMI: bmi\nbmi = np_weight_kg/np_height_m**2\n\n# Print out bmi\nprint(bmi)\n\n[23.11037639 27.60406069 28.48080465 ... 25.62295933 23.74810865\n 25.72686361]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#lightweight-baseball-players",
    "href": "1_Introduction_to_Python.html#lightweight-baseball-players",
    "title": "Introduction to Python",
    "section": "Lightweight baseball players",
    "text": "Lightweight baseball players\nTo subset both regular Python lists and numpy arrays, you can use square brackets:\nx = [4 , 9 , 6, 3, 1] x[1] import numpy as np y = np.array(x) y[1]\nFor numpy specifically, you can also use boolean numpy arrays:\nhigh = y &gt; 5 y[high]\nThe code that calculates the BMI of all baseball players is already included. Follow the instructions and reveal interesting things from the data! height_in and weight_lb are available as regular lists.\n\n\n\n\n\n\nNote\n\n\n\n# Instructions - Create a boolean numpy array: the element of the array should be True if the corresponding baseball player’s BMI is below 21. You can use the &lt; operator for this. Name the array light. - Print the array light. - Print out a numpy array with the BMIs of all baseball players whose BMI is below 21. Use light inside square brackets to do a selection on the bmi array.\n\n\n\n# Import numpy\nimport numpy as np\n\n# Calculate the BMI: bmi\nnp_height_m = np.array(height_in) * 0.0254\nnp_weight_kg = np.array(weight_lb) * 0.453592\nbmi = np_weight_kg / np_height_m ** 2\n\n# Create the light array\nlight = bmi&lt;21\n\n\n# Print out light\nprint(light)\n\n# Print out BMIs of all baseball players whose BMI is below 21\nprint(bmi[light])\n\n[False False False ... False False False]\n[20.54255679 20.54255679 20.69282047 20.69282047 20.34343189 20.34343189\n 20.69282047 20.15883472 19.4984471  20.69282047 20.9205219 ]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#numpy-side-effects",
    "href": "1_Introduction_to_Python.html#numpy-side-effects",
    "title": "Introduction to Python",
    "section": "NumPy Side Effects",
    "text": "NumPy Side Effects\nAs Hugo explained before, numpy is great for doing vector arithmetic. If you compare its functionality with regular Python lists, however, some things have changed.\nFirst of all, numpy arrays cannot contain elements with different types. If you try to build such a list, some of the elements’ types are changed to end up with a homogeneous list. This is known as type coercion.\nSecond, the typical arithmetic operators, such as +, -, * and / have a different meaning for regular Python lists and numpy arrays.\nHave a look at this line of code:\nnp.array([True, 1, 2]) + np.array([3, 4, False])\n\nnp.array([True, 1, 2]) + np.array([3, 4, False])\n\narray([4, 5, 2])",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#subsetting-numpy-arrays",
    "href": "1_Introduction_to_Python.html#subsetting-numpy-arrays",
    "title": "Introduction to Python",
    "section": "Subsetting NumPy Arrays",
    "text": "Subsetting NumPy Arrays\nYou’ve seen it with your own eyes: Python lists and numpy arrays sometimes behave differently. Luckily, there are still certainties in this world. For example, subsetting (using the square bracket notation on lists or arrays) works exactly the same. To see this for yourself, try the following lines of code in the IPython Shell: ` x = [“a”, “b”, “c”] x[1]\nnp_x = np.array(x) np_x[1] ` The script in the editor already contains code that imports numpy as np, and stores both the height and weight of the MLB players as numpy arrays. height_in and weight_lb are available as regular lists.\n\n\n\n\n\n\nInstructions\n\n\n\n\nSubset np_weight_lb by printing out the element at index 50. - Print out a sub-array of np_height_in that contains the elements at index 100 up to and including index 110.\n\n\n\n\n# Import numpy\nimport numpy as np\n\n# Store weight and height lists as numpy arrays\nnp_weight_lb = np.array(weight_lb)\nnp_height_in = np.array(height_in)\n\n# Print out the weight at index 50\n\nprint(np_weight_lb[50])\n# Print out sub-array of np_height_in: index 100 up to and including index 110\nprint(np_height_in[100:111])\n\n200\n[73 74 72 73 69 72 73 75 75 73 72]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#your-first-2d-numpy-array",
    "href": "1_Introduction_to_Python.html#your-first-2d-numpy-array",
    "title": "Introduction to Python",
    "section": "Your First 2D NumPy Array",
    "text": "Your First 2D NumPy Array\nBefore working on the actual MLB data, let’s try to create a 2D numpy array from a small list of lists.\nIn this exercise, baseball is a list of lists. The main list contains 4 elements. Each of these elements is a list containing the height and the weight of 4 baseball players, in this order. baseball is already coded for you in the script.\n\n\n\n\n\n\nNote\n\n\n\n# Instructions - Use np.array() to create a 2D numpy array from baseball. Name it np_baseball. - Print out the type of np_baseball. - Print out the shape attribute of np_baseball. Use np_baseball.shape.\n\n\n\n# Import numpy\nimport numpy as np\n\n# Create baseball, a list of lists\nbaseball = [[180, 78.4],\n            [215, 102.7],\n            [210, 98.5],\n            [188, 75.2]]\n\n# Create a 2D numpy array from baseball: np_baseball\n\nnp_baseball = np.array(baseball)\n# Print out the type of np_baseball\nprint(type(np_baseball))\n\n# Print out the shape of np_baseball\nprint(np_baseball.shape )\n\n&lt;class 'numpy.ndarray'&gt;\n(4, 2)",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#baseball-data-in-2d-form",
    "href": "1_Introduction_to_Python.html#baseball-data-in-2d-form",
    "title": "Introduction to Python",
    "section": "Baseball data in 2D form",
    "text": "Baseball data in 2D form\nYou have another look at the MLB data and realize that it makes more sense to restructure all this information in a 2D numpy array. This array should have 1015 rows, corresponding to the 1015 baseball players you have information on, and 2 columns (for height and weight).\nThe MLB was, again, very helpful and passed you the data in a different structure, a Python list of lists. In this list of lists, each sublist represents the height and weight of a single baseball player. The name of this embedded list is baseball.\nCan you store the data as a 2D array to unlock numpy’s extra functionality? baseball is available as a regular list of lists.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse np.array() to create a 2D numpy array from baseball. Name it np_baseball. - Print out the shape attribute of np_baseball.\n\n\n\n\n# Import numpy package\nimport numpy as np\n\n# Create a 2D numpy array from baseball: np_baseball\nnp_baseball = np.array(baseball)\n\n# Print out the shape of np_baseball\nprint(np_baseball.shape)\n\n(4, 2)",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#subsetting-2d-numpy-arrays",
    "href": "1_Introduction_to_Python.html#subsetting-2d-numpy-arrays",
    "title": "Introduction to Python",
    "section": "Subsetting 2D NumPy Arrays",
    "text": "Subsetting 2D NumPy Arrays\nIf your 2D numpy array has a regular structure, i.e. each row and column has a fixed number of values, complicated ways of subsetting become very easy. Have a look at the code below where the elements “a” and “c” are extracted from a list of lists. `regular list of lists\nx = [[“a”, “b”], [“c”, “d”]]\n[x[0][0], x[1][0]]\nnumpy import numpy as np\nnp_x = np.array(x)\nnp_x[:, 0] ` For regular Python lists, this is a real pain. For 2D numpy arrays, however, it’s pretty intuitive! The indexes before the comma refer to the rows, while those after the comma refer to the columns. The : is for slicing; in this example, it tells Python to include all rows.\nThe code that converts the pre-loaded baseball list to a 2D numpy array is already in the script. The first column contains the players’ height in inches and the second column holds player weight, in pounds. Add some lines to make the correct selections. Remember that in Python, the first element is at index 0! baseball is available as a regular list of lists.\n\n\n\n\n\n\nInstructions\n\n\n\n\nPrint out the 50th row of np_baseball. - Make a new variable, np_weight_lb, containing the entire second column of np_baseball. - Select the height (first column) of the 124th baseball player in np_baseball and print it out.\n\n\n\n\n\n277\n\n\n\n# Import numpy package\nimport numpy as np\n\n# Create np_baseball (2 cols)\nnp_baseball = np.array(baseball)\n\n# Print out the 50th row of np_baseball\nprint(np_baseball[49,:])\n\n# Select the entire second column of np_baseball: np_weight_lb\nnp_weight_lb = np_baseball[:,1]\n\n# Print out height of 124th player\nprint(np_baseball[123,0])\n\n[ 70 195]\n75",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#d-arithmetic",
    "href": "1_Introduction_to_Python.html#d-arithmetic",
    "title": "Introduction to Python",
    "section": "2D Arithmetic",
    "text": "2D Arithmetic\nRemember how you calculated the Body Mass Index for all baseball players? numpy was able to perform all calculations element-wise (i.e. element by element). For 2D numpy arrays this isn’t any different! You can combine matrices with single numbers, with vectors, and with other matrices.\nExecute the code below in the IPython shell and see if you understand: import numpy as np np_mat = np.array([[1, 2],                    [3, 4],                    [5, 6]]) np_mat * 2 np_mat + np.array([10, 10]) np_mat + np_mat np_baseball is coded for you; it’s again a 2D numpy array with 3 columns representing height (in inches), weight (in pounds) and age (in years). baseball is available as a regular list of lists and updated is available as 2D numpy array. Instructions - You managed to get hold of the changes in height, weight and age of all baseball players. It is available as a 2D numpy array, updated. Add np_baseball and updated and print out the result. - You want to convert the units of height and weight to metric (meters and kilograms, respectively). As a first step, create a numpy array with three values: 0.0254, 0.453592 and 1. Name this array conversion. - Multiply np_baseball with conversion and print out the result.\n\n# Import numpy package\nimport numpy as np\n\n# Create np_baseball (3 cols)\nnp_baseball = np.array(baseball)\n\n# Print out addition of np_baseball and updated\nprint(np_baseball+updated)\n\n# Create numpy array: conversion\n\nconversion =np.array([0.0254, 0.453592,1])\n# Print out product of np_baseball and conversion\nprint(np_baseball*conversion)\n\n[[ 75.2303559  168.83775102  23.99      ]\n [ 75.02614252 231.09732309  35.69      ]\n [ 73.1544228  215.08167641  31.78      ]\n [ 72.64427532 204.90461929  36.43      ]\n [ 74.00590086 190.24342718  36.71      ]\n [ 69.97953547 188.19841763  30.39      ]\n [ 69.62874324 222.72324216  31.77      ]\n [ 72.27075194 191.12053687  36.07      ]\n [ 76.47655945 220.17504464  31.19      ]\n [ 71.91699376 172.98883751  28.05      ]]\n[[  1.8796    81.64656   22.99    ]\n [  1.8796    97.52228   34.69    ]\n [  1.8288    95.25432   30.78    ]\n [  1.8288    95.25432   35.43    ]\n [  1.8542    85.275296  35.71    ]\n [  1.7526    79.832192  29.39    ]\n [  1.7526    94.800728  30.77    ]\n [  1.8034    90.7184    35.07    ]\n [  1.9304   104.779752  30.19    ]\n [  1.8034    81.64656   27.05    ]]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#average-versus-median",
    "href": "1_Introduction_to_Python.html#average-versus-median",
    "title": "Introduction to Python",
    "section": "Average versus median",
    "text": "Average versus median\nYou now know how to use numpy functions to get a better feeling for your data. It basically comes down to importing numpy and then calling several simple functions on the numpy arrays:\nimport numpy as np x = [1, 4, 8, 10, 12] np.mean(x) np.median(x)\nThe baseball data is available as a 2D numpy array with 3 columns (height, weight, age) and 1015 rows. The name of this numpy array is np_baseball. After restructuring the data, however, you notice that some height values are abnormally high. Follow the instructions and discover which summary statistic is best suited if you’re dealing with so-called outliers. np_baseball is available.",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#explore-the-baseball-data",
    "href": "1_Introduction_to_Python.html#explore-the-baseball-data",
    "title": "Introduction to Python",
    "section": "Explore the baseball data",
    "text": "Explore the baseball data\nBecause the mean and median are so far apart, you decide to complain to the MLB. They find the error and send the corrected data over to you. It’s again available as a 2D NumPy array np_baseball, with three columns.\nThe Python script in the editor already includes code to print out informative messages with the different summary statistics. Can you finish the job? np_baseball is available.\n\n\n\n\n\n\nInstructions\n\n\n\n\nThe code to print out the mean height is already included. Complete the code for the median height. Replace None with the correct code.\nUse np.std() on the first column of np_baseball to calculate stddev. Replace None with the correct code.\nDo big players tend to be heavier? Use np.corrcoef() to store the correlation between the first and second column of np_baseball in corr. Replace None with the correct code.\n\n\n\n\n# Import numpy\nimport numpy as np\n\n# Print mean height (first column)\navg = np.mean(np_baseball[:,0])\nprint(\"Average: \" + str(avg))\n\n# Print median height. Replace 'None'\nmed = np.median(np_baseball[:,0])\nprint(\"Median: \" + str(med))\n\n# Print out the standard deviation on height. Replace 'None'\nstddev = np.std(np_baseball[:,0])\nprint(\"Standard Deviation: \" + str(stddev))\n\n# Print out correlation between first and second column. Replace 'None'\ncorr = np.corrcoef(np_baseball[:,0],np_baseball[:,1])\nprint(\"Correlation: \" + str(corr))\n\nAverage: 72.1\nMedian: 72.0\nStandard Deviation: 2.118962010041709\nCorrelation: [[1.         0.45629209]\n [0.45629209 1.        ]]",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "1_Introduction_to_Python.html#blend-it-all-together",
    "href": "1_Introduction_to_Python.html#blend-it-all-together",
    "title": "Introduction to Python",
    "section": "Blend it all together",
    "text": "Blend it all together\nIn the last few exercises you’ve learned everything there is to know about heights and weights of baseball players. Now it’s time to dive into another sport: soccer.\nYou’ve contacted FIFA for some data and they handed you two lists. The lists are the following:\npositions = ['GK', 'M', 'A', 'D', ...] heights = [191, 184, 185, 180, ...] Each element in the lists corresponds to a player. The first list, positions, contains strings representing each player’s position. The possible positions are: ‘GK’ (goalkeeper), ‘M’ (midfield), ‘A’ (attack) and ‘D’ (defense). The second list, heights, contains integers representing the height of the player in cm. The first player in the lists is a goalkeeper and is pretty tall (191 cm).\nYou’re fairly confident that the median height of goalkeepers is higher than that of other players on the soccer field. Some of your friends don’t believe you, so you are determined to show them using the data you received from FIFA and your newly acquired Python skills. heights and positions are available as lists Instructions - Convert heights and positions, which are regular lists, to numpy arrays. Call them np_heights and np_positions. - Extract all the heights of the goalkeepers. You can use a little trick here: use np_positions == 'GK' as an index for np_heights. Assign the result to gk_heights. - Extract all the heights of all the other players. This time use np_positions != ‘GK’ as an index for np_heights. Assign the result to other_heights. - Print out the median height of the goalkeepers using np.median(). Replace None with the correct code. - Do the same for the other players. Print out their median height. Replace None with the correct code.\n\npositions=soccer_positions\nheights = soccer_heights\n\n\n# Import numpy\nimport numpy as np\n\n# Convert positions and heights to numpy arrays: np_positions, np_heights\n\nnp_positions = np.array(positions)\nnp_heights = np.array(heights)\n\n# Heights of the goalkeepers: gk_heights\ngk_heights = np_heights[np_positions=='GK']\n\n# Heights of the other players: other_heights\nother_heights = np_heights[np_positions!='GK']\n\n# Print out the median height of goalkeepers. Replace 'None'\nprint(\"Median height of goalkeepers: \" + str(np.median(gk_heights)))\n\n# Print out the median height of other players. Replace 'None'\nprint(\"Median height of other players: \" + str(np.median(other_heights)))\n\nMedian height of goalkeepers: 188.0\nMedian height of other players: 181.0",
    "crumbs": [
      "Introduction to Python"
    ]
  },
  {
    "objectID": "2_Intermediate_Python.html",
    "href": "2_Intermediate_Python.html",
    "title": "Intermediate Python",
    "section": "",
    "text": "Import the course packages and datasets\n\n# Import the course packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Import the two datasets\ngapminder = pd.read_csv(\"datasets/gapminder.csv\")\nbrics = pd.read_csv(\"datasets/brics.csv\")\n\n\nprint(gapminder.head(3))\n\n       country  year  population    cont  life_exp      gdp_cap\n0  Afghanistan  2007  31889923.0    Asia    43.828   974.580338\n1      Albania  2007   3600523.0  Europe    76.423  5937.029526\n2      Algeria  2007  33333216.0  Africa    72.301  6223.367465\n\n\n\nprint(brics.head(3))\n\n  country_ab country    capital    area  population\n0         BR  Brazil   Brasilia   8.516       200.4\n1         RU  Russia     Moscow  17.100       143.5\n2         IN   India  New Delhi   3.286      1252.0\n\n\n\n\nLine plot (1)\nWith matplotlib, you can create a bunch of different plots in Python. The most basic plot is the line plot. A general recipe is given here.\n\nimport matplotlib.pyplot as plt\nplt.plot(x,y)\nplt.show()\n\nIn the video, you already saw how much the world population has grown over the past years. Will it continue to do so? The world bank has estimates of the world population for the years 1950 up to 2100. The years are loaded in your workspace as a list called year, and the corresponding populations as a list called pop.\n\n\n\n\n\n\nInstructions\n\n\n\n\nprint() the last item from both the year and the pop list to see what the predicted population for the year 2100 is. Use two print() functions.\nBefore you can start, you should import matplotlib.pyplot as plt. pyplot is a sub-package of matplotlib, hence the dot.\nUse plt.plot() to build a line plot. year should be mapped on the horizontal axis, pop on the vertical axis. Don’t forget to finish off with the plt.show() function to actually display the plot.\n\n\n\n\n\nCode\n# Print the last item from year and pop\n\nprint(year[-1])\nprint(pop[-1])\n\n# Import matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\n\n# Make a line plot: year on the x-axis, pop on the y-axis\nplt.plot(year,pop)\n\n# Display the plot with plt.show()\nplt.show()\n\n\n2100\n10.85\n\n\n\n\n\n\n\n\n\n\n\nLine plot (2)\nNow that you’ve built your first line plot, let’s start working on the data that professor Hans Rosling used to build his beautiful bubble chart. It was collected in 2007. Two lists are available for you:\n\nlife_exp which contains the life expectancy for each country and\ngdp_cap, which contains the GDP per capita (i.e. per person) for each country expressed in US Dollars.\n\nGDP stands for Gross Domestic Product. It basically represents the size of the economy of a country. Divide this by the population and you get the GDP per capita.\nmatplotlib.pyplot is already imported as plt, so you can get started straight away.\n\n\n\n\n\n\nInstructions\n\n\n\n\nPrint the last item from both the list gdp_cap, and the list life_exp; it is information about Zimbabwe.\nBuild a line chart, with gdp_cap on the x-axis, and life_exp on the y-axis. Does it make sense to plot this data on a line plot?\nDon’t forget to finish off with a plt.show() command, to actually display the plot.\n\n\n\n\n\nCode\n# Print the last item of gdp_cap and life_exp\ngdp_cap = list(gapminder[\"gdp_cap\"])\nlife_exp=list(gapminder.life_exp)\nprint(gdp_cap[-1])\nprint(life_exp[-1])\n\n# Make a line plot, gdp_cap on the x-axis, life_exp on the y-axis\nplt.plot(gdp_cap,life_exp)\n\n# Display the plot\nplt.show()\n\n\n469.7092981\n43.487\n\n\n\n\n\n\n\n\n\n\n\nScatter Plot (1)\nWhen you have a time scale along the horizontal axis, the line plot is your friend. But in many other cases, when you’re trying to assess if there’s a correlation between two variables, for example, the scatter plot is the better choice. Below is an example of how to build a scatter plot.\nimport matplotlib.pyplot as plt plt.scatter(x,y) plt.show()\nLet’s continue with the gdp_cap versus life_exp plot, the GDP and life expectancy data for different countries in 2007. Maybe a scatter plot will be a better alternative?\nAgain, the matplotlib.pyplot package is available as plt.\n\n\n\n\n\n\nInstructions\n\n\n\n\nChange the line plot that’s coded in the script to a scatter plot.\nA correlation will become clear when you display the GDP per capita on a logarithmic scale. Add the line plt.xscale(‘log’).\nFinish off your script with plt.show() to display the plot.\n\n\n\n\n\nCode\n# Change the line plot below to a scatter plot\nplt.scatter(gdp_cap, life_exp)\n\n# Put the x-axis on a logarithmic scale\nplt.xscale('log')\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nScatter plot (2)\nIn the previous exercise, you saw that the higher GDP usually corresponds to a higher life expectancy. In other words, there is a positive correlation.\nDo you think there’s a relationship between population and life expectancy of a country? The list life_exp from the previous exercise is already available. In addition, now also pop is available, listing the corresponding populations for the countries in 2007. The populations are in millions of people.\n\n\n\n\n\n\nInstructions\n\n\n\n\nStart from scratch: import matplotlib.pyplot as plt.\nBuild a scatter plot, where pop is mapped on the horizontal axis, and life_exp is mapped on the vertical axis.\nFinish the script with plt.show() to actually display the plot. Do you see a correlation?\n\n\n\n\n\nCode\n# Import package\nimport matplotlib.pyplot as plt\n\n# Build Scatter plot\nplt.scatter(pop[0:len(life_exp)],life_exp)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBuild a histogram (1)\nlife_exp, the list containing data on the life expectancy for different countries in 2007, is available in your Python shell.\nTo see how life expectancy in different countries is distributed, let’s create a histogram of life_exp.\nmatplotlib.pyplot is already available as plt.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse plt.hist() to create a histogram of the values in life_exp. Do not specify the number of bins; Python will set the number of bins to 10 by default for you.\nAdd plt.show() to actually display the histogram. Can you tell which bin contains the most observations?\n\n\n\n\n\nCode\n# Create histogram of life_exp data\nplt.hist(life_exp)\n\n# Display histogram\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nBuild a histogram (2): bins\nIn the previous exercise, you didn’t specify the number of bins. By default, Python sets the number of bins to 10 in that case. The number of bins is pretty important. Too few bins will oversimplify reality and won’t show you the details. Too many bins will overcomplicate reality and won’t show the bigger picture.\nTo control the number of bins to divide your data in, you can set the bins argument.\nThat’s exactly what you’ll do in this exercise. You’ll be making two plots here. The code in the script already includes plt.show() and plt.clf() calls; plt.show() displays a plot; plt.clf() cleans it up again so you can start afresh.\n\n\n\n\n\n\nInstructions\n\n\n\n\nBuild a histogram of life_exp, with 5 bins. Can you tell which bin contains the most observations?\nBuild another histogram of life_exp, this time with 20 bins. Is this better?\n\n\n\n\n\nCode\n# Build histogram with 5 bins\nplt.hist(life_exp,bins=5)\n\n# Show and clean up plot\nplt.show()\nplt.clf()\n\n# Build histogram with 20 bins\nplt.hist(life_exp,bins=20)\n\n# Show and clean up again\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a histogram (3): compare\nIn the video, you saw population pyramids for the present day and for the future. Because we were using a histogram, it was very easy to make a comparison.\nLet’s do a similar comparison. life_exp contains life expectancy data for different countries in 2007. You also have access to a second list now, life_exp1950, containing similar data for 1950. Can you make a histogram for both datasets?\nYou’ll again be making two plots. The plt.show() and plt.clf() commands to render everything nicely are already included. Also matplotlib.pyplot is imported for you, as plt.\n\n\n\n\n\n\nInstructions\n\n\n\n\nBuild a histogram of life_exp with 15 bins.\nBuild a histogram of life_exp1950, also with 15 bins. Is there a big difference with the histogram for the 2007 data?\n\n\n\n\n\nCode\n# Histogram of life_exp, 15 bins\n\nplt.hist(life_exp,bins = 15)\n# Show and clear plot\nplt.show()\nplt.clf()\n\n# Histogram of life_exp1950, 15 bins\nplt.hist(life_exp1950,bins=15)\n\n# Show and clear plot again\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\nLabels\nIt’s time to customize your own plot. This is the fun part, you will see your plot come to life!\nYou’re going to work on the scatter plot with world development data: GDP per capita on the x-axis (logarithmic scale), life expectancy on the y-axis. The code for this plot is available in the script.\nAs a first step, let’s add axis labels and a title to the plot. You can do this with the xlabel(), ylabel() and title() functions, available in matplotlib.pyplot. This sub-package is already imported as plt.\n\n\n\n\n\n\nInstructions\n\n\n\n\nThe strings xlab and ylab are already set for you. Use these variables to set the label of the x- and y-axis.\nThe string title is also coded for you. Use it to add a title to the plot.\nAfter these customizations, finish the script with plt.show() to actually display the plot.\n\n\n\n\n\nCode\n# Basic scatter plot, log scale\nplt.scatter(gdp_cap, life_exp)\nplt.xscale('log') \n\n# Strings\nxlab = 'GDP per Capita [in USD]'\nylab = 'Life Expectancy [in years]'\ntitle = 'World Development in 2007'\n\n# Add axis labels\n\nplt.xlabel(xlab)\nplt.ylabel(ylab)\n\n# Add title\nplt.title(title)\n\n# After customizing, display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTicks\nThe customizations you’ve coded up to now are available in the script, in a more concise form.\nIn the video, Hugo has demonstrated how you could control the y-ticks by specifying two arguments: plt.yticks([0,1,2], [\"one\",\"two\",\"three\"])\nIn this example, the ticks corresponding to the numbers 0, 1 and 2 will be replaced by one, two and three, respectively.\nLet’s do a similar thing for the x-axis of your world development chart, with the xticks() function. The tick values 1000, 10000 and 100000 should be replaced by 1k, 10k and 100k. To this end, two lists have already been created for you: tick_val and tick_lab.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse tick_val and tick_lab as inputs to the xticks() function to make the the plot more readable.\nAs usual, display the plot with plt.show() after you’ve added the customizations.\n\n\n\n\n\nCode\n# Scatter plot\nplt.scatter(gdp_cap, life_exp)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\n\n# Definition of tick_val and tick_lab\ntick_val = [1000, 10000, 100000]\ntick_lab = ['1k', '10k', '100k']\n\n# Adapt the ticks on the x-axis\nplt.xticks(tick_val,tick_lab)\n\n# After customizing, display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSizes\nRight now, the scatter plot is just a cloud of blue dots, indistinguishable from each other. Let’s change this. Wouldn’t it be nice if the size of the dots corresponds to the population?\nTo accomplish this, there is a list pop loaded in your workspace. It contains population numbers for each country expressed in millions. You can see that this list is added to the scatter method, as the argument s, for size.\n\n\n\n\n\n\nInstructions\n\n\n\n\nRun the script to see how the plot changes.\nLooks good, but increasing the size of the bubbles will make things stand out more.\n\nImport the numpy package as np.\nUse np.array() to create a numpy array from the list pop. Call this NumPy array np_pop.\nDouble the values in np_pop setting the value of np_pop equal to np_pop * 2. Because np_pop is a NumPy array, each array element will be doubled.\nChange the s argument inside plt.scatter() to be np_pop instead of pop.\n\n\n\n\n\n\nCode\n# Import numpy as np\nimport numpy as np\n\n# Store pop as a numpy array: np_pop\nnp_pop = np.array(pop[0:len(gdp_cap)])\n\n# Double np_pop\nnp_pop=2*np_pop\n\n# Update: set s argument to np_pop\nplt.scatter(gdp_cap[0:len(life_exp)], life_exp, s = np_pop)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000, 10000, 100000],['1k', '10k', '100k'])\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nColors\nThe code you’ve written up to now is available in the script.\nThe next step is making the plot more colorful! To do this, a list col has been created for you. It’s a list with a color for each corresponding country, depending on the continent the country is part of.\nHow did we make the list col you ask? The Gapminder data contains a list continent with the continent each country belongs to. A dictionary is constructed that maps continents onto colors:\ndict = {\n    'Asia':'red',\n    'Europe':'green',\n    'Africa':'blue',\n    'Americas':'yellow',\n    'Oceania':'black'\n}\nNothing to worry about now; you will learn about dictionaries in the next chapter.\n\n\n\n\n\n\nInstructions\n\n\n\n\nAdd c = col to the arguments of the plt.scatter() function.\nChange the opacity of the bubbles by setting the alpha argument to 0.8 inside plt.scatter(). Alpha can be set from zero to one, where zero is totally transparent, and one is not at all transparent.\n\n\n\n\n\nCode\n# Specify c and alpha inside plt.scatter()\nplt.scatter(x = gdp_cap, y = life_exp, s = np.array(pop[0:len(gdp_cap)]) * 2, c= col,alpha = 0.8)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000,10000,100000], ['1k','10k','100k'])\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAdditional Customizations\nIf you have another look at the script, under # Additional Customizations, you’ll see that there are two plt.text() functions now. They add the words “India” and “China” in the plot.\n\n\n\n\n\n\nInstructions\n\n\n\n\nAdd plt.grid(True) after the plt.text() calls so that gridlines are drawn on the plot.\n\n\n\n\n\nCode\n# Scatter plot\nplt.scatter(x = gdp_cap, y = life_exp, s = np.array(pop) * 2, c = col, alpha = 0.8)\n\n# Previous customizations\nplt.xscale('log') \nplt.xlabel('GDP per Capita [in USD]')\nplt.ylabel('Life Expectancy [in years]')\nplt.title('World Development in 2007')\nplt.xticks([1000,10000,100000], ['1k','10k','100k'])\n\n# Additional customizations\nplt.text(1550, 71, 'India')\nplt.text(5700, 80, 'China')\n\n# Add grid() call\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMotivation for dictionaries\nTo see why dictionaries are useful, have a look at the two lists defined in the script. countries contains the names of some European countries. capitals lists the corresponding names of their capital.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the index() method on countries to find the index of ‘germany’. Store this index as ind_ger.\nUse ind_ger to access the capital of Germany from the capitals list. Print it out.\n\n\n\n\n\nCode\n# Definition of countries and capital\ncountries = ['spain', 'france', 'germany', 'norway']\ncapitals = ['madrid', 'paris', 'berlin', 'oslo']\n\n# Get index of 'germany': ind_ger\nind_ger = countries.index(\"germany\")\n\n# Use ind_ger to print out capital of Germany\nprint(capitals[ind_ger])\n\n\nberlin\n\n\n\n\nCreate dictionary\nThe countries and capitals lists are again available in the script. It’s your job to convert this data to a dictionary where the country names are the keys and the capitals are the corresponding values. As a refresher, here is a recipe for creating a dictionary:\nmy_dict = {\n   \"key1\":\"value1\",\n   \"key2\":\"value2\",\n}\nIn this recipe, both the keys and the values are strings. This will also be the case for this exercise.\n\n\n\n\n\n\nInstructions\n\n\n\n\nWith the strings in countries and capitals, create a dictionary called europe with 4 key:value pairs. Beware of capitalization! Make sure you use lowercase characters everywhere.\nPrint out europe to see if the result is what you expected.\n\n\n\n\n\nCode\n# Definition of countries and capital\ncountries = ['spain', 'france', 'germany', 'norway']\ncapitals = ['madrid', 'paris', 'berlin', 'oslo']\n\n# From string in countries and capitals, create dictionary europe\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n\n# Print europe\nprint(europe)\n\n\n{'spain': 'madrid', 'france': 'paris', 'germany': 'berlin', 'norway': 'oslo'}\n\n\n\n\nAccess dictionary\nIf the keys of a dictionary are chosen wisely, accessing the values in a dictionary is easy and intuitive. For example, to get the capital for France from europe you can use: europe['france'] Here, ‘france’ is the key and ‘paris’ the value is returned.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCheck out which keys are in europe by calling the keys() method on europe. Print out the result.\nPrint out the value that belongs to the key ‘norway’.\n\n\n\n\n\nCode\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n\n# Print out the keys in europe\n\nprint(europe.keys())\n# Print out value that belongs to key 'norway'\nprint(europe['norway'])\n\n\ndict_keys(['spain', 'france', 'germany', 'norway'])\noslo\n\n\n\n\nAccess dictionary\nIf the keys of a dictionary are chosen wisely, accessing the values in a dictionary is easy and intuitive. For example, to get the capital for France from europe you can use: europe['france'] Here, ‘france’ is the key and ‘paris’ the value is returned.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCheck out which keys are in europe by calling the keys() method on europe. Print out the result.\nPrint out the value that belongs to the key ‘norway’.\n\n\n\n\n\nCode\n# Definition of dictionary\neurope = {'spain':'madrid', 'france':'paris', 'germany':'berlin', 'norway':'oslo' }\n\n# Print out the keys in europe\n\nprint(europe.keys())\n# Print out value that belongs to key 'norway'\nprint(europe['norway'])\n\n\ndict_keys(['spain', 'france', 'germany', 'norway'])\noslo",
    "crumbs": [
      "Intermediate Python"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html",
    "href": "3_Data_Manipulation_with_pandas.html",
    "title": "Data Manipulation with pandas",
    "section": "",
    "text": "Explore Datasets\nUse the DataFrames imported in the first cell to explore the data and practice your skills! - Print the highest weekly sales for each department in the walmart DataFrame. Limit your results to the top five departments, in descending order. If you’re stuck, try reviewing this video. - What was the total nb_sold of organic avocados in 2017 in the avocado DataFrame? If you’re stuck, try reviewing this video. - Create a bar plot of the total number of homeless people by region in the homelessness DataFrame. Order the bars in descending order. Bonus: create a horizontal bar chart. If you’re stuck, try reviewing this video. - Create a line plot with two lines representing the temperatures in Toronto and Rome. Make sure to properly label your plot. Bonus: add a legend for the two lines. If you’re stuck, try reviewing this video.",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#inspecting-a-dataframe",
    "href": "3_Data_Manipulation_with_pandas.html#inspecting-a-dataframe",
    "title": "Data Manipulation with pandas",
    "section": "Inspecting a DataFrame",
    "text": "Inspecting a DataFrame\nWhen you get a new DataFrame to work with, the first thing you need to do is explore it and see what it contains. There are several useful methods and attributes for this.\n.head() returns the first few rows (the “head” of the DataFrame).\n.info() shows information on each of the columns, such as the data type and number of missing values.\n.shape returns the number of rows and columns of the DataFrame.\n.describe() calculates a few summary statistics for each column.\nhomelessness is a DataFrame containing estimates of homelessness in each U.S. state in 2018. The individual column is the number of homeless individuals not part of a family with children. The family_members column is the number of homeless individuals part of a family with children. The state_pop column is the state’s total population.\n\nInstructions\n\nPrint the head of the homelessness DataFrame.\nPrint information about the column types and missing values in homelessness.\nPrint the number of rows and columns in homelessness.\nPrint some summary statistics that describe the homelessness DataFrame.\n\n# Print the head of the homelessness data\nprint(homelessness.head())\n\n# Print information about homelessness\nprint(homelessness.info())\n\n# Print the shape of homelessness\nprint(homelessness.shape)\n\n# Print a description of homelessness\nprint(homelessness.describe())\n               region       state  individuals  family_members  state_pop\n0  East South Central     Alabama       2570.0           864.0    4887681\n1             Pacific      Alaska       1434.0           582.0     735139\n2            Mountain     Arizona       7259.0          2606.0    7158024\n3  West South Central    Arkansas       2280.0           432.0    3009733\n4             Pacific  California     109008.0         20964.0   39461588\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 51 entries, 0 to 50\nData columns (total 5 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   region          51 non-null     object \n 1   state           51 non-null     object \n 2   individuals     51 non-null     float64\n 3   family_members  51 non-null     float64\n 4   state_pop       51 non-null     int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 2.1+ KB\nNone\n(51, 5)\n         individuals  family_members     state_pop\ncount      51.000000       51.000000  5.100000e+01\nmean     7225.784314     3504.882353  6.405637e+06\nstd     15991.025083     7805.411811  7.327258e+06\nmin       434.000000       75.000000  5.776010e+05\n25%      1446.500000      592.000000  1.777414e+06\n50%      3082.000000     1482.000000  4.461153e+06\n75%      6781.500000     3196.000000  7.340946e+06\nmax    109008.000000    52070.000000  3.946159e+07",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#parts-of-a-dataframe",
    "href": "3_Data_Manipulation_with_pandas.html#parts-of-a-dataframe",
    "title": "Data Manipulation with pandas",
    "section": "Parts of a DataFrame",
    "text": "Parts of a DataFrame\nTo better understand DataFrame objects, it’s useful to know that they consist of three components, stored as attributes:\n.values: A two-dimensional NumPy array of values.\n.columns: An index of columns: the column names.\n.index: An index for the rows: either row numbers or row names.\nYou can usually think of indexes as a list of strings or numbers, though the pandas Index data type allows for more sophisticated options. (These will be covered later in the course.)\nhomelessness is available. ### Instructions\nImport pandas using the alias pd.\nPrint a 2D NumPy array of the values in homelessness.\nPrint the column names of homelessness.\nPrint the index of homelessness.\n# Import pandas using the alias pd\nimport pandas as pd\n\n# Print the values of homelessness\nprint(homelessness.values)\n\n# Print the column index of homelessness\nprint(homelessness.columns)\n\n# Print the row index of homelessness\nprint(homelessness.index)\n[['East South Central' 'Alabama' 2570.0 864.0 4887681]\n ['Pacific' 'Alaska' 1434.0 582.0 735139]\n ['Mountain' 'Arizona' 7259.0 2606.0 7158024]\n ['West South Central' 'Arkansas' 2280.0 432.0 3009733]\n ['Pacific' 'California' 109008.0 20964.0 39461588]\n ['Mountain' 'Colorado' 7607.0 3250.0 5691287]\n ['New England' 'Connecticut' 2280.0 1696.0 3571520]\n ['South Atlantic' 'Delaware' 708.0 374.0 965479]\n ['South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n ['South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n ['South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n ['Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n ['Mountain' 'Idaho' 1297.0 715.0 1750536]\n ['East North Central' 'Illinois' 6752.0 3891.0 12723071]\n ['East North Central' 'Indiana' 3776.0 1482.0 6695497]\n ['West North Central' 'Iowa' 1711.0 1038.0 3148618]\n ['West North Central' 'Kansas' 1443.0 773.0 2911359]\n ['East South Central' 'Kentucky' 2735.0 953.0 4461153]\n ['West South Central' 'Louisiana' 2540.0 519.0 4659690]\n ['New England' 'Maine' 1450.0 1066.0 1339057]\n ['South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n ['New England' 'Massachusetts' 6811.0 13257.0 6882635]\n ['East North Central' 'Michigan' 5209.0 3142.0 9984072]\n ['West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n ['East South Central' 'Mississippi' 1024.0 328.0 2981020]\n ['West North Central' 'Missouri' 3776.0 2107.0 6121623]\n ['Mountain' 'Montana' 983.0 422.0 1060665]\n ['West North Central' 'Nebraska' 1745.0 676.0 1925614]\n ['Mountain' 'Nevada' 7058.0 486.0 3027341]\n ['New England' 'New Hampshire' 835.0 615.0 1353465]\n ['Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n ['Mountain' 'New Mexico' 1949.0 602.0 2092741]\n ['Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n ['South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n ['West North Central' 'North Dakota' 467.0 75.0 758080]\n ['East North Central' 'Ohio' 6929.0 3320.0 11676341]\n ['West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n ['Pacific' 'Oregon' 11139.0 3337.0 4181886]\n ['Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n ['New England' 'Rhode Island' 747.0 354.0 1058287]\n ['South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n ['West North Central' 'South Dakota' 836.0 323.0 878698]\n ['East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n ['West South Central' 'Texas' 19199.0 6111.0 28628666]\n ['Mountain' 'Utah' 1904.0 972.0 3153550]\n ['New England' 'Vermont' 780.0 511.0 624358]\n ['South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n ['Pacific' 'Washington' 16424.0 5880.0 7523869]\n ['South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n ['East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n ['Mountain' 'Wyoming' 434.0 205.0 577601]]\nIndex(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')\nRangeIndex(start=0, stop=51, step=1)",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#sorting-rows",
    "href": "3_Data_Manipulation_with_pandas.html#sorting-rows",
    "title": "Data Manipulation with pandas",
    "section": "Sorting rows",
    "text": "Sorting rows\nFinding interesting bits of data in a DataFrame is often easier if you change the order of the rows. You can sort the rows by passing a column name to .sort_values().\nIn cases where rows have the same value (this is common if you sort on a categorical variable), you may wish to break the ties by sorting on another column. You can sort on multiple columns in this way by passing a list of column names.\n\n\n\nSort on …\nSyntax\n\n\n\n\none column\ndf.sort_values(“breed”)\n\n\n\nmultiple columns | df.sort_values([“breed”, “weight_kg”])|\nBy combining .sort_values() with .head(), you can answer questions in the form, “What are the top cases where…?”.\nhomelessness is available and pandas is loaded as pd. ### Instructions - Sort homelessness by the number of homeless individuals, from smallest to largest, and save this as homelessness_ind. Print the head of the sorted DataFrame.\n\nSort homelessness by the number of homeless family_members in descending order, and save this as homelessness_fam. Print the head of the sorted DataFrame.\nSort homelessness first by region (ascending), and then by number of family members (descending). Save this as homelessness_reg_fam.\n\n# Sort homelessness by individuals\nhomelessness_ind = homelessness.sort_values(\"individuals\")\n\n# Print the top few rows\nprint(homelessness_ind.head())\n                region         state  individuals  family_members  state_pop\n50            Mountain       Wyoming        434.0           205.0     577601\n34  West North Central  North Dakota        467.0            75.0     758080\n7       South Atlantic      Delaware        708.0           374.0     965479\n39         New England  Rhode Island        747.0           354.0    1058287\n45         New England       Vermont        780.0           511.0     624358\n# Sort homelessness by descending family members\nhomelessness_fam = homelessness.sort_values(\"family_members\",ascending=False)\n\n# Print the top few rows\nprint(homelessness_fam.head())\n                region          state  individuals  family_members  state_pop\n32        Mid-Atlantic       New York      39827.0         52070.0   19530351\n4              Pacific     California     109008.0         20964.0   39461588\n21         New England  Massachusetts       6811.0         13257.0    6882635\n9       South Atlantic        Florida      21443.0          9587.0   21244317\n43  West South Central          Texas      19199.0          6111.0   28628666\n# Sort homelessness by region, then descending family members\nhomelessness_reg_fam = homelessness.sort_values([\"region\",\"family_members\"], ascending=(True,False))\n\n# Print the top few rows\nprint(homelessness_reg_fam.head())\n                region      state  individuals  family_members  state_pop\n13  East North Central   Illinois       6752.0          3891.0   12723071\n35  East North Central       Ohio       6929.0          3320.0   11676341\n22  East North Central   Michigan       5209.0          3142.0    9984072\n49  East North Central  Wisconsin       2740.0          2167.0    5807406\n14  East North Central    Indiana       3776.0          1482.0    6695497",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#subsetting-columns",
    "href": "3_Data_Manipulation_with_pandas.html#subsetting-columns",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting columns",
    "text": "Subsetting columns\nWhen working with data, you may not need all of the variables in your dataset. Square brackets ([]) can be used to select only the columns that matter to you in an order that makes sense to you. To select only “col_a” of the DataFrame df, use\ndf[“col_a”]\nTo select “col_a” and “col_b” of df, use\ndf[[“col_a”, “col_b”]]\nhomelessness is available and pandas is loaded as pd.\n\nInstructions\n\nCreate a DataFrame called individuals that contains only the individuals column of homelessness. Print the head of the result.\nCreate a DataFrame called state_fam that contains only the state and family_members columns of homelessness, in that order. Print the head of the result.\nCreate a DataFrame called ind_state that contains the individuals and state columns of homelessness, in that order. Print the head of the result.\n\n# Select the individuals column\nindividuals = homelessness[\"individuals\"]\n\n# Print the head of the result\nprint(individuals.head())\n0      2570.0\n1      1434.0\n2      7259.0\n3      2280.0\n4    109008.0\nName: individuals, dtype: float64\n# Select the state and family_members columns\nstate_fam = homelessness[[\"state\",\"family_members\"]]\n\n# Print the head of the result\nprint(state_fam.head())\n        state  family_members\n0     Alabama           864.0\n1      Alaska           582.0\n2     Arizona          2606.0\n3    Arkansas           432.0\n4  California         20964.0\n# Select only the individuals and state columns, in that order\nind_state = homelessness[[\"individuals\",\"state\"]]\n\n# Print the head of the result\nprint(ind_state.head())\n   individuals       state\n0       2570.0     Alabama\n1       1434.0      Alaska\n2       7259.0     Arizona\n3       2280.0    Arkansas\n4     109008.0  California",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#subsetting-rows",
    "href": "3_Data_Manipulation_with_pandas.html#subsetting-rows",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting rows",
    "text": "Subsetting rows\nA large part of data science is about finding which bits of your dataset are interesting. One of the simplest techniques for this is to find a subset of rows that match some criteria. This is sometimes known as filtering rows or selecting rows.\nThere are many ways to subset a DataFrame, perhaps the most common is to use relational operators to return True or False for each row, then pass that inside square brackets.\ndogs[dogs[“height_cm”] &gt; 60] dogs[dogs[“color”] == “tan”]\nYou can filter for multiple conditions at once by using the “bitwise and” operator, &.\ndogs[(dogs[“height_cm”] &gt; 60) & (dogs[“color”] == “tan”)]\nhomelessness is available and pandas is loaded as pd.\n\nInstructions\n\nFilter homelessness for cases where the number of individuals is greater than ten thousand, assigning to ind_gt_10k. View the printed result.\nFilter homelessness for cases where the USA Census region is “Mountain”, assigning to mountain_reg. View the printed result.\nFilter homelessness for cases where the number of family_members is less than one thousand and the region is “Pacific”, assigning to fam_lt_1k_pac. View the printed result.\n\n# Filter for rows where individuals is greater than 10000\nind_gt_10k = homelessness[homelessness.individuals&gt;10000]\n\n# See the result\nprint(ind_gt_10k)\n                region       state  individuals  family_members  state_pop\n4              Pacific  California     109008.0         20964.0   39461588\n9       South Atlantic     Florida      21443.0          9587.0   21244317\n32        Mid-Atlantic    New York      39827.0         52070.0   19530351\n37             Pacific      Oregon      11139.0          3337.0    4181886\n43  West South Central       Texas      19199.0          6111.0   28628666\n47             Pacific  Washington      16424.0          5880.0    7523869\n# Filter for rows where region is Mountain\nmountain_reg = homelessness[homelessness.region=='Mountain']\n\n# See the result\nprint(mountain_reg)\n      region       state  individuals  family_members  state_pop\n2   Mountain     Arizona       7259.0          2606.0    7158024\n5   Mountain    Colorado       7607.0          3250.0    5691287\n12  Mountain       Idaho       1297.0           715.0    1750536\n26  Mountain     Montana        983.0           422.0    1060665\n28  Mountain      Nevada       7058.0           486.0    3027341\n31  Mountain  New Mexico       1949.0           602.0    2092741\n44  Mountain        Utah       1904.0           972.0    3153550\n50  Mountain     Wyoming        434.0           205.0     577601\n# Filter for rows where family_members is less than 1000 \n# and region is Pacific\nfam_lt_1k_pac = homelessness[(homelessness.family_members&lt;1000) & (homelessness.region=='Pacific')]\n\n# See the result\nprint(fam_lt_1k_pac)\n    region   state  individuals  family_members  state_pop\n1  Pacific  Alaska       1434.0           582.0     735139",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#subsetting-rows-by-categorical-variables",
    "href": "3_Data_Manipulation_with_pandas.html#subsetting-rows-by-categorical-variables",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting rows by categorical variables",
    "text": "Subsetting rows by categorical variables\nSubsetting data based on a categorical variable often involves using the “or” operator (|) to select rows from multiple categories. This can get tedious when you want all states in one of three different regions, for example. Instead, use the .isin() method, which will allow you to tackle this problem by writing one condition instead of three separate ones.\ncolors = [“brown”, “black”, “tan”] condition = dogs[“color”].isin(colors) dogs[condition]\nhomelessness is available and pandas is loaded as pd.\n\nInstructions\n\nFilter homelessness for cases where the USA census region is “South Atlantic” or it is “Mid-Atlantic”, assigning to south_mid_atlantic. View the printed result.\nFilter homelessness for cases where the USA census state is in the list of Mojave states, canu, assigning to mojave_homelessness. View the printed result.\n\n# Subset for rows in South Atlantic or Mid-Atlantic regions\nsouth_mid_atlantic = homelessness[(homelessness.region=='South Atlantic') | (homelessness.region=='Mid-Atlantic')]\n\n# See the result\nprint(south_mid_atlantic)\n            region                 state  ...  family_members  state_pop\n7   South Atlantic              Delaware  ...           374.0     965479\n8   South Atlantic  District of Columbia  ...          3134.0     701547\n9   South Atlantic               Florida  ...          9587.0   21244317\n10  South Atlantic               Georgia  ...          2556.0   10511131\n20  South Atlantic              Maryland  ...          2230.0    6035802\n30    Mid-Atlantic            New Jersey  ...          3350.0    8886025\n32    Mid-Atlantic              New York  ...         52070.0   19530351\n33  South Atlantic        North Carolina  ...          2817.0   10381615\n38    Mid-Atlantic          Pennsylvania  ...          5349.0   12800922\n40  South Atlantic        South Carolina  ...           851.0    5084156\n46  South Atlantic              Virginia  ...          2047.0    8501286\n48  South Atlantic         West Virginia  ...           222.0    1804291\n\n[12 rows x 5 columns]\n# The Mojave Desert states\ncanu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n\n# Filter for rows in the Mojave Desert states\nmojave_homelessness = homelessness[homelessness.state.isin(canu)]\n\n# See the result\nprint(mojave_homelessness)\n      region       state  individuals  family_members  state_pop\n2   Mountain     Arizona       7259.0          2606.0    7158024\n4    Pacific  California     109008.0         20964.0   39461588\n28  Mountain      Nevada       7058.0           486.0    3027341\n44  Mountain        Utah       1904.0           972.0    3153550",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#adding-new-columns",
    "href": "3_Data_Manipulation_with_pandas.html#adding-new-columns",
    "title": "Data Manipulation with pandas",
    "section": "Adding new columns",
    "text": "Adding new columns\nYou aren’t stuck with just the data you are given. Instead, you can add new columns to a DataFrame. This has many names, such as transforming, mutating, and feature engineering.\nYou can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding columns together or by changing their units.\nhomelessness is available and pandas is loaded as pd.\n\nInstructions\n\nAdd a new column to homelessness, named total, containing the sum of the individuals and family_members columns.\nAdd another column to homelessness, named p_individuals, containing the proportion of homeless people in each state who are individuals.\n\n# Add total col as sum of individuals and family_members\nhomelessness[\"total\"] = homelessness.individuals + homelessness.family_members\n\n# Add p_individuals col as proportion of total that are individuals\nhomelessness[\"p_individuals\"] = homelessness.individuals/homelessness.total\n\n# See the result\nprint(homelessness)\n                region                 state  ...     total  p_individuals\n0   East South Central               Alabama  ...    3434.0       0.748398\n1              Pacific                Alaska  ...    2016.0       0.711310\n2             Mountain               Arizona  ...    9865.0       0.735834\n3   West South Central              Arkansas  ...    2712.0       0.840708\n4              Pacific            California  ...  129972.0       0.838704\n5             Mountain              Colorado  ...   10857.0       0.700654\n6          New England           Connecticut  ...    3976.0       0.573441\n7       South Atlantic              Delaware  ...    1082.0       0.654344\n8       South Atlantic  District of Columbia  ...    6904.0       0.546060\n9       South Atlantic               Florida  ...   31030.0       0.691041\n10      South Atlantic               Georgia  ...    9499.0       0.730919\n11             Pacific                Hawaii  ...    6530.0       0.632619\n12            Mountain                 Idaho  ...    2012.0       0.644632\n13  East North Central              Illinois  ...   10643.0       0.634408\n14  East North Central               Indiana  ...    5258.0       0.718144\n15  West North Central                  Iowa  ...    2749.0       0.622408\n16  West North Central                Kansas  ...    2216.0       0.651173\n17  East South Central              Kentucky  ...    3688.0       0.741594\n18  West South Central             Louisiana  ...    3059.0       0.830337\n19         New England                 Maine  ...    2516.0       0.576312\n20      South Atlantic              Maryland  ...    7144.0       0.687850\n21         New England         Massachusetts  ...   20068.0       0.339396\n22  East North Central              Michigan  ...    8351.0       0.623758\n23  West North Central             Minnesota  ...    7243.0       0.551291\n24  East South Central           Mississippi  ...    1352.0       0.757396\n25  West North Central              Missouri  ...    5883.0       0.641849\n26            Mountain               Montana  ...    1405.0       0.699644\n27  West North Central              Nebraska  ...    2421.0       0.720777\n28            Mountain                Nevada  ...    7544.0       0.935578\n29         New England         New Hampshire  ...    1450.0       0.575862\n30        Mid-Atlantic            New Jersey  ...    9398.0       0.643541\n31            Mountain            New Mexico  ...    2551.0       0.764014\n32        Mid-Atlantic              New York  ...   91897.0       0.433387\n33      South Atlantic        North Carolina  ...    9268.0       0.696051\n34  West North Central          North Dakota  ...     542.0       0.861624\n35  East North Central                  Ohio  ...   10249.0       0.676066\n36  West South Central              Oklahoma  ...    3871.0       0.729269\n37             Pacific                Oregon  ...   14476.0       0.769481\n38        Mid-Atlantic          Pennsylvania  ...   13512.0       0.604130\n39         New England          Rhode Island  ...    1101.0       0.678474\n40      South Atlantic        South Carolina  ...    3933.0       0.783626\n41  West North Central          South Dakota  ...    1159.0       0.721311\n42  East South Central             Tennessee  ...    7883.0       0.778764\n43  West South Central                 Texas  ...   25310.0       0.758554\n44            Mountain                  Utah  ...    2876.0       0.662031\n45         New England               Vermont  ...    1291.0       0.604183\n46      South Atlantic              Virginia  ...    5975.0       0.657406\n47             Pacific            Washington  ...   22304.0       0.736370\n48      South Atlantic         West Virginia  ...    1243.0       0.821400\n49  East North Central             Wisconsin  ...    4907.0       0.558386\n50            Mountain               Wyoming  ...     639.0       0.679186\n\n[51 rows x 7 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#combo-attack",
    "href": "3_Data_Manipulation_with_pandas.html#combo-attack",
    "title": "Data Manipulation with pandas",
    "section": "Combo-attack!",
    "text": "Combo-attack!\nYou’ve seen the four most common types of data manipulation: sorting rows, subsetting columns, subsetting rows, and adding new columns. In a real-life data analysis, you can mix and match these four manipulations to answer a multitude of questions.\nIn this exercise, you’ll answer the question, “Which state has the highest number of homeless individuals per 10,000 people in the state?” Combine your new pandas skills to find out. ### Instructions - Add a column to homelessness, indiv_per_10k, containing the number of homeless individuals per ten thousand people in each state. - Subset rows where indiv_per_10k is higher than 20, assigning to high_homelessness. - Sort high_homelessness by descending indiv_per_10k, assigning to high_homelessness_srt. - Select only the state and indiv_per_10k columns of high_homelessness_srt and save as result. Look at the result.\n# Create indiv_per_10k col as homeless individuals per 10k state pop\nhomelessness[\"indiv_per_10k\"] = 10000 * homelessness.individuals / homelessness.state_pop \n\n# Subset rows for indiv_per_10k greater than 20\nhigh_homelessness = homelessness[homelessness[\"indiv_per_10k\"]&gt;20]\n\n# Sort high_homelessness by descending indiv_per_10k\nhigh_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\",ascending=False)\n\n# From high_homelessness_srt, select the state and indiv_per_10k cols\nresult = high_homelessness_srt[[\"state\",  \"indiv_per_10k\"]]\n\n# See the result\nprint(result)\n                   state  indiv_per_10k\n8   District of Columbia      53.738381\n11                Hawaii      29.079406\n4             California      27.623825\n37                Oregon      26.636307\n28                Nevada      23.314189\n47            Washington      21.829195\n32              New York      20.392363",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#mean-and-median",
    "href": "3_Data_Manipulation_with_pandas.html#mean-and-median",
    "title": "Data Manipulation with pandas",
    "section": "Mean and median",
    "text": "Mean and median\nSummary statistics are exactly what they sound like - they summarize many numbers in one statistic. For example, mean, median, minimum, maximum, and standard deviation are summary statistics. Calculating summary statistics allows you to get a better sense of your data, even if there’s a lot of it.\nsales is available and pandas is loaded as pd. ### Instructions - Explore your new DataFrame first by printing the first few rows of the sales DataFrame. - Print information about the columns in sales. - Print the mean of the weekly_sales column. - Print the median of the weekly_sales column.\nsales = pd.read_csv(\"datasets/walmart.csv\")\n# Print the head of the sales DataFrame\nprint(sales.head())\n\n# Print the info about the sales DataFrame\nprint(sales.info())\n\n# Print the mean of weekly_sales\nprint(sales.weekly_sales.mean())\n\n# Print the median of weekly_sales\nprint(sales[\"weekly_sales\"].median())\n   store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0      1    A           1  ...      5.727778              0.679451         8.106\n1      1    A           1  ...      8.055556              0.693452         8.106\n2      1    A           1  ...     16.816667              0.718284         7.808\n3      1    A           1  ...     22.527778              0.748928         7.808\n4      1    A           1  ...     27.050000              0.714586         7.808\n\n[5 rows x 9 columns]\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10774 entries, 0 to 10773\nData columns (total 9 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   store                 10774 non-null  int64  \n 1   type                  10774 non-null  object \n 2   department            10774 non-null  int64  \n 3   date                  10774 non-null  object \n 4   weekly_sales          10774 non-null  float64\n 5   is_holiday            10774 non-null  bool   \n 6   temperature_c         10774 non-null  float64\n 7   fuel_price_usd_per_l  10774 non-null  float64\n 8   unemployment          10774 non-null  float64\ndtypes: bool(1), float64(4), int64(2), object(2)\nmemory usage: 684.0+ KB\nNone\n23843.95014850566\n12049.064999999999\n\nSummarizing dates\nSummary statistics can also be calculated on date columns that have values with the data type datetime64. Some summary statistics — like mean — don’t make a ton of sense on dates, but others are super helpful, for example, minimum and maximum, which allow you to see what time range your data covers.\nsales is available and pandas is loaded as pd. ### Instructions - Print the maximum of the date column. - Print the minimum of the date column.\n# Print the maximum of the date column\nprint(sales.date.max())\n\n# Print the minimum of the date column\nprint(sales.date.min())\n2012-10-26\n2010-02-05",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#efficient-summaries",
    "href": "3_Data_Manipulation_with_pandas.html#efficient-summaries",
    "title": "Data Manipulation with pandas",
    "section": "Efficient summaries",
    "text": "Efficient summaries\nWhile pandas and NumPy have tons of functions, sometimes, you may need a different function to summarize your data.\nThe .agg() method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super-efficient. For example,\ndf[‘column’].agg(function)\nIn the custom function for this exercise, “IQR” is short for inter-quartile range, which is the 75th percentile minus the 25th percentile. It’s an alternative to standard deviation that is helpful if your data contains outliers.\nsales is available and pandas is loaded as pd. ### Instructions - Use the custom iqr function defined for you along with .agg() to print the IQR of the temperature_c column of sales. - Update the column selection to use the custom iqr function with .agg() to print the IQR of temperature_c, fuel_price_usd_per_l, and unemployment, in that order. - Update the aggregation functions called by .agg(): include iqr and np.median in that order.\n# A custom IQR function\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n    \n# Print IQR of the temperature_c column\nprint(sales[\"temperature_c\"].agg(iqr))\n16.583333333333336",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#cumulative-statistics",
    "href": "3_Data_Manipulation_with_pandas.html#cumulative-statistics",
    "title": "Data Manipulation with pandas",
    "section": "Cumulative statistics",
    "text": "Cumulative statistics\nCumulative statistics can also be helpful in tracking summary statistics over time. In this exercise, you’ll calculate the cumulative sum and cumulative max of a department’s weekly sales, which will allow you to identify what the total sales were so far as well as what the highest weekly sales were so far.\nA DataFrame called sales_1_1 has been created for you, which contains the sales data for department 1 of store 1. pandas is loaded as pd. ### Instructions - Sort the rows of sales_1_1 by the date column in ascending order. - Get the cumulative sum of weekly_sales and add it as a new column of sales_1_1 called cum_weekly_sales. - Get the cumulative maximum of weekly_sales, and add it as a column called cum_max_sales. - Print the date, weekly_sales, cum_weekly_sales, and cum_max_sales columns.\nsales_1_1 = sales[(sales.store==1) & (sales.department ==1)]\nsales_1_1.head(4)\n\n\n\n\n\n\n\n\n\n\n\nstore\n\n\n\n\ntype\n\n\n\n\ndepartment\n\n\n\n\ndate\n\n\n\n\nweekly_sales\n\n\n\n\nis_holiday\n\n\n\n\ntemperature_c\n\n\n\n\nfuel_price_usd_per_l\n\n\n\n\nunemployment\n\n\n\n\n\n\n\n\n0\n\n\n\n\n1\n\n\n\n\nA\n\n\n\n\n1\n\n\n\n\n2010-02-05\n\n\n\n\n24924.50\n\n\n\n\nFalse\n\n\n\n\n5.727778\n\n\n\n\n0.679451\n\n\n\n\n8.106\n\n\n\n\n\n\n1\n\n\n\n\n1\n\n\n\n\nA\n\n\n\n\n1\n\n\n\n\n2010-03-05\n\n\n\n\n21827.90\n\n\n\n\nFalse\n\n\n\n\n8.055556\n\n\n\n\n0.693452\n\n\n\n\n8.106\n\n\n\n\n\n\n2\n\n\n\n\n1\n\n\n\n\nA\n\n\n\n\n1\n\n\n\n\n2010-04-02\n\n\n\n\n57258.43\n\n\n\n\nFalse\n\n\n\n\n16.816667\n\n\n\n\n0.718284\n\n\n\n\n7.808\n\n\n\n\n\n\n3\n\n\n\n\n1\n\n\n\n\nA\n\n\n\n\n1\n\n\n\n\n2010-05-07\n\n\n\n\n17413.94\n\n\n\n\nFalse\n\n\n\n\n22.527778\n\n\n\n\n0.748928\n\n\n\n\n7.808\n\n\n\n\n\n\n# Sort sales_1_1 by date\nsales_1_1 = sales_1_1.sort_values(\"date\")\n\n# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\nsales_1_1['cum_weekly_sales'] = sales_1_1[\"weekly_sales\"].cumsum()\n\n# Get the cumulative max of weekly_sales, add as cum_max_sales col\nsales_1_1[\"cum_max_sales\"]=sales_1_1['weekly_sales'].cummax()\n\n# See the columns you calculated\nprint(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])\n          date  weekly_sales  cum_weekly_sales  cum_max_sales\n0   2010-02-05      24924.50          24924.50       24924.50\n1   2010-03-05      21827.90          46752.40       24924.50\n2   2010-04-02      57258.43         104010.83       57258.43\n3   2010-05-07      17413.94         121424.77       57258.43\n4   2010-06-04      17558.09         138982.86       57258.43\n5   2010-07-02      16333.14         155316.00       57258.43\n6   2010-08-06      17508.41         172824.41       57258.43\n7   2010-09-03      16241.78         189066.19       57258.43\n8   2010-10-01      20094.19         209160.38       57258.43\n9   2010-11-05      34238.88         243399.26       57258.43\n10  2010-12-03      22517.56         265916.82       57258.43\n11  2011-01-07      15984.24         281901.06       57258.43",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#dropping-duplicates",
    "href": "3_Data_Manipulation_with_pandas.html#dropping-duplicates",
    "title": "Data Manipulation with pandas",
    "section": "Dropping duplicates",
    "text": "Dropping duplicates\nRemoving duplicates is an essential skill to get accurate counts because often, you don’t want to count the same thing multiple times. In this exercise, you’ll create some new DataFrames using unique values from sales.\nsales is available and pandas is imported as pd. ### Instructions - Remove rows of sales with duplicate pairs of store and type and save as store_types and print the head. - Remove rows of sales with duplicate pairs of store and department and save as store_depts and print the head. - Subset the rows that are holiday weeks using the is_holiday column, and drop the duplicate dates, saving as holiday_dates. - Select the date column of holiday_dates, and print.\n# Drop duplicate store/type combinations\nstore_types = sales.drop_duplicates(subset=[\"store\",\"type\"])\nprint(store_types.head())\n\n# Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset=[\"store\",\"department\"])\nprint(store_depts.head())\n\n# Subset the rows where is_holiday is True and drop duplicate dates\nholiday_dates = sales[sales['is_holiday']].drop_duplicates(\"date\")\n\n# Print date col of holiday_dates\nprint(holiday_dates)\n      store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0         1    A           1  ...      5.727778              0.679451         8.106\n901       2    A           1  ...      4.550000              0.679451         8.324\n1798      4    A           1  ...      6.533333              0.686319         8.623\n2699      6    A           1  ...      4.683333              0.679451         7.259\n3593     10    B           1  ...     12.411111              0.782478         9.765\n\n[5 rows x 9 columns]\n    store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0       1    A           1  ...      5.727778              0.679451         8.106\n12      1    A           2  ...      5.727778              0.679451         8.106\n24      1    A           3  ...      5.727778              0.679451         8.106\n36      1    A           4  ...      5.727778              0.679451         8.106\n48      1    A           5  ...      5.727778              0.679451         8.106\n\n[5 rows x 9 columns]\n      store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n498       1    A          45  ...     25.938889              0.677602         7.787\n691       1    A          77  ...     15.633333              0.854861         7.866\n2315      4    A          47  ...     -1.755556              0.679715         8.623\n6735     19    A          39  ...     22.333333              1.076766         8.193\n6810     19    A          47  ...     -1.861111              0.881278         8.067\n6815     19    A          47  ...      0.338889              1.010723         7.943\n6820     19    A          48  ...     20.155556              1.038197         7.806\n\n[7 rows x 9 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#counting-categorical-variables",
    "href": "3_Data_Manipulation_with_pandas.html#counting-categorical-variables",
    "title": "Data Manipulation with pandas",
    "section": "Counting categorical variables",
    "text": "Counting categorical variables\nCounting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise. In this exercise, you’ll count the number of each type of store and the number of each department number using the DataFrames you created in the previous exercise:\n#Drop duplicate store/type combinations\nstore_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n\n#Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\nThe store_types and store_depts DataFrames you created in the last exercise are available, and pandas is imported as pd. ### Instructions - Count the number of stores of each store type in store_types. - Count the proportion of stores of each store type in store_types. - Count the number of different departments in store_depts, sorting the counts in descending order. - Count the proportion of different departments in store_depts, sorting the proportions in descending order.\n# Count the number of stores of each type\nstore_counts = store_types.type.value_counts()\nprint(store_counts)\n\n# Get the proportion of stores of each type\nstore_props = store_types.type.value_counts(normalize = True)\nprint(store_props)\n\n# Count the number of each department number and sort\ndept_counts_sorted = store_depts.department.value_counts(sort = True)\nprint(dept_counts_sorted)\n\n# Get the proportion of departments of each number and sort\ndept_props_sorted = store_depts.department.value_counts(sort=True, normalize=True)\nprint(dept_props_sorted)\nA    11\nB     1\nName: type, dtype: int64\nA    0.916667\nB    0.083333\nName: type, dtype: float64\n1     12\n55    12\n72    12\n71    12\n67    12\n      ..\n37    10\n48     8\n50     6\n39     4\n43     2\nName: department, Length: 80, dtype: int64\n1     0.012917\n55    0.012917\n72    0.012917\n71    0.012917\n67    0.012917\n        ...   \n37    0.010764\n48    0.008611\n50    0.006459\n39    0.004306\n43    0.002153\nName: department, Length: 80, dtype: float64",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#what-percent-of-sales-occurred-at-each-store-type",
    "href": "3_Data_Manipulation_with_pandas.html#what-percent-of-sales-occurred-at-each-store-type",
    "title": "Data Manipulation with pandas",
    "section": "What percent of sales occurred at each store type?",
    "text": "What percent of sales occurred at each store type?\nWhile .groupby() is useful, you can calculate grouped summary statistics without it.\nWalmart distinguishes three types of stores: “supercenters,” “discount stores,” and “neighborhood markets,” encoded in this dataset as type “A,” “B,” and “C.” In this exercise, you’ll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart’s total sales were made at each type.\nsales is available and pandas is imported as pd. ### Instructions - Calculate the total weekly_sales over the whole dataset. - Subset for type “A” stores, and calculate their total weekly sales. - Do the same for type “B” and type “C” stores. - Combine the A/B/C results into a list, and divide by sales_all to get the proportion of sales by type.\n# Calc total weekly sales\nsales_all = sales[\"weekly_sales\"].sum()\n\n# Subset for type A stores, calc total weekly sales\nsales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n\n# Subset for type B stores, calc total weekly sales\nsales_B = sales[sales.type==\"B\"].weekly_sales.sum()\n\n# Subset for type C stores, calc total weekly sales\nsales_C = sales[sales.type==\"C\"][\"weekly_sales\"].sum()\n\n# Get proportion for each type\nsales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\nprint(sales_propn_by_type)\n[0.9097747 0.0902253 0.       ]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#calculations-with-.groupby",
    "href": "3_Data_Manipulation_with_pandas.html#calculations-with-.groupby",
    "title": "Data Manipulation with pandas",
    "section": "Calculations with .groupby()",
    "text": "Calculations with .groupby()\nThe .groupby() method makes life much easier. In this exercise, you’ll perform the same calculations as last time, except you’ll use the .groupby() method. You’ll also perform calculations on data grouped by two variables to see if sales differ by store type depending on if it’s a holiday week or not.\nsales is available and pandas is loaded as pd. ### Instructions - Group sales by “type”, take the sum of “weekly_sales”, and store as sales_by_type. - Calculate the proportion of sales at each store type by dividing by the sum of sales_by_type. Assign to sales_propn_by_type.\n# Group by type; calc total weekly sales\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Get proportion for each type\nsales_propn_by_type = sales_by_type / sum(sales.weekly_sales)\nprint(sales_propn_by_type)\ntype\nA    0.909775\nB    0.090225\nName: weekly_sales, dtype: float64\n\nGroup sales by “type” and “is_holiday”, take the sum of weekly_sales, and store as sales_by_type_is_holiday.\n\n# From previous step\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Group by type and is_holiday; calc total weekly sales\nsales_by_type_is_holiday = sales.groupby([\"type\",\"is_holiday\"]).weekly_sales.sum()\nprint(sales_by_type_is_holiday)\ntype  is_holiday\nA     False         2.336927e+08\n      True          2.360181e+04\nB     False         2.317678e+07\n      True          1.621410e+03\nName: weekly_sales, dtype: float64",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#multiple-grouped-summaries",
    "href": "3_Data_Manipulation_with_pandas.html#multiple-grouped-summaries",
    "title": "Data Manipulation with pandas",
    "section": "Multiple grouped summaries",
    "text": "Multiple grouped summaries\nEarlier in this chapter, you saw that the .agg() method is useful to compute multiple statistics on multiple variables. It also works with grouped data. NumPy, which is imported as np, has many different summary statistics functions, including: np.min, np.max, np.mean, and np.median.\nsales is available and pandas is imported as pd. ### Instructions - Import numpy with the alias np. - Get the min, max, mean, and median of weekly_sales for each store type using .groupby() and .agg(). Store this as sales_stats. Make sure to use numpy functions! - Get the min, max, mean, and median of unemployment and fuel_price_usd_per_l for each store type. Store this as unemp_fuel_stats.\n# Import numpy with the alias np\nimport numpy as np\n\n# For each store type, aggregate weekly_sales: get min, max, mean, and median\nsales_stats = sales.groupby(\"type\").weekly_sales.agg([np.min,np.max,np.mean,np.median])\n\n# Print sales_stats\nprint(sales_stats)\n\n# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\nunemp_fuel_stats = sales.groupby(\"type\")[\"unemployment\",\"fuel_price_usd_per_l\"].agg([np.min,np.max,np.mean,np.median])\n\n# Print unemp_fuel_stats\nprint(unemp_fuel_stats)\n        amin       amax          mean    median\ntype                                           \nA    -1098.0  293966.05  23674.667242  11943.92\nB     -798.0  232558.51  25696.678370  13336.08\n     unemployment                   ... fuel_price_usd_per_l                    \n             amin   amax      mean  ...                 amax      mean    median\ntype                                ...                                         \nA           3.879  8.992  7.972611  ...             1.107410  0.744619  0.735455\nB           7.170  9.765  9.279323  ...             1.107674  0.805858  0.803348\n\n[2 rows x 8 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#pivoting-on-one-variable",
    "href": "3_Data_Manipulation_with_pandas.html#pivoting-on-one-variable",
    "title": "Data Manipulation with pandas",
    "section": "Pivoting on one variable",
    "text": "Pivoting on one variable\nPivot tables are the standard way of aggregating data in spreadsheets.\nIn pandas, pivot tables are essentially another way of performing grouped calculations. That is, the .pivot_table() method is an alternative to .groupby().\nIn this exercise, you’ll perform calculations using .pivot_table() to replicate the calculations you performed in the last lesson using .groupby().\nsales is available and pandas is imported as pd. ### Instructions - Get the mean weekly_sales by type using .pivot_table() and store as mean_sales_by_type. - Get the mean and median (using NumPy functions) of weekly_sales by type using .pivot_table() and store as mean_med_sales_by_type. - Get the mean of weekly_sales by type and is_holiday using .pivot_table() and store as mean_sales_by_type_holiday.\n# Pivot for mean weekly_sales for each store type\nmean_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index = \"type\")\n\n# Print mean_sales_by_type\nprint(mean_sales_by_type)\n      weekly_sales\ntype              \nA     23674.667242\nB     25696.678370\n# Import NumPy as np\nimport numpy as np\n\n# Pivot for mean and median weekly_sales for each store type\nmean_med_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index = \"type\", aggfunc = [np.mean, np.median])\n\n# Print mean_med_sales_by_type\nprint(mean_med_sales_by_type)\n              mean       median\n      weekly_sales weekly_sales\ntype                           \nA     23674.667242     11943.92\nB     25696.678370     13336.08\n# Pivot for mean weekly_sales by store type and holiday \nmean_sales_by_type_holiday = sales.pivot_table(values = \"weekly_sales\",index = [\"type\"],columns = [\"is_holiday\"])\n\n# Print mean_sales_by_type_holiday\nprint(mean_sales_by_type_holiday)\nis_holiday         False      True \ntype                               \nA           23768.583523  590.04525\nB           25751.980533  810.70500",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#fill-in-missing-values-and-sum-values-with-pivot-tables",
    "href": "3_Data_Manipulation_with_pandas.html#fill-in-missing-values-and-sum-values-with-pivot-tables",
    "title": "Data Manipulation with pandas",
    "section": "Fill in missing values and sum values with pivot tables",
    "text": "Fill in missing values and sum values with pivot tables\nThe .pivot_table() method has several useful arguments, including fill_value and margins.\nfill_value replaces missing values with a real value (known as imputation). What to replace missing values with is a topic big enough to have its own course (Dealing with Missing Data in Python), but the simplest thing to do is to substitute a dummy value.\nmargins is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents.\nIn this exercise, you’ll practice using these arguments to up your pivot table skills, which will help you crunch numbers more efficiently!\nsales is available and pandas is imported as pd. ### Instructions - Print the mean weekly_sales by department and type, filling in any missing values with 0. - Print the mean weekly_sales by department and type, filling in any missing values with 0 and summing all rows and columns.\n# Print mean weekly_sales by department and type; fill missing values with 0\nprint(sales.pivot_table(values = \"weekly_sales\", index=\"department\", columns = \"type\", fill_value = 0))\ntype                    A              B\ndepartment                              \n1            30961.725379   44050.626667\n2            67600.158788  112958.526667\n3            17160.002955   30580.655000\n4            44285.399091   51219.654167\n5            34821.011364   63236.875000\n...                   ...            ...\n95          123933.787121   77082.102500\n96           21367.042857    9528.538333\n97           28471.266970    5828.873333\n98           12875.423182     217.428333\n99             379.123659       0.000000\n\n[80 rows x 2 columns]\n# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\nprint(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value = 0, margins=True))\ntype                   A              B           All\ndepartment                                           \n1           30961.725379   44050.626667  32052.467153\n2           67600.158788  112958.526667  71380.022778\n3           17160.002955   30580.655000  18278.390625\n4           44285.399091   51219.654167  44863.253681\n5           34821.011364   63236.875000  37189.000000\n...                  ...            ...           ...\n96          21367.042857    9528.538333  20337.607681\n97          28471.266970    5828.873333  26584.400833\n98          12875.423182     217.428333  11820.590278\n99            379.123659       0.000000    379.123659\nAll         23674.667242   25696.678370  23843.950149\n\n[81 rows x 3 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#setting-and-removing-indexes",
    "href": "3_Data_Manipulation_with_pandas.html#setting-and-removing-indexes",
    "title": "Data Manipulation with pandas",
    "section": "Setting and removing indexes",
    "text": "Setting and removing indexes\npandas allows you to designate columns as an index. This enables cleaner code when taking subsets (as well as providing more efficient lookup under some circumstances).\nIn this chapter, you’ll be exploring temperatures, a DataFrame of average temperatures in cities around the world. pandas is loaded as pd. ### Instructions - Look at temperatures. - Set the index of temperatures to “city”, assigning to temperatures_ind. - Look at temperatures_ind. How is it different from temperatures? - Reset the index of temperatures_ind, keeping its contents. - Reset the index of temperatures_ind, dropping its contents.\n# Look at temperatures\nprint(temperatures.head())\n\n# Set the index of temperatures to city\ntemperatures_ind = temperatures.set_index(\"city\")\n\n# Look at temperatures_ind\nprint(temperatures_ind.head())\n\n# Reset the temperatures_ind index, keeping its contents\nprint(temperatures_ind.reset_index(drop=False))\n\n# Reset the temperatures_ind index, dropping its contents\nprint(temperatures_ind.reset_index(drop = True))\n         date     city        country  avg_temp_c\n0  2000-01-01  Abidjan  Côte D'Ivoire      27.293\n1  2000-02-01  Abidjan  Côte D'Ivoire      27.685\n2  2000-03-01  Abidjan  Côte D'Ivoire      29.061\n3  2000-04-01  Abidjan  Côte D'Ivoire      28.162\n4  2000-05-01  Abidjan  Côte D'Ivoire      27.547\n               date        country  avg_temp_c\ncity                                          \nAbidjan  2000-01-01  Côte D'Ivoire      27.293\nAbidjan  2000-02-01  Côte D'Ivoire      27.685\nAbidjan  2000-03-01  Côte D'Ivoire      29.061\nAbidjan  2000-04-01  Côte D'Ivoire      28.162\nAbidjan  2000-05-01  Côte D'Ivoire      27.547\n          city        date        country  avg_temp_c\n0      Abidjan  2000-01-01  Côte D'Ivoire      27.293\n1      Abidjan  2000-02-01  Côte D'Ivoire      27.685\n2      Abidjan  2000-03-01  Côte D'Ivoire      29.061\n3      Abidjan  2000-04-01  Côte D'Ivoire      28.162\n4      Abidjan  2000-05-01  Côte D'Ivoire      27.547\n...        ...         ...            ...         ...\n16495     Xian  2013-05-01          China      18.979\n16496     Xian  2013-06-01          China      23.522\n16497     Xian  2013-07-01          China      25.251\n16498     Xian  2013-08-01          China      24.528\n16499     Xian  2013-09-01          China         NaN\n\n[16500 rows x 4 columns]\n             date        country  avg_temp_c\n0      2000-01-01  Côte D'Ivoire      27.293\n1      2000-02-01  Côte D'Ivoire      27.685\n2      2000-03-01  Côte D'Ivoire      29.061\n3      2000-04-01  Côte D'Ivoire      28.162\n4      2000-05-01  Côte D'Ivoire      27.547\n...           ...            ...         ...\n16495  2013-05-01          China      18.979\n16496  2013-06-01          China      23.522\n16497  2013-07-01          China      25.251\n16498  2013-08-01          China      24.528\n16499  2013-09-01          China         NaN\n\n[16500 rows x 3 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#subsetting-with-.loc",
    "href": "3_Data_Manipulation_with_pandas.html#subsetting-with-.loc",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting with .loc[]",
    "text": "Subsetting with .loc[]\nThe killer feature for indexes is .loc[]: a subsetting method that accepts index values. When you pass it a single argument, it will take a subset of rows.\nThe code for subsetting using .loc[] can be easier to read than standard square bracket subsetting, which can make your code less burdensome to maintain.\npandas is loaded as pd. temperatures and temperatures_ind are available; the latter is indexed by city. ### Instructions - Create a list called cities that contains “Moscow” and “Saint Petersburg”. - Use [] subsetting to filter temperatures for rows where the city column takes a value in the cities list. - Use .loc[] subsetting to filter temperatures_ind for rows where the city is in the cities list.\n# Make a list of cities to subset on\ncities = [\"Moscow\", \"Saint Petersburg\"]\n\n# Subset temperatures using square brackets\nprint(temperatures[temperatures.city.isin(cities)])\n\n# Subset temperatures_ind using .loc[]\nprint(temperatures_ind.loc[cities])\n             date              city country  avg_temp_c\n10725  2000-01-01            Moscow  Russia      -7.313\n10726  2000-02-01            Moscow  Russia      -3.551\n10727  2000-03-01            Moscow  Russia      -1.661\n10728  2000-04-01            Moscow  Russia      10.096\n10729  2000-05-01            Moscow  Russia      10.357\n...           ...               ...     ...         ...\n13360  2013-05-01  Saint Petersburg  Russia      12.355\n13361  2013-06-01  Saint Petersburg  Russia      17.185\n13362  2013-07-01  Saint Petersburg  Russia      17.234\n13363  2013-08-01  Saint Petersburg  Russia      17.153\n13364  2013-09-01  Saint Petersburg  Russia         NaN\n\n[330 rows x 4 columns]\n                        date country  avg_temp_c\ncity                                            \nMoscow            2000-01-01  Russia      -7.313\nMoscow            2000-02-01  Russia      -3.551\nMoscow            2000-03-01  Russia      -1.661\nMoscow            2000-04-01  Russia      10.096\nMoscow            2000-05-01  Russia      10.357\n...                      ...     ...         ...\nSaint Petersburg  2013-05-01  Russia      12.355\nSaint Petersburg  2013-06-01  Russia      17.185\nSaint Petersburg  2013-07-01  Russia      17.234\nSaint Petersburg  2013-08-01  Russia      17.153\nSaint Petersburg  2013-09-01  Russia         NaN\n\n[330 rows x 3 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#setting-multi-level-indexes",
    "href": "3_Data_Manipulation_with_pandas.html#setting-multi-level-indexes",
    "title": "Data Manipulation with pandas",
    "section": "Setting multi-level indexes",
    "text": "Setting multi-level indexes\nIndexes can also be made out of multiple columns, forming a multi-level index (sometimes called a hierarchical index). There is a trade-off to using these.\nThe benefit is that multi-level indexes make it more natural to reason about nested categorical variables. For example, in a clinical trial, you might have control and treatment groups. Then each test subject belongs to one or another group, and we can say that a test subject is nested inside the treatment group. Similarly, in the temperature dataset, the city is located in the country, so we can say a city is nested inside the country.\nThe main downside is that the code for manipulating indexes is different from the code for manipulating columns, so you have to learn two syntaxes and keep track of how your data is represented.\npandas is loaded as pd. temperatures is available. ### Instructions - Set the index of temperatures to the “country” and “city” columns, and assign this to temperatures_ind. - Specify two country/city pairs to keep: “Brazil”/“Rio De Janeiro” and “Pakistan”/“Lahore”, assigning to rows_to_keep. - Print and subset temperatures_ind for rows_to_keep using .loc[].\n# Index temperatures by country & city\ntemperatures_ind = temperatures.set_index([\"country\",\"city\"])\n\n# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\nrows_to_keep = [(\"Brazil\", \"Rio De Janeiro\") , (\"Pakistan\", \"Lahore\")]\n\n# Subset for rows to keep\nprint(temperatures_ind.loc[rows_to_keep])\n                               date  avg_temp_c\ncountry  city                                  \nBrazil   Rio De Janeiro  2000-01-01      25.974\n         Rio De Janeiro  2000-02-01      26.699\n         Rio De Janeiro  2000-03-01      26.270\n         Rio De Janeiro  2000-04-01      25.750\n         Rio De Janeiro  2000-05-01      24.356\n...                             ...         ...\nPakistan Lahore          2013-05-01      33.457\n         Lahore          2013-06-01      34.456\n         Lahore          2013-07-01      33.279\n         Lahore          2013-08-01      31.511\n         Lahore          2013-09-01         NaN\n\n[330 rows x 2 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#sorting-by-index-values",
    "href": "3_Data_Manipulation_with_pandas.html#sorting-by-index-values",
    "title": "Data Manipulation with pandas",
    "section": "Sorting by index values",
    "text": "Sorting by index values\nPreviously, you changed the order of the rows in a DataFrame by calling .sort_values(). It’s also useful to be able to sort by elements in the index. For this, you need to use .sort_index().\npandas is loaded as pd. temperatures_ind has a multi-level index of country and city, and is available. ### Instructions - Sort temperatures_ind by the index values. - Sort temperatures_ind by the index values at the “city” level. - Sort temperatures_ind by ascending country then descending city.\n# Sort temperatures_ind by index values\nprint(temperatures_ind.sort_index())\n\n# Sort temperatures_ind by index values at the city level\nprint(temperatures_ind.sort_index(level=\"city\"))\n\n# Sort temperatures_ind by country then descending city\nprint(temperatures_ind.sort_index(level=[\"country\",\"city\"],ascending = (True,False)))\n                          date  avg_temp_c\ncountry     city                          \nAfghanistan Kabul   2000-01-01       3.326\n            Kabul   2000-02-01       3.454\n            Kabul   2000-03-01       9.612\n            Kabul   2000-04-01      17.925\n            Kabul   2000-05-01      24.658\n...                        ...         ...\nZimbabwe    Harare  2013-05-01      18.298\n            Harare  2013-06-01      17.020\n            Harare  2013-07-01      16.299\n            Harare  2013-08-01      19.232\n            Harare  2013-09-01         NaN\n\n[16500 rows x 2 columns]\n                             date  avg_temp_c\ncountry       city                           \nCôte D'Ivoire Abidjan  2000-01-01      27.293\n              Abidjan  2000-02-01      27.685\n              Abidjan  2000-03-01      29.061\n              Abidjan  2000-04-01      28.162\n              Abidjan  2000-05-01      27.547\n...                           ...         ...\nChina         Xian     2013-05-01      18.979\n              Xian     2013-06-01      23.522\n              Xian     2013-07-01      25.251\n              Xian     2013-08-01      24.528\n              Xian     2013-09-01         NaN\n\n[16500 rows x 2 columns]\n                          date  avg_temp_c\ncountry     city                          \nAfghanistan Kabul   2000-01-01       3.326\n            Kabul   2000-02-01       3.454\n            Kabul   2000-03-01       9.612\n            Kabul   2000-04-01      17.925\n            Kabul   2000-05-01      24.658\n...                        ...         ...\nZimbabwe    Harare  2013-05-01      18.298\n            Harare  2013-06-01      17.020\n            Harare  2013-07-01      16.299\n            Harare  2013-08-01      19.232\n            Harare  2013-09-01         NaN\n\n[16500 rows x 2 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#slicing-index-values",
    "href": "3_Data_Manipulation_with_pandas.html#slicing-index-values",
    "title": "Data Manipulation with pandas",
    "section": "Slicing index values",
    "text": "Slicing index values\nSlicing lets you select consecutive elements of an object using first:last syntax. DataFrames can be sliced by index values or by row/column number; we’ll start with the first case. This involves slicing inside the .loc[] method.\nCompared to slicing lists, there are a few things to remember.\nYou can only slice an index if the index is sorted (using .sort_index()).\nTo slice at the outer level, first and last can be strings.\nTo slice at inner levels, first and last should be tuples.\nIf you pass a single slice to .loc[], it will slice the rows.\npandas is loaded as pd. temperatures_ind has country and city in the index, and is available. ### Instructions - Sort the index of temperatures_ind. - Use slicing with .loc[] to get these subsets: - from Pakistan to Russia. - from Lahore to Moscow. (This will return nonsense.) - from Pakistan, Lahore to Russia, Moscow.\n# Sort the index of temperatures_ind\ntemperatures_srt = temperatures_ind.sort_index()\n\n# Subset rows from Pakistan to Russia\nprint(temperatures_srt.loc[\"Pakistan\" : \"Russia\"])\n\n# Try to subset rows from Lahore to Moscow\nprint(temperatures_srt.loc[\"Lahore\" : \"Moscow\"])\n\n# Subset rows from Pakistan, Lahore to Russia, Moscow\nprint(temperatures_srt.loc[(\"Pakistan\", \"Lahore\") : (\"Russia\", \"Moscow\")])\n                                 date  avg_temp_c\ncountry  city                                    \nPakistan Faisalabad        2000-01-01      12.792\n         Faisalabad        2000-02-01      14.339\n         Faisalabad        2000-03-01      20.309\n         Faisalabad        2000-04-01      29.072\n         Faisalabad        2000-05-01      34.845\n...                               ...         ...\nRussia   Saint Petersburg  2013-05-01      12.355\n         Saint Petersburg  2013-06-01      17.185\n         Saint Petersburg  2013-07-01      17.234\n         Saint Petersburg  2013-08-01      17.153\n         Saint Petersburg  2013-09-01         NaN\n\n[1155 rows x 2 columns]\n                          date  avg_temp_c\ncountry city                              \nMexico  Mexico      2000-01-01      12.694\n        Mexico      2000-02-01      14.677\n        Mexico      2000-03-01      17.376\n        Mexico      2000-04-01      18.294\n        Mexico      2000-05-01      18.562\n...                        ...         ...\nMorocco Casablanca  2013-05-01      19.217\n        Casablanca  2013-06-01      23.649\n        Casablanca  2013-07-01      27.488\n        Casablanca  2013-08-01      27.952\n        Casablanca  2013-09-01         NaN\n\n[330 rows x 2 columns]\n                       date  avg_temp_c\ncountry  city                          \nPakistan Lahore  2000-01-01      12.792\n         Lahore  2000-02-01      14.339\n         Lahore  2000-03-01      20.309\n         Lahore  2000-04-01      29.072\n         Lahore  2000-05-01      34.845\n...                     ...         ...\nRussia   Moscow  2013-05-01      16.152\n         Moscow  2013-06-01      18.718\n         Moscow  2013-07-01      18.136\n         Moscow  2013-08-01      17.485\n         Moscow  2013-09-01         NaN\n\n[660 rows x 2 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#slicing-in-both-directions",
    "href": "3_Data_Manipulation_with_pandas.html#slicing-in-both-directions",
    "title": "Data Manipulation with pandas",
    "section": "Slicing in both directions",
    "text": "Slicing in both directions\nYou’ve seen slicing DataFrames by rows and by columns, but since DataFrames are two-dimensional objects, it is often natural to slice both dimensions at once. That is, by passing two arguments to .loc[], you can subset by rows and columns in one go.\npandas is loaded as pd. temperatures_srt is indexed by country and city, has a sorted index, and is available. ### Instructions - Use .loc[] slicing to subset rows from India, Hyderabad to Iraq, Baghdad. - Use .loc[] slicing to subset columns from date to avg_temp_c. - Slice in both directions at once from Hyderabad to Baghdad, and date to avg_temp_c.\n# Subset rows from India, Hyderabad to Iraq, Baghdad\nprint(temperatures_srt.loc[(\"India\", \"Hyderabad\"): (\"Iraq\", \"Baghdad\")])\n\n# Subset columns from date to avg_temp_c\nprint(temperatures_srt.loc[:,\"date\":\"avg_temp_c\"])\n\n# Subset in both directions at once\nprint(temperatures_srt.loc[(\"India\", \"Hyderabad\"): (\"Iraq\", \"Baghdad\"),\"date\":\"avg_temp_c\"])\n                         date  avg_temp_c\ncountry city                             \nIndia   Hyderabad  2000-01-01      23.779\n        Hyderabad  2000-02-01      25.826\n        Hyderabad  2000-03-01      28.821\n        Hyderabad  2000-04-01      32.698\n        Hyderabad  2000-05-01      32.438\n...                       ...         ...\nIraq    Baghdad    2013-05-01      28.673\n        Baghdad    2013-06-01      33.803\n        Baghdad    2013-07-01      36.392\n        Baghdad    2013-08-01      35.463\n        Baghdad    2013-09-01         NaN\n\n[2145 rows x 2 columns]\n                          date  avg_temp_c\ncountry     city                          \nAfghanistan Kabul   2000-01-01       3.326\n            Kabul   2000-02-01       3.454\n            Kabul   2000-03-01       9.612\n            Kabul   2000-04-01      17.925\n            Kabul   2000-05-01      24.658\n...                        ...         ...\nZimbabwe    Harare  2013-05-01      18.298\n            Harare  2013-06-01      17.020\n            Harare  2013-07-01      16.299\n            Harare  2013-08-01      19.232\n            Harare  2013-09-01         NaN\n\n[16500 rows x 2 columns]\n                         date  avg_temp_c\ncountry city                             \nIndia   Hyderabad  2000-01-01      23.779\n        Hyderabad  2000-02-01      25.826\n        Hyderabad  2000-03-01      28.821\n        Hyderabad  2000-04-01      32.698\n        Hyderabad  2000-05-01      32.438\n...                       ...         ...\nIraq    Baghdad    2013-05-01      28.673\n        Baghdad    2013-06-01      33.803\n        Baghdad    2013-07-01      36.392\n        Baghdad    2013-08-01      35.463\n        Baghdad    2013-09-01         NaN\n\n[2145 rows x 2 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#slicing-time-series",
    "href": "3_Data_Manipulation_with_pandas.html#slicing-time-series",
    "title": "Data Manipulation with pandas",
    "section": "Slicing time series",
    "text": "Slicing time series\nSlicing is particularly useful for time series since it’s a common thing to want to filter for data within a date range. Add the date column to the index, then use .loc[] to perform the subsetting. The important thing to remember is to keep your dates in ISO 8601 format, that is, “yyyy-mm-dd” for year-month-day, “yyyy-mm” for year-month, and “yyyy” for year.\nRecall from Chapter 1 that you can combine multiple Boolean conditions using logical operators, such as &. To do so in one line of code, you’ll need to add parentheses () around each condition.\npandas is loaded as pd and temperatures, with no index, is available. ### Instructions - Use Boolean conditions, not .isin() or .loc[], and the full date “yyyy-mm-dd”, to subset temperatures for rows in 2010 and 2011 and print the results. - Set the index of temperatures to the date column and sort it. - Use .loc[] to subset temperatures_ind for rows in 2010 and 2011. - Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011.\n# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\ntemperatures_bool = temperatures.loc[(temperatures[\"date\"] &gt;= \"2010-01-01\") & (temperatures[\"date\"] &lt;= \"2011-12-31\")]\nprint(temperatures_bool)\n\n# Set date as the index and sort the index\ntemperatures_ind = temperatures.set_index(\"date\").sort_index()\n\n# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\nprint(temperatures_ind.loc[\"2010\":\"2011\",:])\n\n# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\nprint(temperatures_ind.loc[\"2010-08\":\"2011-02\",:])\n             date     city        country  avg_temp_c\n120    2010-01-01  Abidjan  Côte D'Ivoire      28.270\n121    2010-02-01  Abidjan  Côte D'Ivoire      29.262\n122    2010-03-01  Abidjan  Côte D'Ivoire      29.596\n123    2010-04-01  Abidjan  Côte D'Ivoire      29.068\n124    2010-05-01  Abidjan  Côte D'Ivoire      28.258\n...           ...      ...            ...         ...\n16474  2011-08-01     Xian          China      23.069\n16475  2011-09-01     Xian          China      16.775\n16476  2011-10-01     Xian          China      12.587\n16477  2011-11-01     Xian          China       7.543\n16478  2011-12-01     Xian          China      -0.490\n\n[2400 rows x 4 columns]\n                  city    country  avg_temp_c\ndate                                         \n2010-01-01  Faisalabad   Pakistan      11.810\n2010-01-01   Melbourne  Australia      20.016\n2010-01-01   Chongqing      China       7.921\n2010-01-01   São Paulo     Brazil      23.738\n2010-01-01   Guangzhou      China      14.136\n...                ...        ...         ...\n2010-12-01     Jakarta  Indonesia      26.602\n2010-12-01       Gizeh      Egypt      16.530\n2010-12-01      Nagpur      India      19.120\n2010-12-01      Sydney  Australia      19.559\n2010-12-01    Salvador     Brazil      26.265\n\n[1200 rows x 3 columns]\n                     city        country  avg_temp_c\ndate                                                \n2010-08-01       Calcutta          India      30.226\n2010-08-01           Pune          India      24.941\n2010-08-01          Izmir         Turkey      28.352\n2010-08-01        Tianjin          China      25.543\n2010-08-01         Manila    Philippines      27.101\n...                   ...            ...         ...\n2011-01-01  Dar Es Salaam       Tanzania      28.541\n2011-01-01        Nairobi          Kenya      17.768\n2011-01-01    Addis Abeba       Ethiopia      17.708\n2011-01-01        Nanjing          China       0.144\n2011-01-01       New York  United States      -4.463\n\n[600 rows x 3 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#subsetting-by-rowcolumn-number",
    "href": "3_Data_Manipulation_with_pandas.html#subsetting-by-rowcolumn-number",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting by row/column number",
    "text": "Subsetting by row/column number\nThe most common ways to subset rows are the ways we’ve previously discussed: using a Boolean condition or by index labels. However, it is also occasionally useful to pass row numbers.\nThis is done using .iloc[], and like .loc[], it can take two arguments to let you subset by rows and columns.\npandas is loaded as pd. temperatures (without an index) is available. ### Instructions\nUse .iloc[] on temperatures to take subsets.\n\nGet the 23rd row, 2nd column (index positions 22 and 1).\nGet the first 5 rows (index positions 0 to 5).\nGet all rows, columns 3 and 4 (index positions 2 to 4).\nGet the first 5 rows, columns 3 and 4.\n\n# Get 23rd row, 2nd column (index 22, 1)\nprint(temperatures.iloc[22,1])\n\n# Use slicing to get the first 5 rows\nprint(temperatures.iloc[:5,:])\n\n# Use slicing to get columns 3 to 4\nprint(temperatures.iloc[:,2:4])\n\n# Use slicing in both directions at once\nprint(temperatures.iloc[:5,2:4])\nAbidjan\n         date     city        country  avg_temp_c\n0  2000-01-01  Abidjan  Côte D'Ivoire      27.293\n1  2000-02-01  Abidjan  Côte D'Ivoire      27.685\n2  2000-03-01  Abidjan  Côte D'Ivoire      29.061\n3  2000-04-01  Abidjan  Côte D'Ivoire      28.162\n4  2000-05-01  Abidjan  Côte D'Ivoire      27.547\n             country  avg_temp_c\n0      Côte D'Ivoire      27.293\n1      Côte D'Ivoire      27.685\n2      Côte D'Ivoire      29.061\n3      Côte D'Ivoire      28.162\n4      Côte D'Ivoire      27.547\n...              ...         ...\n16495          China      18.979\n16496          China      23.522\n16497          China      25.251\n16498          China      24.528\n16499          China         NaN\n\n[16500 rows x 2 columns]\n         country  avg_temp_c\n0  Côte D'Ivoire      27.293\n1  Côte D'Ivoire      27.685\n2  Côte D'Ivoire      29.061\n3  Côte D'Ivoire      28.162\n4  Côte D'Ivoire      27.547",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#pivot-temperature-by-city-and-year",
    "href": "3_Data_Manipulation_with_pandas.html#pivot-temperature-by-city-and-year",
    "title": "Data Manipulation with pandas",
    "section": "Pivot temperature by city and year",
    "text": "Pivot temperature by city and year\nIt’s interesting to see how temperatures for each city change over time—looking at every month results in a big table, which can be tricky to reason about. Instead, let’s look at how temperatures change by year.\nYou can access the components of a date (year, month and day) using code of the form dataframe[“column”].dt.component. For example, the month component is dataframe[“column”].dt.month, and the year component is dataframe[“column”].dt.year.\nOnce you have the year column, you can create a pivot table with the data aggregated by city and year, which you’ll explore in the coming exercises.\npandas is loaded as pd. temperatures is available. ### Instructions - Add a year column to temperatures, from the year component of the date column. - Make a pivot table of the avg_temp_c column, with country and city as rows, and year as columns. Assign to temp_by_country_city_vs_year, and look at the result.\ntemperatures.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\ncity\ncountry\navg_temp_c\n\n\n\n\n0\n2000-01-01\nAbidjan\nCôte D’Ivoire\n27.293\n\n\n1\n2000-02-01\nAbidjan\nCôte D’Ivoire\n27.685\n\n\n2\n2000-03-01\nAbidjan\nCôte D’Ivoire\n29.061\n\n\n\n\n# Convert the 'date' column to datetime type\ntemperatures['date'] = pd.to_datetime(temperatures['date'])\n# Add a year column to temperatures\ntemperatures[\"year\"] = temperatures.date.dt.year\n\n# Pivot avg_temp_c by country and city vs year\ntemp_by_country_city_vs_year = temperatures.pivot_table(values = \"avg_temp_c\", index = [\"country\",\"city\"], columns = \"year\")\n\n# See the result\nprint(temp_by_country_city_vs_year)\nyear                                 2000       2001  ...       2012       2013\ncountry       city                                    ...                      \nAfghanistan   Kabul             15.822667  15.847917  ...  14.510333  16.206125\nAngola        Luanda            24.410333  24.427083  ...  24.240083  24.553875\nAustralia     Melbourne         14.320083  14.180000  ...  14.268667  14.741500\n              Sydney            17.567417  17.854500  ...  17.474333  18.089750\nBangladesh    Dhaka             25.905250  25.931250  ...  26.283583  26.587000\n...                                   ...        ...  ...        ...        ...\nUnited States Chicago           11.089667  11.703083  ...  12.821250  11.586889\n              Los Angeles       16.643333  16.466250  ...  17.089583  18.120667\n              New York           9.969083  10.931000  ...  11.971500  12.163889\nVietnam       Ho Chi Minh City  27.588917  27.831750  ...  28.248750  28.455000\nZimbabwe      Harare            20.283667  20.861000  ...  20.523333  19.756500\n\n[100 rows x 14 columns]",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#subsetting-pivot-tables",
    "href": "3_Data_Manipulation_with_pandas.html#subsetting-pivot-tables",
    "title": "Data Manipulation with pandas",
    "section": "Subsetting pivot tables",
    "text": "Subsetting pivot tables\nA pivot table is just a DataFrame with sorted indexes, so the techniques you have learned already can be used to subset them. In particular, the .loc[] + slicing combination is often helpful.\npandas is loaded as pd. temp_by_country_city_vs_year is available. ### Instructions\nUse .loc[] on temp_by_country_city_vs_year to take subsets.\n\nFrom Egypt to India.\nFrom Egypt, Cairo to India, Delhi.\nFrom Egypt, Cairo to India, Delhi, and 2005 to 2010.\n\n# Subset for Egypt to India\ntemp_by_country_city_vs_year.loc[\"Egypt\" : \"India\"]\n\n# Subset for Egypt, Cairo to India, Delhi\ntemp_by_country_city_vs_year.loc[(\"Egypt\",\"Cairo\") : (\"India\",\"Delhi\")]\n\n# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\ntemp_by_country_city_vs_year.loc[(\"Egypt\",\"Cairo\") : (\"India\",\"Delhi\"),\"2005\":\"2010\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\n2005\n2006\n2007\n2008\n2009\n2010\n\n\n\n\ncountry\ncity\n\n\n\n\n\n\n\n\nEgypt\nCairo\n22.006500\n22.050000\n22.361000\n22.644500\n22.625000\n23.718250\n\n\nGizeh\n22.006500\n22.050000\n22.361000\n22.644500\n22.625000\n23.718250\n\n\n\nEthiopia\nAddis Abeba\n18.312833\n18.427083\n18.142583\n18.165000\n18.765333\n18.298250\n\n\nFrance\nParis\n11.552917\n11.788500\n11.750833\n11.278250\n11.464083\n10.409833\n\n\nGermany\nBerlin\n9.919083\n10.545333\n10.883167\n10.657750\n10.062500\n8.606833\n\n\nIndia\nAhmadabad\n26.828083\n27.282833\n27.511167\n27.048500\n28.095833\n28.017833\n\n\nBangalore\n25.476500\n25.418250\n25.464333\n25.352583\n25.725750\n25.705250\n\n\n\nBombay\n27.035750\n27.381500\n27.634667\n27.177750\n27.844500\n27.765417\n\n\n\nCalcutta\n26.729167\n26.986250\n26.584583\n26.522333\n27.153250\n27.288833\n\n\n\nDelhi\n25.716083\n26.365917\n26.145667\n25.675000\n26.554250\n26.520250",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#calculating-on-a-pivot-table",
    "href": "3_Data_Manipulation_with_pandas.html#calculating-on-a-pivot-table",
    "title": "Data Manipulation with pandas",
    "section": "Calculating on a pivot table",
    "text": "Calculating on a pivot table\nPivot tables are filled with summary statistics, but they are only a first step to finding something insightful. Often you’ll need to perform further calculations on them. A common thing to do is to find the rows or columns where the highest or lowest value occurs.\nRecall from Chapter 1 that you can easily subset a Series or DataFrame to find rows of interest using a logical condition inside of square brackets. For example: series[series &gt; value].\npandas is loaded as pd and the DataFrame temp_by_country_city_vs_year is available. ### Instructions - Calculate the mean temperature for each year, assigning to mean_temp_by_year. - Filter mean_temp_by_year for the year that had the highest mean temperature. - Calculate the mean temperature for each city (across columns), assigning to mean_temp_by_city. - Filter mean_temp_by_city for the city that had the lowest mean temperature.\n# Get the worldwide mean temp by year\nmean_temp_by_year = temp_by_country_city_vs_year.mean()\n\n# Filter for the year that had the highest mean temp\nprint(mean_temp_by_year[mean_temp_by_year==mean_temp_by_year.max()])\n\n# Get the mean temp by city\nmean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n\n# Filter for the city that had the lowest mean temp\nprint(mean_temp_by_city[mean_temp_by_city==mean_temp_by_city.min()])\nyear\n2013    20.312285\ndtype: float64\ncountry  city  \nChina    Harbin    4.876551\ndtype: float64",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#which-avocado-size-is-most-popular",
    "href": "3_Data_Manipulation_with_pandas.html#which-avocado-size-is-most-popular",
    "title": "Data Manipulation with pandas",
    "section": "Which avocado size is most popular?",
    "text": "Which avocado size is most popular?\nAvocados are increasingly popular and delicious in guacamole and on toast. The Hass Avocado Board keeps track of avocado supply and demand across the USA, including the sales of three different sizes of avocado. In this exercise, you’ll use a bar plot to figure out which size is the most popular.\nBar plots are great for revealing relationships between categorical (size) and numeric (number sold) variables, but you’ll often have to manipulate your data first in order to get the numbers you need for plotting.\npandas has been imported as pd, and avocados is available. ### Instructions - Print the head of the avocados dataset. What columns are available? - For each avocado size group, calculate the total number sold, storing as nb_sold_by_size. - Create a bar plot of the number of avocados sold by size. - Show the plot.\navocados = pd.read_csv(\"datasets/avocado.csv\")\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Look at the first few rows of data\nprint(avocados.head())\n\n# Get the total number of avocados sold of each size\nnb_sold_by_size = avocados.groupby(\"size\")[\"nb_sold\"].sum()\n\n# Create a bar plot of the number of avocados sold by size\nnb_sold_by_size.plot(kind=\"bar\")\n\n# Show the plot\nplt.show()\n         date          type  year  avg_price   size     nb_sold\n0  2015-12-27  conventional  2015       0.95  small  9626901.09\n1  2015-12-20  conventional  2015       0.98  small  8710021.76\n2  2015-12-13  conventional  2015       0.93  small  9855053.66\n3  2015-12-06  conventional  2015       0.89  small  9405464.36\n4  2015-11-29  conventional  2015       0.99  small  8094803.56\n\n\n\npng",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#changes-in-sales-over-time",
    "href": "3_Data_Manipulation_with_pandas.html#changes-in-sales-over-time",
    "title": "Data Manipulation with pandas",
    "section": "Changes in sales over time",
    "text": "Changes in sales over time\nLine plots are designed to visualize the relationship between two numeric variables, where each data values is connected to the next one. They are especially useful for visualizing the change in a number over time since each time point is naturally connected to the next time point. In this exercise, you’ll visualize the change in avocado sales over three years.\npandas has been imported as pd, and avocados is available. ### Instructions - Get the total number of avocados sold on each date. The DataFrame has two rows for each date—one for organic, and one for conventional. Save this as nb_sold_by_date. - Create a line plot of the number of avocados sold.\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Get the total number of avocados sold on each date\nnb_sold_by_date = avocados.groupby(\"date\").nb_sold.sum()\n\n# Create a line plot of the number of avocados sold by date\nnb_sold_by_date.plot(kind=\"line\")\n\n# Show the plot\nplt.show()\n\n\n\npng",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#avocado-supply-and-demand",
    "href": "3_Data_Manipulation_with_pandas.html#avocado-supply-and-demand",
    "title": "Data Manipulation with pandas",
    "section": "Avocado supply and demand",
    "text": "Avocado supply and demand\nScatter plots are ideal for visualizing relationships between numerical variables. In this exercise, you’ll compare the number of avocados sold to average price and see if they’re at all related. If they’re related, you may be able to use one number to predict the other.\nmatplotlib.pyplot has been imported as plt, pandas has been imported as pd, and avocados is available. ### Instructions - Create a scatter plot with nb_sold on the x-axis and avg_price on the y-axis. Title it “Number of avocados sold vs. average price”. - Show the plot.\n# Scatter plot of avg_price vs. nb_sold with title\navocados.plot(y=\"avg_price\", x=\"nb_sold\", kind = \"scatter\", title=\"Number of avocados sold vs. average price\")\n\n# Show the plot\nplt.show()\n\n\n\npng",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#price-of-conventional-vs.-organic-avocados",
    "href": "3_Data_Manipulation_with_pandas.html#price-of-conventional-vs.-organic-avocados",
    "title": "Data Manipulation with pandas",
    "section": "Price of conventional vs. organic avocados",
    "text": "Price of conventional vs. organic avocados\nCreating multiple plots for different subsets of data allows you to compare groups. In this exercise, you’ll create multiple histograms to compare the prices of conventional and organic avocados.\nmatplotlib.pyplot has been imported as plt and pandas has been imported as pd. ### Instructions - Subset avocados for the conventional type, and the average price column. Create a histogram. - Create a histogram of avg_price for organic type avocados. - Add a legend to your plot, with the names “conventional” and “organic”. - Show your plot.\n# Histogram of conventional avg_price \navocados[avocados.type==\"conventional\"][\"avg_price\"].hist()\n\n# Histogram of organic avg_price\navocados[avocados.type==\"organic\"][\"avg_price\"].hist()\n\n# Add a legend\nplt.legend([\"conventional\" , \"organic\"])\n\n# Show the plot\nplt.show()\n\n\n\npng",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#finding-missing-values",
    "href": "3_Data_Manipulation_with_pandas.html#finding-missing-values",
    "title": "Data Manipulation with pandas",
    "section": "Finding missing values",
    "text": "Finding missing values\nMissing values are everywhere, and you don’t want them interfering with your work. Some functions ignore missing data by default, but that’s not always the behavior you might want. Some functions can’t handle missing values at all, so these values need to be taken care of before you can use them. If you don’t know where your missing values are, or if they exist, you could make mistakes in your analysis. In this exercise, you’ll determine if there are missing values in the dataset, and if so, how many.\npandas has been imported as pd and avocados_2016, a subset of avocados that contains only sales from 2016, is available. ### Instructions - Print a DataFrame that shows whether each value in avocados_2016 is missing or not. - Print a summary that shows whether any value in each column is missing or not. - Create a bar plot of the total number of missing values in each column.\navocados_2016 = avocados[(avocados.date &gt;= \"2016-01-01\") & (avocados.date &lt;= \"2016-12-31\") ]\navocados_2016.tail(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\ntype\nyear\navg_price\nsize\nnb_sold\n\n\n\n\n945\n2016-01-24\norganic\n2016\n1.41\nextra_large\n12933.97\n\n\n946\n2016-01-17\norganic\n2016\n1.44\nextra_large\n13106.29\n\n\n947\n2016-01-10\norganic\n2016\n1.35\nextra_large\n8771.79\n\n\n948\n2016-01-03\norganic\n2016\n1.43\nextra_large\n7749.40\n\n\n\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Check individual values for missing values\nprint(avocados_2016.isna())\n\n# Check each column for missing values\nprint(avocados_2016.isna().any())\n\n# Bar plot of missing values by variable\navocados_2016.isna().sum().plot(kind=\"bar\")\n\n# Show plot\nplt.show()\n      date   type   year  avg_price   size  nb_sold\n52   False  False  False      False  False    False\n53   False  False  False      False  False    False\n54   False  False  False      False  False    False\n55   False  False  False      False  False    False\n56   False  False  False      False  False    False\n..     ...    ...    ...        ...    ...      ...\n944  False  False  False      False  False    False\n945  False  False  False      False  False    False\n946  False  False  False      False  False    False\n947  False  False  False      False  False    False\n948  False  False  False      False  False    False\n\n[312 rows x 6 columns]\ndate         False\ntype         False\nyear         False\navg_price    False\nsize         False\nnb_sold      False\ndtype: bool\n\n\n\npng",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#removing-missing-values",
    "href": "3_Data_Manipulation_with_pandas.html#removing-missing-values",
    "title": "Data Manipulation with pandas",
    "section": "Removing missing values",
    "text": "Removing missing values\nNow that you know there are some missing values in your DataFrame, you have a few options to deal with them. One way is to remove them from the dataset completely. In this exercise, you’ll remove missing values by removing all rows that contain missing values.\npandas has been imported as pd and avocados_2016 is available. ### Instructions - Remove the rows of avocados_2016 that contain missing values and store the remaining rows in avocados_complete. - Verify that all missing values have been removed from avocados_complete. Calculate each column that has NAs and print.\n# Remove rows with missing values\navocados_complete = avocados_2016.dropna()\n\n# Check if any columns contain missing values\nprint(avocados_complete.isna().any())\ndate         False\ntype         False\nyear         False\navg_price    False\nsize         False\nnb_sold      False\ndtype: bool",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#replacing-missing-values",
    "href": "3_Data_Manipulation_with_pandas.html#replacing-missing-values",
    "title": "Data Manipulation with pandas",
    "section": "Replacing missing values",
    "text": "Replacing missing values\nAnother way of handling missing values is to replace them all with the same value. For numerical variables, one option is to replace values with 0— you’ll do this here. However, when you replace missing values, you make assumptions about what a missing value means. In this case, you will assume that a missing number sold means that no sales for that avocado type were made that week.\nIn this exercise, you’ll see how replacing missing values can affect the distribution of a variable using histograms. You can plot histograms for multiple variables at a time as follows:\ndogs[[\"height_cm\", \"weight_kg\"]].hist()\npandas has been imported as pd and matplotlib.pyplot has been imported as plt. The avocados_2016 dataset is available. ### Instructions - A list has been created, cols_with_missing, containing the names of columns with missing values: “small_sold”, “large_sold”, and “xl_sold”. - Create a histogram of those columns. - Show the plot.\n# List the columns with missing values\ncols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n\n# Create histograms showing the distributions cols_with_missing\navocados_2016[cols_with_missing].hist()\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#list-of-dictionaries",
    "href": "3_Data_Manipulation_with_pandas.html#list-of-dictionaries",
    "title": "Data Manipulation with pandas",
    "section": "List of dictionaries",
    "text": "List of dictionaries\nYou recently got some new avocado data from 2019 that you’d like to put in a DataFrame using the list of dictionaries method. Remember that with this method, you go through the data row by row. |date |small_sold |large_sold| |——–|————–|———–| |“2019-11-03”| 10376832| 7835071 | |“2019-11-10” | 10717154| 8561348 |\npandas as pd is imported. ### Instructions - Create a list of dictionaries with the new data called avocados_list. - Convert the list into a DataFrame called avocados_2019. - Print your new DataFrame.\n# Create a list of dictionaries with new data\navocados_list = [\n    {\"date\": \"2019-11-03\", \"small_sold\": 10376832, \"large_sold\": 7835071},\n    {\"date\": \"2019-11-10\", \"small_sold\": 10717154, \"large_sold\": 8561348},\n]\n\n# Convert list into DataFrame\navocados_2019 = pd.DataFrame(avocados_list)\n\n# Print the new DataFrame\nprint(avocados_2019)\n         date  small_sold  large_sold\n0  2019-11-03    10376832     7835071\n1  2019-11-10    10717154     8561348",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#dictionary-of-lists",
    "href": "3_Data_Manipulation_with_pandas.html#dictionary-of-lists",
    "title": "Data Manipulation with pandas",
    "section": "Dictionary of lists",
    "text": "Dictionary of lists\nSome more data just came in! This time, you’ll use the dictionary of lists method, parsing the data column by column. |date |small_sold |large_sold| |——–|————–|———–| |“2019-11-17”| 10859987| 7674135| |“2019-12-01”| 9291631 | 6238096|\npandas as pd is imported. ### Instructions - Create a dictionary of lists with the new data called avocados_dict. - Convert the dictionary to a DataFrame called avocados_2019. - Print your new DataFrame.\n# Create a dictionary of lists with new data\navocados_dict = {\n  \"date\": [\"2019-11-17\",\"2019-12-01\"],\n  \"small_sold\": [10859987,9291631],\n  \"large_sold\": [7674135,6238096]\n}\n\n# Convert dictionary into DataFrame\navocados_2019 = pd.DataFrame(avocados_dict)\n\n# Print the new DataFrame\nprint(avocados_2019)\n         date  small_sold  large_sold\n0  2019-11-17    10859987     7674135\n1  2019-12-01     9291631     6238096",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#csv-to-dataframe",
    "href": "3_Data_Manipulation_with_pandas.html#csv-to-dataframe",
    "title": "Data Manipulation with pandas",
    "section": "CSV to DataFrame",
    "text": "CSV to DataFrame\nYou work for an airline, and your manager has asked you to do a competitive analysis and see how often passengers flying on other airlines are involuntarily bumped from their flights. You got a CSV file (airline_bumping.csv) from the Department of Transportation containing data on passengers that were involuntarily denied boarding in 2016 and 2017, but it doesn’t have the exact numbers you want. In order to figure this out, you’ll need to get the CSV into a pandas DataFrame and do some manipulation!\npandas is imported for you as pd. “airline_bumping.csv” is in your working directory. ### Instructions - Read the CSV file “airline_bumping.csv” and store it as a DataFrame called airline_bumping. - Print the first few rows of airline_bumping.\n# Read CSV as DataFrame called airline_bumping\nairline_bumping = pd.read_csv(\"airline_bumping.csv\")\n\n# Take a look at the DataFrame\nprint(airline_bumping.head())",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "3_Data_Manipulation_with_pandas.html#dataframe-to-csv",
    "href": "3_Data_Manipulation_with_pandas.html#dataframe-to-csv",
    "title": "Data Manipulation with pandas",
    "section": "DataFrame to CSV",
    "text": "DataFrame to CSV\nYou’re almost there! To make things easier to read, you’ll need to sort the data and export it to CSV so that your colleagues can read it.\npandas as pd has been imported for you.\n\nInstructions\n\nSort airline_totals by the values of bumps_per_10k from highest to lowest, storing as airline_totals_sorted.\nPrint your sorted DataFrame.\nSave the sorted DataFrame as a CSV called “airline_totals_sorted.csv”.\n\n# Create airline_totals_sorted\nairline_totals_sorted = airline_totals.sort_values(\"bumps_per_10k\",ascending=False)\n\n# Print airline_totals_sorted\nprint(airline_totals_sorted)\n\n# Save as airline_totals_sorted.csv\nairline_totals_sorted.to_csv(\"airline_totals_sorted.csv\")",
    "crumbs": [
      "Data Manipulation with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html",
    "href": "4_Joining_Data_with_pandas.html",
    "title": "Joining Data with pandas",
    "section": "",
    "text": "Chapter 1: Data Merging Basics\nLearn how you can merge disparate data using inner joins. By combining information from multiple sources you’ll uncover compelling insights that may have previously been hidden. You’ll also learn how the relationship between those sources, such as one-to-one or one-to-many, can affect your result.",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#your-first-inner-join",
    "href": "4_Joining_Data_with_pandas.html#your-first-inner-join",
    "title": "Joining Data with pandas",
    "section": "Your first inner join",
    "text": "Your first inner join\nYou have been tasked with figuring out what the most popular types of fuel used in Chicago taxis are. To complete the analysis, you need to merge the taxi_owners and taxi_veh tables together on the vid column. You can then use the merged table along with the .value_counts() method to find the most common fuel_type.\nSince you’ll be working with pandas throughout the course, the package will be preloaded for you as pd in each exercise in this course. Also the taxi_owners and taxi_veh DataFrames are loaded for you. ### Instructions - Merge taxi_owners with taxi_veh on the column vid, and save the result to taxi_own_veh. - Set the left and right table suffixes for overlapping columns of the merge to _own and _veh, respectively. - Select the fuel_type column from taxi_own_veh and print the value_counts() to find the most popular fuel_types used.\ntaxi_owners = pd.read_pickle(\"datasets/taxi_owners.p\")\ntaxi_veh = pd.read_pickle(\"datasets/taxi_vehicles.p\")\nprint(taxi_owners.head(3))\nprint(taxi_veh.head(3))\n     rid   vid           owner                 address    zip\n0  T6285  6285  AGEAN TAXI LLC     4536 N. ELSTON AVE.  60630\n1  T4862  4862    MANGIB CORP.  5717 N. WASHTENAW AVE.  60659\n2  T1495  1495   FUNRIDE, INC.     3351 W. ADDISON ST.  60618\n    vid    make   model  year fuel_type           owner\n0  2767  TOYOTA   CAMRY  2013    HYBRID  SEYED M. BADRI\n1  1411  TOYOTA    RAV4  2017    HYBRID     DESZY CORP.\n2  6500  NISSAN  SENTRA  2019  GASOLINE  AGAPH CAB CORP\n\n# Merge the taxi_owners and taxi_veh tables\ntaxi_own_veh = taxi_owners.merge(taxi_veh, on = 'vid')\n\n# Print the column names of the taxi_own_veh\nprint(taxi_own_veh.columns)\nIndex(['rid', 'vid', 'owner_x', 'address', 'zip', 'make', 'model', 'year',\n       'fuel_type', 'owner_y'],\n      dtype='object')\n# Merge the taxi_owners and taxi_veh tables setting a suffix\ntaxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes = (\"_own\",\"_veh\"))\n\n# Print the column names of taxi_own_veh\nprint(taxi_own_veh.columns)\nIndex(['rid', 'vid', 'owner_own', 'address', 'zip', 'make', 'model', 'year',\n       'fuel_type', 'owner_veh'],\n      dtype='object')\n# Merge the taxi_owners and taxi_veh tables setting a suffix\ntaxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own','_veh'))\n\n# Print the value_counts to find the most popular fuel_type\nprint(taxi_own_veh['fuel_type'].value_counts())\nHYBRID                    2792\nGASOLINE                   611\nFLEX FUEL                   89\nCOMPRESSED NATURAL GAS      27\nName: fuel_type, dtype: int64",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#inner-joins-and-number-of-rows-returned",
    "href": "4_Joining_Data_with_pandas.html#inner-joins-and-number-of-rows-returned",
    "title": "Joining Data with pandas",
    "section": "Inner joins and number of rows returned",
    "text": "Inner joins and number of rows returned\nAll of the merges you have studied to this point are called inner joins. It is necessary to understand that inner joins only return the rows with matching values in both tables. You will explore this further by reviewing the merge between the wards and census tables, then comparing it to merges of copies of these tables that are slightly altered, named wards_altered, and census_altered. The first row of the wards column has been changed in the altered tables. You will examine how this affects the merge between them. The tables have been loaded for you.\nFor this exercise, it is important to know that the wards and census tables start with 50 rows. ### Instructions - Merge wards and census on the ward column and save the result to wards_census. - Merge the wards_altered and census tables on the ward column, and notice the difference in returned rows. - Merge the wards and census_altered tables on the ward column, and notice the difference in returned rows.\nwards = pd.read_pickle(\"datasets/ward.p\")\nprint(wards.head(3))\n\ncensus = pd.read_pickle(\"datasets/census.p\")\nprint(census.head(3))\n  ward            alderman                     address    zip\n0    1  Proco \"Joe\" Moreno   2058 NORTH WESTERN AVENUE  60647\n1    2       Brian Hopkins  1400 NORTH  ASHLAND AVENUE  60622\n2    3          Pat Dowell     5046 SOUTH STATE STREET  60609\n  ward  pop_2000  pop_2010 change                      address    zip\n0    1     52951     56149     6%  2765 WEST SAINT MARY STREET  60647\n1    2     54361     55805     3%     WM WASTE MANAGEMENT 1500  60622\n2    3     40385     53039    31%          17 EAST 38TH STREET  60653\n# Merge the wards and census tables on the ward column\nwards_census = wards.merge(census, on ='ward')\n\n# Print the shape of wards_census\nprint('wards_census table shape:', wards_census.shape)\nwards_census table shape: (50, 9)\n# Print the first few rows of the wards_altered table to view the change \nprint(wards_altered[['ward']].head())\n\n# Merge the wards_altered and census tables on the ward column\nwards_altered_census = wards_altered.merge(census, on = 'ward')\n\n# Print the shape of wards_altered_census\nprint('wards_altered_census table shape:', wards_altered_census.shape)",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#one-to-many-merge",
    "href": "4_Joining_Data_with_pandas.html#one-to-many-merge",
    "title": "Joining Data with pandas",
    "section": "One-to-many merge",
    "text": "One-to-many merge\nA business may have one or multiple owners. In this exercise, you will continue to gain experience with one-to-many merges by merging a table of business owners, called biz_owners, to the licenses table. Recall from the video lesson, with a one-to-many relationship, a row in the left table may be repeated if it is related to multiple rows in the right table. In this lesson, you will explore this further by finding out what is the most common business owner title. (i.e., secretary, CEO, or vice president)\nThe licenses and biz_owners DataFrames are loaded for you. ### Instructions - Starting with the licenses table on the left, merge it to the biz_owners table on the column account, and save the results to a variable named licenses_owners. - Group licenses_owners by title and count the number of accounts for each title. Save the result as counted_df - Sort counted_df by the number of accounts in descending order, and save this as a variable named sorted_df. - Use the .head() method to print the first few rows of the sorted_df.\nlicenses = pd.read_pickle(\"datasets/licenses.p\")\nprint(licenses.head(3))\nbiz_owners = pd.read_pickle(\"datasets/business_owners.p\")\nprint(biz_owners.head(3))\n  account ward  aid              business              address    zip\n0  307071    3  743  REGGIE'S BAR & GRILL      2105 S STATE ST  60616\n1      10   10  829            HONEYBEERS  13200 S HOUSTON AVE  60633\n2   10002   14  775           CELINA DELI    5089 S ARCHER AVE  60632\n  account first_name last_name      title\n0      10      PEARL   SHERMAN  PRESIDENT\n1      10      PEARL   SHERMAN  SECRETARY\n2   10002     WALTER    MROZEK    PARTNER\n# Merge the licenses and biz_owners table on account\nlicenses_owners = licenses.merge(biz_owners, on = 'account')\n\n# Group the results by title then count the number of accounts\ncounted_df = licenses_owners.groupby('title').agg({'account':'count'})\n\n# Sort the counted_df in desending order\nsorted_df = counted_df.sort_values(by = 'account',ascending=False)\n\n# Use .head() method to print the first few rows of sorted_df\nprint(sorted_df.head())\n                 account\ntitle                   \nPRESIDENT           6259\nSECRETARY           5205\nSOLE PROPRIETOR     1658\nOTHER               1200\nVICE PRESIDENT       970",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#total-riders-in-a-month",
    "href": "4_Joining_Data_with_pandas.html#total-riders-in-a-month",
    "title": "Joining Data with pandas",
    "section": "Total riders in a month",
    "text": "Total riders in a month\nYour goal is to find the total number of rides provided to passengers passing through the Wilson station (station_name == ‘Wilson’) when riding Chicago’s public transportation system on weekdays (day_type == ‘Weekday’) in July (month == 7). Luckily, Chicago provides this detailed data, but it is in three different tables. You will work on merging these tables together to answer the question. This data is different from the business related data you have seen so far, but all the information you need to answer the question is provided.\n\nInstructions\n\nMerge the ridership and cal tables together, starting with the ridership table on the left and save the result to the variable ridership_cal. If you code takes too long to run, your merge conditions might be incorrect.\nExtend the previous merge to three tables by also merging the stations table.\nCreate a variable called filter_criteria to select the appropriate rows from the merged table so that you can sum the rides column.\n\nridership = pd.read_pickle(\"datasets/cta_ridership.p\")\nridership.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstation_id\nyear\nmonth\nday\nrides\n\n\n\n\n0\n40010\n2019\n1\n1\n576\n\n\n1\n40010\n2019\n1\n2\n1457\n\n\n2\n40010\n2019\n1\n3\n1543\n\n\n\n\nstations = pd.read_pickle(\"datasets/stations.p\")\nstations.head(4)\n\n\n\n\n\n\n\n\n\n\n\n\nstation_id\nstation_name\nlocation\n\n\n\n\n0\n40010\nAustin-Forest Park\n(41.870851, -87.776812)\n\n\n1\n40020\nHarlem-Lake\n(41.886848, -87.803176)\n\n\n2\n40030\nPulaski-Lake\n(41.885412, -87.725404)\n\n\n3\n40040\nQuincy/Wells\n(41.878723, -87.63374)\n\n\n\n\ncal = pd.read_pickle(\"datasets/cta_calendar.p\")\ncal.head(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nday\nday_type\n\n\n\n\n0\n2019\n1\n1\nSunday/Holiday\n\n\n1\n2019\n1\n2\nWeekday\n\n\n2\n2019\n1\n3\nWeekday\n\n\n3\n2019\n1\n4\nWeekday\n\n\n\n\n# Merge the ridership and cal tables\nridership_cal = ridership.merge(cal)\n# Merge the ridership, cal, and stations tables\nridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n                            .merge(stations,on=\"station_id\")\n# Merge the ridership, cal, and stations tables\nridership_cal_stations = ridership.merge(cal, on=['year','month','day']) \\\n                            .merge(stations, on='station_id')\n\n# Create a filter to filter ridership_cal_stations\nfilter_criteria = ((ridership_cal_stations['month'] == 7) \n                   & (ridership_cal_stations['day_type'] == 'Weekday') \n                   & (ridership_cal_stations['station_name'] == 'Wilson'))\n\n# Use .loc and the filter to select for rides\nprint(ridership_cal_stations.loc[filter_criteria, 'rides'].sum())\n140005",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#three-table-merge",
    "href": "4_Joining_Data_with_pandas.html#three-table-merge",
    "title": "Joining Data with pandas",
    "section": "Three table merge",
    "text": "Three table merge\nTo solidify the concept of a three DataFrame merge, practice another exercise. A reasonable extension of our review of Chicago business data would include looking at demographics information about the neighborhoods where the businesses are. A table with the median income by zip code has been provided to you. You will merge the licenses and wards tables with this new income-by-zip-code table called zip_demo.\nThe licenses, wards, and zip_demo DataFrames have been loaded for you. ### Instructions - Starting with the licenses table, merge to it the zip_demo table on the zip column. Then merge the resulting table to the wards table on the ward column. Save result of the three merged tables to a variable named licenses_zip_ward. - Group the results of the three merged tables by the column alderman and find the median income.\nzip_demo = pd.read_pickle(\"datasets/zip_demo.p\")\nzip_demo.head(4)\n\n\n\n\n\n\n\n\n\n\n\nzip\nincome\n\n\n\n\n0\n60630\n70122\n\n\n1\n60640\n50488\n\n\n2\n60622\n87143\n\n\n3\n60614\n100116\n\n\n\n\n# Merge licenses and zip_demo, on zip; and merge the wards on ward\nlicenses_zip_ward = licenses.merge(zip_demo, on =\"zip\") \\\n                        .merge(wards, on = \"ward\")\n\n# Print the results by alderman and show median income\nprint(licenses_zip_ward.groupby(\"alderman\").agg({'income':'median'}))\n                             income\nalderman                           \nAmeya Pawar                 66246.0\nAnthony A. Beale            38206.0\nAnthony V. Napolitano       82226.0\nAriel E. Reyboras           41307.0\nBrendan Reilly             110215.0\nBrian Hopkins               87143.0\nCarlos Ramirez-Rosa         66246.0\nCarrie M. Austin            38206.0\nChris Taliaferro            55566.0\nDaniel \"Danny\" Solis        41226.0\nDavid H. Moore              33304.0\nDeborah Mell                66246.0\nDebra L. Silverstein        50554.0\nDerrick G. Curtis           65770.0\nEdward M. Burke             42335.0\nEmma M. Mitts               36283.0\nGeorge Cardenas             33959.0\nGilbert Villegas            41307.0\nGregory I. Mitchell         24941.0\nHarry Osterman              45442.0\nHoward B. Brookins, Jr.     33304.0\nJames Cappleman             79565.0\nJason C. Ervin              41226.0\nJoe Moore                   39163.0\nJohn S. Arena               70122.0\nLeslie A. Hairston          28024.0\nMargaret Laurino            70122.0\nMarty Quinn                 67045.0\nMatthew J. O'Shea           59488.0\nMichael R. Zalewski         42335.0\nMichael Scott, Jr.          31445.0\nMichelle A. Harris          32558.0\nMichelle Smith             100116.0\nMilagros \"Milly\" Santiago   41307.0\nNicholas Sposato            62223.0\nPat Dowell                  46340.0\nPatrick Daley Thompson      41226.0\nPatrick J. O'Connor         50554.0\nProco \"Joe\" Moreno          87143.0\nRaymond A. Lopez            33959.0\nRicardo Munoz               31445.0\nRoberto Maldonado           68223.0\nRoderick T. Sawyer          32558.0\nScott Waguespack            68223.0\nSusan Sadlowski Garza       38417.0\nTom Tunney                  88708.0\nToni L. Foulkes             27573.0\nWalter Burnett, Jr.         87143.0\nWilliam D. Burns           107811.0\nWillie B. Cochran           28024.0",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#one-to-many-merge-with-multiple-tables",
    "href": "4_Joining_Data_with_pandas.html#one-to-many-merge-with-multiple-tables",
    "title": "Joining Data with pandas",
    "section": "One-to-many merge with multiple tables",
    "text": "One-to-many merge with multiple tables\nIn this exercise, assume that you are looking to start a business in the city of Chicago. Your perfect idea is to start a company that uses goats to mow the lawn for other businesses. However, you have to choose a location in the city to put your goat farm. You need a location with a great deal of space and relatively few businesses and people around to avoid complaints about the smell. You will need to merge three tables to help you choose your location. The land_use table has info on the percentage of vacant land by city ward. The census table has population by ward, and the licenses table lists businesses by ward.\nThe land_use, census, and licenses tables have been loaded for you. ### Instructions - Merge land_use and census on the ward column. Merge the result of this with licenses on the ward column, using the suffix _cen for the left table and _lic for the right table. Save this to the variable land_cen_lic. - Group land_cen_lic by ward, pop_2010 (the population in 2010), and vacant, then count the number of accounts. Save the results to pop_vac_lic. - Sort pop_vac_lic by vacant, account, andpop_2010 in descending, ascending, and ascending order respectively. Save it as sorted_pop_vac_lic.\nland_use = pd.read_pickle(\"datasets/land_use.p\")\nland_use.head(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nward\nresidential\ncommercial\nindustrial\nvacant\nother\n\n\n\n\n0\n1\n41\n9\n2\n2\n46\n\n\n1\n2\n31\n11\n6\n2\n50\n\n\n2\n3\n20\n5\n3\n13\n59\n\n\n3\n4\n22\n13\n0\n7\n58\n\n\n\n\n# Merge land_use and census and merge result with licenses including suffixes\nland_cen_lic = land_use.merge(census, on = \"ward\").merge(licenses, on = \"ward\", suffixes = (\"_cen\",\"_lic\"))\n# Merge land_use and census and merge result with licenses including suffixes\nland_cen_lic = land_use.merge(census, on='ward') \\\n                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n\n# Group by ward, pop_2010, and vacant, then count the # of accounts\npop_vac_lic = land_cen_lic.groupby(['ward',\"pop_2010\",\"vacant\"], \n                                   as_index=False).agg({'account':'count'})\n# Merge land_use and census and merge result with licenses including suffixes\nland_cen_lic = land_use.merge(census, on='ward') \\\n                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n\n# Group by ward, pop_2010, and vacant, then count the # of accounts\npop_vac_lic = land_cen_lic.groupby(['ward','pop_2010','vacant'], \n                                   as_index=False).agg({'account':'count'})\n\n# Sort pop_vac_lic and print the results\nsorted_pop_vac_lic = pop_vac_lic.sort_values(by = [\"vacant\",\"account\",\"pop_2010\"], \n                                             ascending=[False,True,True])\n\n# Print the top few rows of sorted_pop_vac_lic\nprint(sorted_pop_vac_lic.head())\n   ward  pop_2010  vacant  account\n47    7     51581      19       80\n12   20     52372      15      123\n1    10     51535      14      130\n16   24     54909      13       98\n7    16     51954      13      156",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#enriching-a-dataset",
    "href": "4_Joining_Data_with_pandas.html#enriching-a-dataset",
    "title": "Joining Data with pandas",
    "section": "Enriching a dataset",
    "text": "Enriching a dataset\nSetting how=‘left’ with the .merge()method is a useful technique for enriching or enhancing a dataset with additional information from a different table. In this exercise, you will start off with a sample of movie data from the movie series Toy Story. Your goal is to enrich this data by adding the marketing tag line for each movie. You will compare the results of a left join versus an inner join.\nThe toy_story DataFrame contains the Toy Story movies. The toy_story and taglines DataFrames have been loaded for you. ### Instructions - Merge toy_story and taglines on the id column with a left join, and save the result as toystory_tag. - With toy_story as the left table, merge to it taglines on the id column with an inner join, and save as toystory_tag.\nmovies = pd.read_pickle(\"datasets/movies.p\")\nmovies.head(4)\ntoy_story = movies[movies[\"title\"].str.contains(\"Toy Story\")].reset_index(drop=True)\ntoy_story\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\npopularity\nrelease_date\n\n\n\n\n0\n10193\nToy Story 3\n59.995418\n2010-06-16\n\n\n1\n863\nToy Story 2\n73.575118\n1999-10-30\n\n\n2\n862\nToy Story\n73.640445\n1995-10-30\n\n\n\n\ntaglines = pd.read_pickle(\"datasets/taglines.p\")\ntaglines.head(4)\n\n\n\n\n\n\n\n\n\n\n\nid\ntagline\n\n\n\n\n0\n19995\nEnter the World of Pandora.\n\n\n1\n285\nAt the end of the world, the adventure begins.\n\n\n2\n206647\nA Plan No One Escapes\n\n\n3\n49026\nThe Legend Ends\n\n\n\n\n# Merge the toy_story and taglines tables with a left join\ntoystory_tag = toy_story.merge(taglines, how = \"left\", on=\"id\")\n\n# Print the rows and shape of toystory_tag\nprint(toystory_tag)\nprint(toystory_tag.shape)\n      id        title  popularity release_date                   tagline\n0  10193  Toy Story 3   59.995418   2010-06-16  No toy gets left behind.\n1    863  Toy Story 2   73.575118   1999-10-30        The toys are back!\n2    862    Toy Story   73.640445   1995-10-30                       NaN\n(3, 5)\n# Merge the toy_story and taglines tables with a inner join\ntoystory_tag = toy_story.merge(taglines,on=\"id\")\n\n# Print the rows and shape of toystory_tag\nprint(toystory_tag)\nprint(toystory_tag.shape)\n      id        title  popularity release_date                   tagline\n0  10193  Toy Story 3   59.995418   2010-06-16  No toy gets left behind.\n1    863  Toy Story 2   73.575118   1999-10-30        The toys are back!\n(2, 5)",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#right-join-to-find-unique-movies",
    "href": "4_Joining_Data_with_pandas.html#right-join-to-find-unique-movies",
    "title": "Joining Data with pandas",
    "section": "Right join to find unique movies",
    "text": "Right join to find unique movies\nMost of the recent big-budget science fiction movies can also be classified as action movies. You are given a table of science fiction movies called scifi_movies and another table of action movies called action_movies. Your goal is to find which movies are considered only science fiction movies. Once you have this table, you can merge the movies table in to see the movie names. Since this exercise is related to science fiction movies, use a right join as your superhero power to solve this problem.\nThe movies, scifi_movies, and action_movies tables have been loaded for you. ### Instructions - Merge action_movies and scifi_movies tables with a right join on movie_id. Save the result as action_scifi. - Update the merge to add suffixes, where ‘_act’ and ‘_sci’ are suffixes for the left and right tables, respectively. - From action_scifi, subset only the rows where the genre_act column is null. - Merge movies and scifi_only using the id column in the left table and the movie_id column in the right table with an inner join.\nmovie_to_genres = pd.read_pickle(\"datasets/movie_to_genres.p\")\nmovie_to_genres.head(4)\nmovies_n_genres = movies.merge(movie_to_genres, left_on='id', right_on=\"movie_id\")\nmovies_n_genres.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\npopularity\nrelease_date\nmovie_id\ngenre\n\n\n\n\n0\n257\nOliver Twist\n20.415572\n2005-09-23\n257\nCrime\n\n\n1\n257\nOliver Twist\n20.415572\n2005-09-23\n257\nDrama\n\n\n2\n257\nOliver Twist\n20.415572\n2005-09-23\n257\nFamily\n\n\n\n\n# action movies\naction_movies = movies_n_genres[movies_n_genres.genre == 'Action']\naction_movies.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\npopularity\nrelease_date\nmovie_id\ngenre\n\n\n\n\n11\n49529\nJohn Carter\n43.926995\n2012-03-07\n49529\nAction\n\n\n20\n76757\nJupiter Ascending\n85.369080\n2015-02-04\n76757\nAction\n\n\n34\n308531\nTeenage Mutant Ninja Turtles: Out of the Shadows\n39.873791\n2016-06-01\n308531\nAction\n\n\n\n\nscifi_movies = movies_n_genres[movies_n_genres.genre == 'Science Fiction']\nscifi_movies.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\npopularity\nrelease_date\nmovie_id\ngenre\n\n\n\n\n9\n49529\nJohn Carter\n43.926995\n2012-03-07\n49529\nScience Fiction\n\n\n17\n18841\nThe Lost Skeleton of Cadavra\n1.680525\n2001-09-12\n18841\nScience Fiction\n\n\n21\n76757\nJupiter Ascending\n85.369080\n2015-02-04\n76757\nScience Fiction\n\n\n\n\n# Merge action_movies to scifi_movies with right join\naction_scifi = action_movies.merge(scifi_movies, on = \"movie_id\",how ='right' )\n# Merge action_movies to scifi_movies with right join\naction_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n                                   suffixes = (\"_act\",\"_sci\"))\n\n# Print the first few rows of action_scifi to see the structure\nprint(action_scifi.head())\n    id_act          title_act  ...  release_date_sci        genre_sci\n0  49529.0        John Carter  ...        2012-03-07  Science Fiction\n1      NaN                NaN  ...        2001-09-12  Science Fiction\n2  76757.0  Jupiter Ascending  ...        2015-02-04  Science Fiction\n3      NaN                NaN  ...        1993-09-23  Science Fiction\n4      NaN                NaN  ...        1983-06-24  Science Fiction\n\n[5 rows x 11 columns]\n# Merge action_movies to the scifi_movies with right join\naction_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n                                   suffixes=('_act','_sci'))\n\n# From action_scifi, select only the rows where the genre_act column is null\nscifi_only = action_scifi[action_scifi[\"genre_act\"].isnull()]\n# Merge action_movies to the scifi_movies with right join\naction_scifi = action_movies.merge(scifi_movies, on='movie_id', how='right',\n                                   suffixes=('_act','_sci'))\n\n# From action_scifi, select only the rows where the genre_act column is null\nscifi_only = action_scifi[action_scifi['genre_act'].isnull()]\n\n# Merge the movies and scifi_only tables with an inner join\nmovies_and_scifi_only = movies.merge(scifi_only,left_on = \"id\", right_on = \"movie_id\")\n\n# Print the first few rows and shape of movies_and_scifi_only\nprint(movies_and_scifi_only.head())\nprint(movies_and_scifi_only.shape)\n      id                         title  ...  release_date_sci        genre_sci\n0  18841  The Lost Skeleton of Cadavra  ...        2001-09-12  Science Fiction\n1  26672     The Thief and the Cobbler  ...        1993-09-23  Science Fiction\n2  15301      Twilight Zone: The Movie  ...        1983-06-24  Science Fiction\n3   8452                   The 6th Day  ...        2000-11-17  Science Fiction\n4   1649    Bill & Ted's Bogus Journey  ...        1991-07-19  Science Fiction\n\n[5 rows x 15 columns]\n(258, 15)",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#popular-genres-with-right-join",
    "href": "4_Joining_Data_with_pandas.html#popular-genres-with-right-join",
    "title": "Joining Data with pandas",
    "section": "Popular genres with right join",
    "text": "Popular genres with right join\nWhat are the genres of the most popular movies? To answer this question, you need to merge data from the movies and movie_to_genres tables. In a table called pop_movies, the top 10 most popular movies in the movies table have been selected. To ensure that you are analyzing all of the popular movies, merge it with the movie_to_genres table using a right join. To complete your analysis, count the number of different genres. Also, the two tables can be merged by the movie ID. However, in pop_movies that column is called id, and in movies_to_genres it’s called movie_id.\nThe pop_movies and movie_to_genres tables have been loaded for you. ### Instructions - Merge movie_to_genres and pop_movies using a right join. Save the results as genres_movies. - Group genres_movies by genre and count the number of id values.\n#scifi_movies = movies_n_genres[movies_n_genres.genre == 'Science Fiction']\n\n#movies_n_genres.genre.unique\nratings = pd.read_pickle(\"datasets/ratings.p\")\nratings.vote_average.max()\n\npop_movies = movies[movies[\"id\"].isin([211672,157336,293660,118340,76341,135397,22,119450,131631,177572])]\npop_movies\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\npopularity\nrelease_date\n\n\n\n\n1106\n119450\nDawn of the Planet of the Apes\n243.791743\n2014-06-26\n\n\n1867\n135397\nJurassic World\n418.708552\n2015-06-09\n\n\n1966\n293660\nDeadpool\n514.569956\n2016-02-09\n\n\n2423\n118340\nGuardians of the Galaxy\n481.098624\n2014-07-30\n\n\n2614\n177572\nBig Hero 6\n203.734590\n2014-10-24\n\n\n4216\n131631\nThe Hunger Games: Mockingjay - Part 1\n206.227151\n2014-11-18\n\n\n4220\n76341\nMad Max: Fury Road\n434.278564\n2015-05-13\n\n\n4343\n157336\nInterstellar\n724.247784\n2014-11-05\n\n\n4375\n22\nPirates of the Caribbean: The Curse of the Bla...\n271.972889\n2003-07-09\n\n\n4546\n211672\nMinions\n875.581305\n2015-06-17\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Use right join to merge the movie_to_genres and pop_movies tables\ngenres_movies = movie_to_genres.merge(pop_movies, how='right', \n                                      left_on = \"movie_id\", \n                                      right_on = \"id\")\n\n# Count the number of genres\ngenre_count = genres_movies.groupby('genre').agg({'id':'count'})\n\n# Plot a bar chart of the genre_count\ngenre_count.plot(kind='bar')\nplt.show()\n\n\n\npng",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#using-outer-join-to-select-actors",
    "href": "4_Joining_Data_with_pandas.html#using-outer-join-to-select-actors",
    "title": "Joining Data with pandas",
    "section": "Using outer join to select actors",
    "text": "Using outer join to select actors\nOne cool aspect of using an outer join is that, because it returns all rows from both merged tables and null where they do not match, you can use it to find rows that do not have a match in the other table. To try for yourself, you have been given two tables with a list of actors from two popular movies: Iron Man 1 and Iron Man 2. Most of the actors played in both movies. Use an outer join to find actors who did not act in both movies.\nThe Iron Man 1 table is called iron_1_actors, and Iron Man 2 table is called iron_2_actors. Both tables have been loaded for you and a few rows printed so you can see the structure.\n\nInstructions\n\nSave to iron_1_and_2 the merge of iron_1_actors (left) with iron_2_actors tables with an outer join on the id column, and set suffixes to (‘_1’,‘_2’).\nCreate an index that returns True if name_1 or name_2 are null, and False otherwise.\n\ncasts = pd.read_pickle(\"datasets/casts.p\")\ncasts.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovie_id\ncast_id\ncharacter\ngender\nid\nname\n\n\n\n\n7\n5\n22\nJezebel\n1\n3122\nSammi Davis\n\n\n8\n5\n23\nDiana\n1\n3123\nAmanda de Cadenet\n\n\n9\n5\n24\nAthena\n1\n3124\nValeria Golino\n\n\n\n\nmovies[movies.title.str.contains(\"Iron\")]\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\npopularity\nrelease_date\n\n\n\n\n181\n10386\nThe Iron Giant\n61.245957\n1999-08-06\n\n\n2198\n1726\nIron Man\n120.725053\n2008-04-30\n\n\n3041\n38543\nIronclad\n12.317080\n2011-03-03\n\n\n3271\n97430\nThe Man with the Iron Fists\n17.672021\n2012-11-02\n\n\n3286\n68721\nIron Man 3\n77.682080\n2013-04-18\n\n\n3401\n10138\nIron Man 2\n77.300194\n2010-04-28\n\n\n3900\n9313\nThe Man in the Iron Mask\n28.980270\n1998-03-12\n\n\n4539\n71688\nThe Iron Lady\n23.378400\n2011-12-30\n\n\n\n\niron_1_actors = casts[casts.movie_id==1726]\niron_1_actors.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovie_id\ncast_id\ncharacter\ngender\nid\nname\n\n\n\n\n3\n1726\n9\nYinsen\n2\n17857\nShaun Toub\n\n\n4\n1726\n10\nVirginia “Pepper” Potts\n1\n12052\nGwyneth Paltrow\n\n\n2\n1726\n11\nObadiah Stane / Iron Monger\n2\n1229\nJeff Bridges\n\n\n\n\niron_2_actors = casts[casts.movie_id==10138]\niron_2_actors.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovie_id\ncast_id\ncharacter\ngender\nid\nname\n\n\n\n\n4\n10138\n3\nIvan Vanko / Whiplash\n2\n2295\nMickey Rourke\n\n\n3\n10138\n5\nNatalie Rushman / Natasha Romanoff / Black Widow\n1\n1245\nScarlett Johansson\n\n\n5\n10138\n6\nJustin Hammer\n2\n6807\nSam Rockwell\n\n\n\n\n# Merge iron_1_actors to iron_2_actors on id with outer join using suffixes\niron_1_and_2 = iron_1_actors.merge(iron_2_actors,\n                                     on = \"id\",\n                                     how='outer',\n                                     suffixes=(\"_1\",\"_2\"))\n\n# Create an index that returns true if name_1 or name_2 are null\nm = ((iron_1_and_2['name_1'].isnull()) | \n     (iron_1_and_2['name_2'].isnull()))\n\n# Print the first few rows of iron_1_and_2\nprint(iron_1_and_2[m].head())\n   movie_id_1  cast_id_1  ... gender_2  name_2\n0      1726.0        9.0  ...      NaN     NaN\n2      1726.0       11.0  ...      NaN     NaN\n3      1726.0       12.0  ...      NaN     NaN\n5      1726.0       18.0  ...      NaN     NaN\n8      1726.0       23.0  ...      NaN     NaN\n\n[5 rows x 11 columns]",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#self-join",
    "href": "4_Joining_Data_with_pandas.html#self-join",
    "title": "Joining Data with pandas",
    "section": "Self join",
    "text": "Self join\nMerging a table to itself can be useful when you want to compare values in a column to other values in the same column. In this exercise, you will practice this by creating a table that for each movie will list the movie director and a member of the crew on one row. You have been given a table called crews, which has columns id, job, and name. First, merge the table to itself using the movie ID. This merge will give you a larger table where for each movie, every job is matched against each other. Then select only those rows with a director in the left table, and avoid having a row where the director’s job is listed in both the left and right tables. This filtering will remove job combinations that aren’t with the director.\nThe crews table has been loaded for you. ### Instructions - To a variable called crews_self_merged, merge the crews table to itself on the id column using an inner join, setting the suffixes to ‘_dir’ and ‘_crew’ for the left and right tables respectively. - Create a Boolean index, named boolean_filter, that selects rows from the left table with the job of ‘Director’ and avoids rows with the job of ‘Director’ in the right table. - Use the .head() method to print the first few rows of direct_crews.\ncrews = pd.read_pickle(\"datasets/crews.p\")\ncrews.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ndepartment\njob\nname\n\n\n\n\n0\n19995\nEditing\nEditor\nStephen E. Rivkin\n\n\n2\n19995\nSound\nSound Designer\nChristopher Boyes\n\n\n4\n19995\nProduction\nCasting\nMali Finn\n\n\n\n\n# Merge the crews table to itself\ncrews_self_merged = crews.merge(crews,on = 'id',suffixes = (\"_dir\",\"_crew\"))\n# Merge the crews table to itself\ncrews_self_merged = crews.merge(crews, on='id', how='inner',\n                                suffixes=('_dir','_crew'))\n\n# Create a Boolean index to select the appropriate\nboolean_filter = ((crews_self_merged['job_dir'] == \"Director\") & \n     (crews_self_merged['job_crew'] != \"Director\"))\ndirect_crews = crews_self_merged[boolean_filter]\n# Merge the crews table to itself\ncrews_self_merged = crews.merge(crews, on='id', how='inner',\n                                suffixes=('_dir','_crew'))\n\n# Create a boolean index to select the appropriate rows\nboolean_filter = ((crews_self_merged['job_dir'] == 'Director') & \n                  (crews_self_merged['job_crew'] != 'Director'))\ndirect_crews = crews_self_merged[boolean_filter]\n\n# Print the first few rows of direct_crews\nprint(direct_crews.head())\n        id department_dir  ...        job_crew          name_crew\n156  19995      Directing  ...          Editor  Stephen E. Rivkin\n157  19995      Directing  ...  Sound Designer  Christopher Boyes\n158  19995      Directing  ...         Casting          Mali Finn\n160  19995      Directing  ...          Writer      James Cameron\n161  19995      Directing  ...    Set Designer    Richard F. Mays\n\n[5 rows x 7 columns]",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#index-merge-for-movie-ratings",
    "href": "4_Joining_Data_with_pandas.html#index-merge-for-movie-ratings",
    "title": "Joining Data with pandas",
    "section": "Index merge for movie ratings",
    "text": "Index merge for movie ratings\nTo practice merging on indexes, you will merge movies and a table called ratings that holds info about movie ratings. Make sure your merge returns all of the rows from the movies table and not all the rows of ratings table need to be included in the result.\nThe movies and ratings tables have been loaded for you. ### Instructions - Merge movies and ratings on the index and save to a variable called movies_ratings, ensuring that all of the rows from the movies table are returned.\n# Merge to the movies table the ratings table on the index\nmovies_ratings = movies.merge(ratings,on = 'id')\n\n# Print the first few rows of movies_ratings\nprint(movies_ratings.head())\n      id                 title  ...  vote_average vote_count\n0    257          Oliver Twist  ...           6.7      274.0\n1  14290  Better Luck Tomorrow  ...           6.5       27.0\n2  38365             Grown Ups  ...           6.0     1705.0\n3   9672              Infamous  ...           6.4       60.0\n4  12819       Alpha and Omega  ...           5.3      124.0\n\n[5 rows x 6 columns]",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#do-sequels-earn-more",
    "href": "4_Joining_Data_with_pandas.html#do-sequels-earn-more",
    "title": "Joining Data with pandas",
    "section": "Do sequels earn more?",
    "text": "Do sequels earn more?\nIt is time to put together many of the aspects that you have learned in this chapter. In this exercise, you’ll find out which movie sequels earned the most compared to the original movie. To answer this question, you will merge a modified version of the sequels and financials tables where their index is the movie ID. You will need to choose a merge type that will return all of the rows from the sequels table and not all the rows of financials table need to be included in the result. From there, you will join the resulting table to itself so that you can compare the revenue values of the original movie to the sequel. Next, you will calculate the difference between the two revenues and sort the resulting dataset.\nThe sequels and financials tables have been provided. ### Instructions - With the sequels table on the left, merge to it the financials table on index named id, ensuring that all the rows from the sequels are returned and some rows from the other table may not be returned, Save the results to sequels_fin. - Merge the sequels_fin table to itself with an inner join, where the left and right tables merge on sequel and id respectively with suffixes equal to (‘_org’,‘_seq’), saving to orig_seq. - Select the title_org, title_seq, and diff columns of orig_seq and save this as titles_diff. - Sort by titles_diff by diff in descending order and print the first few rows.\nsequels = pd.read_pickle(\"datasets/sequels.p\")\nprint(sequels.head(3))\nfinancials = pd.read_pickle(\"datasets/financials.p\")\nprint(financials.head(3))\n      id        title  sequel\n0  19995       Avatar    &lt;NA&gt;\n1    862    Toy Story     863\n2    863  Toy Story 2   10193\n       id     budget       revenue\n0   19995  237000000  2.787965e+09\n1     285  300000000  9.610000e+08\n2  206647  245000000  8.806746e+08\n# Merge sequels and financials on index id\nsequels_fin = sequels.merge(financials,on = 'id', how = \"left\")\nsequels_fin.shape\n(4803, 5)\n# Merge sequels and financials on index id\nsequels_fin = sequels.merge(financials, on='id', how='left')\nsequels_fin.set_index(\"id\", inplace=True)\n\n# Self merge with suffixes as inner join with left on sequel and right on id\norig_seq = sequels_fin.merge(sequels_fin,  left_on='sequel', \n                             right_on='id', right_index=True,\n                             suffixes=(\"_org\",\"_seq\"))\n\n# Add calculation to subtract revenue_org from revenue_seq \norig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n# Merge sequels and financials on index id\nsequels_fin = sequels.merge(financials, on='id', how='left')\nsequels_fin.set_index(\"id\", inplace=True)\n# Self merge with suffixes as inner join with left on sequel and right on id\norig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', \n                             right_on='id', right_index=True,\n                             suffixes=('_org','_seq'))\n\n# Add calculation to subtract revenue_org from revenue_seq \norig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n\n# Select the title_org, title_seq, and diff \ntitles_diff = orig_seq[[\"title_org\", \"title_seq\", \"diff\" ]]\n# Merge sequels and financials on index id\nsequels_fin = sequels.merge(financials, on='id', how='left')\nsequels_fin.set_index(\"id\", inplace=True)\n# Self merge with suffixes as inner join with left on sequel and right on id\norig_seq = sequels_fin.merge(sequels_fin, how='inner', left_on='sequel', \n                             right_on='id', right_index=True,\n                             suffixes=('_org','_seq'))\n\n# Add calculation to subtract revenue_org from revenue_seq \norig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n\n# Select the title_org, title_seq, and diff \ntitles_diff = orig_seq[['title_org','title_seq','diff']]\n\n# Print the first rows of the sorted titles_diff\nprint(titles_diff.sort_values(\"diff\", ascending=False).head())\n               title_org        title_seq          diff\nid                                                     \n331    Jurassic Park III   Jurassic World  1.144748e+09\n272        Batman Begins  The Dark Knight  6.303398e+08\n10138         Iron Man 2       Iron Man 3  5.915067e+08\n863          Toy Story 2      Toy Story 3  5.696028e+08\n10764  Quantum of Solace          Skyfall  5.224703e+08",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#performing-an-anti-join",
    "href": "4_Joining_Data_with_pandas.html#performing-an-anti-join",
    "title": "Joining Data with pandas",
    "section": "Performing an anti join",
    "text": "Performing an anti join\nIn our music streaming company dataset, each customer is assigned an employee representative to assist them. In this exercise, filter the employee table by a table of top customers, returning only those employees who are not assigned to a customer. The results should resemble the results of an anti join. The company’s leadership will assign these employees additional training so that they can work with high valued customers.\nThe top_cust and employees tables have been provided for you. ### Instructions - Merge employees and top_cust with a left join, setting indicator argument to True. Save the result to empl_cust. - Select the srid column of empl_cust and the rows where _merge is ‘left_only’. Save the result to srid_list. - Subset the employees table and select those rows where the srid is in the variable srid_list and print the results.\nemployees = pd.read_csv(\"datasets/employee.csv\")\nemployees.head(2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nind\nsrid\nlname\nfname\ntitle\nhire_date\nemail\n\n\n\n\n0\n0\n1\nAdams\nAndrew\nGeneral Manager\n2002-08-14\nandrew@chinookcorp.com\n\n\n1\n1\n2\nEdwards\nNancy\nSales Manager\n2002-05-01\nnancy@chinookcorp.com\n\n\n\n\ntop_cust = pd.read_csv(\"datasets/top_cust.csv\")\ntop_cust.columns\nIndex(['cid', 'srid', 'fname', 'lname', 'phone', 'fax', 'email'], dtype='object')\n# Merge employees and top_cust\nempl_cust = employees.merge(top_cust, on='srid', \n                            how=\"left\", indicator=True)\n# Merge employees and top_cust\nempl_cust = employees.merge(top_cust, on='srid', \n                            how='left', indicator=True)\n\n# Select the srid column where _merge is left_only\nsrid_list = empl_cust.loc[empl_cust[\"_merge\"]=='left_only', 'srid']\n# Merge employees and top_cust\nempl_cust = employees.merge(top_cust, on='srid', \n                                 how='left', indicator=True)\n\n# Select the srid column where _merge is left_only\nsrid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n\n# Get employees not working with top customers\nprint(employees[employees.srid.isin(srid_list)])\n   ind  srid  ...      hire_date                      email\n0    0     1  ...    2002-08-14      andrew@chinookcorp.com\n1    1     2  ...    2002-05-01       nancy@chinookcorp.com\n5    5     6  ...   2003-10-17      michael@chinookcorp.com\n6    6     7  ...    2004-01-02      robert@chinookcorp.com\n7    7     8  ...       4/3/2004      laura@chinookcorp.com\n\n[5 rows x 7 columns]",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#performing-a-semi-join",
    "href": "4_Joining_Data_with_pandas.html#performing-a-semi-join",
    "title": "Joining Data with pandas",
    "section": "Performing a semi join",
    "text": "Performing a semi join\nSome of the tracks that have generated the most significant amount of revenue are from TV-shows or are other non-musical audio. You have been given a table of invoices that include top revenue-generating items. Additionally, you have a table of non-musical tracks from the streaming service. In this exercise, you’ll use a semi join to find the top revenue-generating non-musical tracks..\nThe tables non_mus_tcks, top_invoices, and genres have been loaded for you.\n\nInstructions\n\nMerge non_mus_tcks and top_invoices on tid using an inner join. Save the result as tracks_invoices.\nUse .isin() to subset the rows of non_mus_tck where tid is in the tid column of tracks_invoices. Save the result as top_tracks.\nGroup top_tracks by gid and count the tid rows. Save the result to cnt_by_gid.\nMerge cnt_by_gid with the genres table on gid and print the result.\n\n# Merge the non_mus_tck and top_invoices tables on tid\ntracks_invoices = non_mus_tcks.merge(top_invoices,on = 'tid')\n\n# Use .isin() to subset non_mus_tcks to rows with tid in tracks_invoices\ntop_tracks = non_mus_tcks[non_mus_tcks['tid'].isin(tracks_invoices.tid)]\n\n# Group the top_tracks by gid and count the tid rows\ncnt_by_gid = top_tracks.groupby(['gid'], as_index=False).agg({'tid':'count'})\n\n# Merge the genres table to cnt_by_gid on gid and print\nprint(cnt_by_gid.merge(genres, on =\"gid\"))",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#concatenation-basics",
    "href": "4_Joining_Data_with_pandas.html#concatenation-basics",
    "title": "Joining Data with pandas",
    "section": "Concatenation basics",
    "text": "Concatenation basics\nYou have been given a few tables of data with musical track info for different albums from the metal band, Metallica. The track info comes from their Ride The Lightning, Master Of Puppets, and St. Anger albums. Try various features of the .concat() method by concatenating the tables vertically together in different ways.\nThe tables tracks_master, tracks_ride, and tracks_st have loaded for you. ### Instructions - Concatenate tracks_master, tracks_ride, and tracks_st, in that order, setting sort to True. - Concatenate tracks_master, tracks_ride, and tracks_st, where the index goes from 0 to n-1. - Concatenate tracks_master, tracks_ride, and tracks_st, showing only columns that are in all tables.\n# Concatenate the tracks\ntracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],\n                               sort=True)\nprint(tracks_from_albums)\n# Concatenate the tracks so the index goes from 0 to n-1\ntracks_from_albums = pd.concat([tracks_master, tracks_ride, tracks_st],\n                               ignore_index = True,\n                               sort=True)\nprint(tracks_from_albums)\n# Concatenate the tracks, show only columns names that are in all tables\ntracks_from_albums = pd.concat([tracks_master, tracks_ride,tracks_st],\n                               join = 'inner',\n                               sort=True)\nprint(tracks_from_albums)",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#concatenating-with-keys",
    "href": "4_Joining_Data_with_pandas.html#concatenating-with-keys",
    "title": "Joining Data with pandas",
    "section": "Concatenating with keys",
    "text": "Concatenating with keys\nThe leadership of the music streaming company has come to you and asked you for assistance in analyzing sales for a recent business quarter. They would like to know which month in the quarter saw the highest average invoice total. You have been given three tables with invoice data named inv_jul, inv_aug, and inv_sep. Concatenate these tables into one to create a graph of the average monthly invoice total.\n\nInstructions\n\nConcatenate the three tables together vertically in order with the oldest month first, adding ‘7Jul’, ‘8Aug’, and ‘9Sep’ as keys for their respective months, and save to variable avg_inv_by_month.\nUse the .agg() method to find the average of the total column from the grouped invoices.\nCreate a bar chart of avg_inv_by_month.\n\n# Concatenate the tables and add keys\ninv_jul_thr_sep = pd.concat([inv_jul, inv_aug, inv_sep], \n                            keys=['7Jul', '8Aug', '9Sep'])\n\n# Group the invoices by the index keys and find avg of the total column\navg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({\"total\":\"mean\"})\n\n# Bar plot of avg_inv_by_month\navg_inv_by_month.plot(kind=\"bar\")\nplt.show()",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#concatenate-and-merge-to-find-common-songs",
    "href": "4_Joining_Data_with_pandas.html#concatenate-and-merge-to-find-common-songs",
    "title": "Joining Data with pandas",
    "section": "Concatenate and merge to find common songs",
    "text": "Concatenate and merge to find common songs\nThe senior leadership of the streaming service is requesting your help again. You are given the historical files for a popular playlist in the classical music genre in 2018 and 2019. Additionally, you are given a similar set of files for the most popular pop music genre playlist on the streaming service in 2018 and 2019. Your goal is to concatenate the respective files to make a large classical playlist table and overall popular music table. Then filter the classical music table using a semi join to return only the most popular classical music tracks.\nThe tables classic_18, classic_19, and pop_18, pop_19 have been loaded for you. Additionally, pandas has been loaded as pd. ### Instructions - Concatenate the classic_18 and classic_19 tables vertically where the index goes from 0 to n-1, and save to classic_18_19. - Concatenate the pop_18 and pop_19 tables vertically where the index goes from 0 to n-1, and save to pop_18_19. - With classic_18_19 on the left, merge it with pop_18_19 on tid using an inner join. - Use .isin() to filter classic_18_19 where tid is in classic_pop.\n# Concatenate the classic tables vertically\nclassic_18_19 = pd.concat([classic_18, classic_19],ignore_index=True) \n\n# Concatenate the pop tables vertically\npop_18_19 = pd.concat([pop_18, pop_19], ignore_index = True)\n\nWith classic_18_19 on the left, merge it with pop_18_19 on tid using an inner join.\nUse .isin() to filter classic_18_19 where tid is in classic_pop.\n\n# Concatenate the classic tables vertically\nclassic_18_19 = pd.concat([classic_18, classic_19], ignore_index=True)\n\n# Concatenate the pop tables vertically\npop_18_19 = pd.concat([pop_18, pop_19], ignore_index=True)\n\n# Merge classic_18_19 with pop_18_19\nclassic_pop = classic_18_19.merge(pop_18_19,on='tid')\n\n# Using .isin(), filter classic_18_19 rows where tid is in classic_pop\npopular_classic = classic_18_19[classic_18_19[\"tid\"].isin(classic_pop.tid)]\n\n# Print popular chart\nprint(popular_classic)",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#correlation-between-gdp-and-sp500",
    "href": "4_Joining_Data_with_pandas.html#correlation-between-gdp-and-sp500",
    "title": "Joining Data with pandas",
    "section": "Correlation between GDP and S&P500",
    "text": "Correlation between GDP and S&P500\nIn this exercise, you want to analyze stock returns from the S&P 500. You believe there may be a relationship between the returns of the S&P 500 and the GDP of the US. Merge the different datasets together to compute the correlation.\nTwo tables have been provided for you, named sp500, and gdp. As always, pandas has been imported for you as pd. Instructions - Use merge_ordered() to merge gdp and sp500 using a left join on year and date. Save the results as gdp_sp500. - Print gdp_sp500 and look at the returns for the year 2018.\ngdp = pd.read_csv(\"datasets/WorldBank_GDP.csv\")\nprint(gdp.head(4))\nsp500 = pd.read_csv(\"datasets/S&P500.csv\")\nsp500.head(3)\n    Country Name Country Code     Indicator Name  Year           GDP\n0          China          CHN  GDP (current US$)  2010  6.087160e+12\n1        Germany          DEU  GDP (current US$)  2010  3.417090e+12\n2          Japan          JPN  GDP (current US$)  2010  5.700100e+12\n3  United States          USA  GDP (current US$)  2010  1.499210e+13\n\n\n\n\n\n\n\n\n\n\n\nDate\nReturns\n\n\n\n\n0\n2008\n-38.49\n\n\n1\n2009\n23.45\n\n\n2\n2010\n12.78\n\n\n\n\n# Use merge_ordered() to merge gdp and sp500 on year and date\ngdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='Year', right_on='Date', \n                             how='left')\n\n# Print gdp_sp500\nprint(gdp_sp500)\n     Country Name Country Code  ...    Date  Returns\n0           China          CHN  ...  2010.0    12.78\n1         Germany          DEU  ...  2010.0    12.78\n2           Japan          JPN  ...  2010.0    12.78\n3   United States          USA  ...  2010.0    12.78\n4           China          CHN  ...  2011.0     0.00\n5         Germany          DEU  ...  2011.0     0.00\n6           Japan          JPN  ...  2011.0     0.00\n7   United States          USA  ...  2011.0     0.00\n8           China          CHN  ...  2012.0    13.41\n9         Germany          DEU  ...  2012.0    13.41\n10          Japan          JPN  ...  2012.0    13.41\n11  United States          USA  ...  2012.0    13.41\n12          China          CHN  ...  2012.0    13.41\n13        Germany          DEU  ...  2012.0    13.41\n14          Japan          JPN  ...  2012.0    13.41\n15  United States          USA  ...  2012.0    13.41\n16          China          CHN  ...  2013.0    29.60\n17        Germany          DEU  ...  2013.0    29.60\n18          Japan          JPN  ...  2013.0    29.60\n19  United States          USA  ...  2013.0    29.60\n20          China          CHN  ...  2014.0    11.39\n21        Germany          DEU  ...  2014.0    11.39\n22          Japan          JPN  ...  2014.0    11.39\n23  United States          USA  ...  2014.0    11.39\n24          China          CHN  ...  2015.0    -0.73\n25        Germany          DEU  ...  2015.0    -0.73\n26          Japan          JPN  ...  2015.0    -0.73\n27  United States          USA  ...  2015.0    -0.73\n28          China          CHN  ...  2016.0     9.54\n29        Germany          DEU  ...  2016.0     9.54\n30          Japan          JPN  ...  2016.0     9.54\n31  United States          USA  ...  2016.0     9.54\n32          China          CHN  ...  2017.0    19.42\n33        Germany          DEU  ...  2017.0    19.42\n34          Japan          JPN  ...  2017.0    19.42\n35  United States          USA  ...  2017.0    19.42\n36          China          CHN  ...     NaN      NaN\n37        Germany          DEU  ...     NaN      NaN\n38          Japan          JPN  ...     NaN      NaN\n39  United States          USA  ...     NaN      NaN\n\n[40 rows x 7 columns]\n\nUse merge_ordered(), again similar to before, to merge gdp and sp500 use the function’s ability to interpolate missing data to forward fill the missing value for returns, assigning this table to the variable gdp_sp500.\n\n# Use merge_ordered() to merge gdp and sp500, interpolate missing value\ngdp_sp500 = pd.merge_ordered(gdp, sp500,left_on='Year',right_on='Date',how=\"left\",fill_method='ffill')\n\n\n# Print gdp_sp500\nprint (gdp_sp500)\n     Country Name Country Code     Indicator Name  ...           GDP  Date  Returns\n0           China          CHN  GDP (current US$)  ...  6.087160e+12  2010    12.78\n1         Germany          DEU  GDP (current US$)  ...  3.417090e+12  2010    12.78\n2           Japan          JPN  GDP (current US$)  ...  5.700100e+12  2010    12.78\n3   United States          USA  GDP (current US$)  ...  1.499210e+13  2010    12.78\n4           China          CHN  GDP (current US$)  ...  7.551500e+12  2011     0.00\n5         Germany          DEU  GDP (current US$)  ...  3.757700e+12  2011     0.00\n6           Japan          JPN  GDP (current US$)  ...  6.157460e+12  2011     0.00\n7   United States          USA  GDP (current US$)  ...  1.554260e+13  2011     0.00\n8           China          CHN  GDP (current US$)  ...  8.532230e+12  2012    13.41\n9         Germany          DEU  GDP (current US$)  ...  3.543980e+12  2012    13.41\n10          Japan          JPN  GDP (current US$)  ...  6.203210e+12  2012    13.41\n11  United States          USA  GDP (current US$)  ...  1.619700e+13  2012    13.41\n12          China          CHN  GDP (current US$)  ...  8.532230e+12  2012    13.41\n13        Germany          DEU  GDP (current US$)  ...  3.543980e+12  2012    13.41\n14          Japan          JPN  GDP (current US$)  ...  6.203210e+12  2012    13.41\n15  United States          USA  GDP (current US$)  ...  1.619700e+13  2012    13.41\n16          China          CHN  GDP (current US$)  ...  9.570410e+12  2013    29.60\n17        Germany          DEU  GDP (current US$)  ...  3.752510e+12  2013    29.60\n18          Japan          JPN  GDP (current US$)  ...  5.155720e+12  2013    29.60\n19  United States          USA  GDP (current US$)  ...  1.678480e+13  2013    29.60\n20          China          CHN  GDP (current US$)  ...  1.043850e+13  2014    11.39\n21        Germany          DEU  GDP (current US$)  ...  3.898730e+12  2014    11.39\n22          Japan          JPN  GDP (current US$)  ...  4.850410e+12  2014    11.39\n23  United States          USA  GDP (current US$)  ...  1.752170e+13  2014    11.39\n24          China          CHN  GDP (current US$)  ...  1.101550e+13  2015    -0.73\n25        Germany          DEU  GDP (current US$)  ...  3.381390e+12  2015    -0.73\n26          Japan          JPN  GDP (current US$)  ...  4.389480e+12  2015    -0.73\n27  United States          USA  GDP (current US$)  ...  1.821930e+13  2015    -0.73\n28          China          CHN  GDP (current US$)  ...  1.113790e+13  2016     9.54\n29        Germany          DEU  GDP (current US$)  ...  3.495160e+12  2016     9.54\n30          Japan          JPN  GDP (current US$)  ...  4.926670e+12  2016     9.54\n31  United States          USA  GDP (current US$)  ...  1.870720e+13  2016     9.54\n32          China          CHN  GDP (current US$)  ...  1.214350e+13  2017    19.42\n33        Germany          DEU  GDP (current US$)  ...  3.693200e+12  2017    19.42\n34          Japan          JPN  GDP (current US$)  ...  4.859950e+12  2017    19.42\n35  United States          USA  GDP (current US$)  ...  1.948540e+13  2017    19.42\n36          China          CHN  GDP (current US$)  ...  1.360820e+13  2017    19.42\n37        Germany          DEU  GDP (current US$)  ...  3.996760e+12  2017    19.42\n38          Japan          JPN  GDP (current US$)  ...  4.970920e+12  2017    19.42\n39  United States          USA  GDP (current US$)  ...  2.049410e+13  2017    19.42\n\n[40 rows x 7 columns]\n\nSubset the gdp_sp500 table, select the gdp and returns columns, and save as gdp_returns.\nPrint the correlation matrix of the gdp_returns table using the .corr() method.\n\n# Use merge_ordered() to merge gdp and sp500, interpolate missing value\ngdp_sp500 = pd.merge_ordered(gdp, sp500, left_on='Year', right_on='Date', \n                             how='left',  fill_method='ffill')\n\n# Subset the gdp and returns columns\ngdp_returns = gdp_sp500[[\"GDP\",\"Returns\"]]\n\n# Print gdp_returns correlation\nprint (gdp_returns.corr())\n              GDP   Returns\nGDP      1.000000  0.040669\nReturns  0.040669  1.000000",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#phillips-curve-using-merge_ordered",
    "href": "4_Joining_Data_with_pandas.html#phillips-curve-using-merge_ordered",
    "title": "Joining Data with pandas",
    "section": "Phillips curve using merge_ordered()",
    "text": "Phillips curve using merge_ordered()\nThere is an economic theory developed by A. W. Phillips which states that inflation and unemployment have an inverse relationship. The theory claims that with economic growth comes inflation, which in turn should lead to more jobs and less unemployment.\nYou will take two tables of data from the U.S. Bureau of Labor Statistics, containing unemployment and inflation data over different periods, and create a Phillips curve. The tables have different frequencies. One table has a data entry every six months, while the other has a data entry every month. You will need to use the entries where you have data within both tables.\nThe tables unemployment and inflation have been loaded for you. ### Instructions - Use merge_ordered() to merge the inflation and unemployment tables on date with an inner join, and save the results as inflation_unemploy. - Print the inflation_unemploy variable. - Using inflation_unemploy, create a scatter plot with unemployment_rate on the horizontal axis and cpi (inflation) on the vertical axis.\n# Use merge_ordered() to merge inflation, unemployment with inner join\ninflation_unemploy = pd.merge_ordered(inflation, unemployment, how=\"inner\", on = \"date\")\n\n# Print inflation_unemploy \nprint(inflation_unemploy)\n\n# Plot a scatter plot of unemployment_rate vs cpi of inflation_unemploy\ninflation_unemploy.plot(x=\"unemployment_rate\", y = \"cpi\", kind='scatter')\nplt.show()",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#merge_ordered-caution-multiple-columns",
    "href": "4_Joining_Data_with_pandas.html#merge_ordered-caution-multiple-columns",
    "title": "Joining Data with pandas",
    "section": "merge_ordered() caution, multiple columns",
    "text": "merge_ordered() caution, multiple columns\nWhen using merge_ordered() to merge on multiple columns, the order is important when you combine it with the forward fill feature. The function sorts the merge on columns in the order provided. In this exercise, we will merge GDP and population data from the World Bank for the Australia and Sweden, reversing the order of the merge on columns. The frequency of the series are different, the GDP values are quarterly, and the population is yearly. Use the forward fill feature to fill in the missing data. Depending on the order provided, the fill forward will use unintended data to fill in the missing values.\nThe tables gdp and pop have been loaded. ### Instructions - Use merge_ordered() on gdp and pop, merging on columns date and country with the fill feature, save to ctry_date. - Perform the same merge of gdp and pop, but join on country and date (reverse of step 1) with the fill feature, saving this as date_ctry.\npop = pd.read_csv(\"datasets/WorldBank_POP.csv\")\npop.head(3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry Name\nCountry Code\nIndicator Name\nYear\nPop\n\n\n\n\n0\nAruba\nABW\nPopulation, total\n2010\n101669.0\n\n\n1\nAfghanistan\nAFG\nPopulation, total\n2010\n29185507.0\n\n\n2\nAngola\nAGO\nPopulation, total\n2010\n23356246.0\n\n\n\n\n# Merge gdp and pop on date and country with fill and notice rows 2 and 3\nctry_date = pd.merge_ordered(gdp,pop,on=['Year','Country Name'], \n                             fill_method='ffill')\n\n# Print ctry_date\nprint(ctry_date)\n            Country Name Country Code_x  ...   Indicator Name_y           Pop\n0            Afghanistan            NaN  ...  Population, total  2.918551e+07\n1                Albania            NaN  ...  Population, total  2.913021e+06\n2                Algeria            NaN  ...  Population, total  3.597746e+07\n3         American Samoa            NaN  ...  Population, total  5.607900e+04\n4                Andorra            NaN  ...  Population, total  8.444900e+04\n...                  ...            ...  ...                ...           ...\n2643  West Bank and Gaza            USA  ...  Population, total  4.569087e+06\n2644               World            USA  ...  Population, total  7.594270e+09\n2645         Yemen, Rep.            USA  ...  Population, total  2.849869e+07\n2646              Zambia            USA  ...  Population, total  1.735182e+07\n2647            Zimbabwe            USA  ...  Population, total  1.443902e+07\n\n[2648 rows x 8 columns]\n# Merge gdp and pop on country and date with fill\ndate_ctry = pd.merge_ordered(gdp,pop,on = [\"Country Name\",\"Year\"],fill_method='ffill')\n\n# Print date_ctry\nprint(date_ctry)\n     Country Name Country Code_x  ...   Indicator Name_y         Pop\n0     Afghanistan            NaN  ...  Population, total  29185507.0\n1     Afghanistan            NaN  ...  Population, total  30117413.0\n2     Afghanistan            NaN  ...  Population, total  31161376.0\n3     Afghanistan            NaN  ...  Population, total  31161376.0\n4     Afghanistan            NaN  ...  Population, total  32269589.0\n...           ...            ...  ...                ...         ...\n2643     Zimbabwe            USA  ...  Population, total  13586681.0\n2644     Zimbabwe            USA  ...  Population, total  13814629.0\n2645     Zimbabwe            USA  ...  Population, total  14030390.0\n2646     Zimbabwe            USA  ...  Population, total  14236745.0\n2647     Zimbabwe            USA  ...  Population, total  14439018.0\n\n[2648 rows x 8 columns]",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#using-merge_asof-to-study-stocks",
    "href": "4_Joining_Data_with_pandas.html#using-merge_asof-to-study-stocks",
    "title": "Joining Data with pandas",
    "section": "Using merge_asof() to study stocks",
    "text": "Using merge_asof() to study stocks\nYou have a feed of stock market prices that you record. You attempt to track the price every five minutes. Still, due to some network latency, the prices you record are roughly every 5 minutes. You pull your price logs for three banks, JP Morgan (JPM), Wells Fargo (WFC), and Bank Of America (BAC). You want to know how the price change of the two other banks compare to JP Morgan. Therefore, you will need to merge these three logs into one table. Afterward, you will use the pandas .diff() method to compute the price change over time. Finally, plot the price changes so you can review your analysis.\nThe three log files have been loaded for you as tables named jpm, wells, and bac. ## Instructions - Use merge_asof() to merge jpm (left table) and wells together on the date_time column, where the rows with the nearest times are matched, and with suffixes=(’‘,’_wells’). Save to jpm_wells. - Use merge_asof() to merge jpm_wells (left table) and bac together on the date_time column, where the rows with the closest times are matched, and with suffixes=(‘_jpm’, ‘_bac’). Save to jpm_wells_bac. - Using price_diffs, create a line plot of the close price of JPM, WFC, and BAC only.\n# Use merge_asof() to merge jpm and wells\njpm_wells = pd.merge_asof(jpm, wells, on='date_time', \n                          suffixes=('', '_wells'), direction='nearest')\n\n# Use merge_asof() to merge jpm_wells and bac\njpm_wells_bac = pd.merge_asof(jpm_wells, bac, on='date_time', \n                              suffixes=('_jpm', '_bac'), direction='nearest')\n\n# Compute price diff\nprice_diffs = jpm_wells_bac.diff()\n\n# Plot the price diff of the close of jpm, wells and bac only\nprice_diffs.plot(y=['close_jpm','close_wells','close_bac'])\nplt.show()",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#using-merge_asof-to-create-dataset",
    "href": "4_Joining_Data_with_pandas.html#using-merge_asof-to-create-dataset",
    "title": "Joining Data with pandas",
    "section": "Using merge_asof() to create dataset",
    "text": "Using merge_asof() to create dataset\nThe merge_asof() function can be used to create datasets where you have a table of start and stop dates, and you want to use them to create a flag in another table. You have been given gdp, which is a table of quarterly GDP values of the US during the 1980s. Additionally, the table recession has been given to you. It holds the starting date of every US recession since 1980, and the date when the recession was declared to be over. Use merge_asof() to merge the tables and create a status flag if a quarter was during a recession. Finally, to check your work, plot the data in a bar chart.\nThe tables gdp and recession have been loaded for you. ### Instructions - Using merge_asof(), merge gdp and recession on date, with gdp as the left table. Save to the variable gdp_recession. - Create a list using a list comprehension and a conditional expression, named is_recession, where for each row if the gdp_recession[‘econ_status’] value is equal to ‘recession’ then enter ‘r’ else ‘g’. - Using gdp_recession, plot a bar chart of gdp versus date, setting the color argument equal to is_recession.\n# Merge gdp and recession on date using merge_asof()\ngdp_recession = pd.merge_asof(gdp, recession, on='date')\n\n# Create a list based on the row value of gdp_recession['econ_status']\nis_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n\n# Plot a bar chart of gdp_recession\ngdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)\nplt.show()",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#subsetting-rows-with-.query",
    "href": "4_Joining_Data_with_pandas.html#subsetting-rows-with-.query",
    "title": "Joining Data with pandas",
    "section": "Subsetting rows with .query()",
    "text": "Subsetting rows with .query()\nIn this exercise, you will revisit GDP and population data for Australia and Sweden from the World Bank and expand on it using the .query() method. You’ll merge the two tables and compute the GDP per capita. Afterwards, you’ll use the .query() method to sub-select the rows and create a plot. Recall that you will need to merge on multiple columns in the proper order.\nThe tables gdp and pop have been loaded for you. ### Instructions - Use merge_ordered() on gdp and pop on columns country and date with the fill feature, save to gdp_pop and print. - Add a column named gdp_per_capita to gdp_pop that divides gdp by pop. - Pivot gdp_pop so values=‘gdp_per_capita’, index=‘date’, and columns=‘country’, save as gdp_pivot. - Use .query() to select rows from gdp_pivot where date is greater than equal to “1991-01-01”. Save as recent_gdp_pop.\n# Merge gdp and pop on date and country with fill\ngdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n# Merge gdp and pop on date and country with fill\ngdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n\n# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\ngdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n# Merge gdp and pop on date and country with fill\ngdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n\n# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\ngdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n\n# Pivot table of gdp_per_capita, where index is date and columns is country\ngdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n# Merge gdp and pop on date and country with fill\ngdp_pop = pd.merge_ordered(gdp, pop, on=['country','date'], fill_method='ffill')\n\n# Add a column named gdp_per_capita to gdp_pop that divides the gdp by pop\ngdp_pop['gdp_per_capita'] = gdp_pop['gdp'] / gdp_pop['pop']\n\n# Pivot data so gdp_per_capita, where index is date and columns is country\ngdp_pivot = gdp_pop.pivot_table('gdp_per_capita', 'date', 'country')\n\n# Select dates equal to or greater than 1991-01-01\nrecent_gdp_pop = gdp_pivot.query('date &gt;= \"1991-01-01\"')\n\n# Plot recent_gdp_pop\nrecent_gdp_pop.plot(rot=90)\nplt.show()",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#using-.melt-to-reshape-government-data",
    "href": "4_Joining_Data_with_pandas.html#using-.melt-to-reshape-government-data",
    "title": "Joining Data with pandas",
    "section": "Using .melt() to reshape government data",
    "text": "Using .melt() to reshape government data\nThe US Bureau of Labor Statistics (BLS) often provides data series in an easy-to-read format - it has a separate column for each month, and each year is a different row. Unfortunately, this wide format makes it difficult to plot this information over time. In this exercise, you will reshape a table of US unemployment rate data from the BLS into a form you can plot using .melt(). You will need to add a date column to the table and sort by it to plot the data correctly.\nThe unemployment rate data has been loaded for you in a table called ur_wide. You are encouraged to explore this table before beginning the exercise. ### Instructions - Use .melt() to unpivot all of the columns of ur_wide except year and ensure that the columns with the months and values are named month and unempl_rate, respectively. Save the result as ur_tall. - Add a column to ur_tall named date which combines the year and month columns as year-month format into a larger string, and converts it to a date data type. - Sort ur_tall by date and save as ur_sorted. - Using ur_sorted, plot unempl_rate on the y-axis and date on the x-axis.\n# Unpivot everything besides the year column\nur_tall = ur_wide.melt(id_vars=['year'], var_name='month', \n                       value_name='unempl_rate')\n\n# Create a date column using the month and year columns of ur_tall\nur_tall['date'] = pd.to_datetime(ur_tall['month'] + '-' + ur_tall['year'])\n\n# Sort ur_tall by date in ascending order\nur_sorted = ur_tall.sort_values('date')\n\n# Plot the unempl_rate by date\nur_sorted.plot(x='date', y='unempl_rate')\nplt.show()",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "4_Joining_Data_with_pandas.html#using-.melt-for-stocks-vs-bond-performance",
    "href": "4_Joining_Data_with_pandas.html#using-.melt-for-stocks-vs-bond-performance",
    "title": "Joining Data with pandas",
    "section": "Using .melt() for stocks vs bond performance",
    "text": "Using .melt() for stocks vs bond performance\nIt is widespread knowledge that the price of bonds is inversely related to the price of stocks. In this last exercise, you’ll review many of the topics in this chapter to confirm this. You have been given a table of percent change of the US 10-year treasury bond price. It is in a wide format where there is a separate column for each year. You will need to use the .melt() method to reshape this table.\nAdditionally, you will use the .query() method to filter out unneeded data. You will merge this table with a table of the percent change of the Dow Jones Industrial stock index price. Finally, you will plot data.\nThe tables ten_yr and dji have been loaded for you. ### Instructions - Use .melt() on ten_yr to unpivot everything except the metric column, setting var_name=‘date’ and value_name=‘close’. Save the result to bond_perc. - Using the .query() method, select only those rows were metric equals ‘close’, and save to bond_perc_close. - Use merge_ordered() to merge dji (left table) and bond_perc_close on date with an inner join, and set suffixes equal to (‘_dow’, ‘_bond’). Save the result to dow_bond. - Using dow_bond, plot only the Dow and bond values.\n# Use melt on ten_yr, unpivot everything besides the metric column\nbond_perc = ten_yr.melt(id_vars='metric', var_name='date', value_name='close')\n\n# Use query on bond_perc to select only the rows where metric=close\nbond_perc_close = bond_perc.query('metric == \"close\"')\n\n# Merge (ordered) dji and bond_perc_close on date with an inner join\ndow_bond = pd.merge_ordered(dji, bond_perc_close, on='date', \n                            suffixes=('_dow', '_bond'), how='inner')\n\n# Plot only the close_dow and close_bond columns\ndow_bond.plot(y=['close_dow', 'close_bond'], x='date', rot=90)\nplt.show()",
    "crumbs": [
      "Joining Data with pandas"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html",
    "href": "5_Introduction_to_statistics_in_python.html",
    "title": "Introduction to Statistics in Python",
    "section": "",
    "text": "Chapter 1: Summary Statistics\nSummary statistics gives you the tools you need to boil down massive datasets to reveal the highlights. In this chapter, you’ll explore summary statistics including mean, median, and standard deviation, and learn how to accurately interpret them. You’ll also develop your critical thinking skills, allowing you to choose the best summary statistics for your data.",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#mean-and-median",
    "href": "5_Introduction_to_statistics_in_python.html#mean-and-median",
    "title": "Introduction to Statistics in Python",
    "section": "Mean and median",
    "text": "Mean and median\nIn this chapter, you’ll be working with the 2018 Food Carbon Footprint Index from nu3. The food_consumption dataset contains information about the kilograms of food consumed per person per year in each country in each food category (consumption) as well as information about the carbon footprint of that food category (co2_emissions) measured in kilograms of carbon dioxide, or CO2, per person per year in each country.\nIn this exercise, you’ll compute measures of center to compare food consumption in the US and Belgium using your pandas and numpy skills.\npandas is imported as pd for you and food_consumption is pre-loaded. ### Instructions - Import numpy with the alias np. - Create two DataFrames: one that holds the rows of food_consumption for ‘Belgium’ and another that holds rows for ‘USA’. Call these be_consumption and usa_consumption. - Calculate the mean and median of kilograms of food consumed per person per year for both countries. - Subset food_consumption for rows with data about Belgium and the USA. - Group the subsetted data by country and select only the consumption column. - Calculate the mean and median of the kilograms of food consumed per person per year in each country using .agg().\n# Import numpy with alias np\nimport numpy as np\n\n# Filter for Belgium\nbe_consumption = food_consumption[food_consumption.country==\"Belgium\"]\n\n# Filter for USA\nusa_consumption = food_consumption[food_consumption.country==\"USA\"]\n\n\n# Calculate mean and median consumption in Belgium\nprint(be_consumption[\"consumption\"].mean())\nprint(be_consumption[\"consumption\"].median)\n\n# Calculate mean and median consumption in USA\nprint(usa_consumption.consumption.mean())\nprint(usa_consumption.consumption.median)\n42.13272727272727\n&lt;bound method NDFrame._add_numeric_operations.&lt;locals&gt;.median of 396     38.65\n397     12.20\n398     15.63\n399      1.32\n400     18.97\n401     12.59\n402    236.19\n403    111.91\n404      8.61\n405      0.07\n406      7.32\nName: consumption, dtype: float64&gt;\n44.650000000000006\n&lt;bound method NDFrame._add_numeric_operations.&lt;locals&gt;.median of 55     27.64\n56     50.01\n57     36.24\n58      0.43\n59     12.35\n60     14.58\n61    254.69\n62     80.43\n63      6.88\n64      0.04\n65      7.86\nName: consumption, dtype: float64&gt;",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#mean-vs.-median",
    "href": "5_Introduction_to_statistics_in_python.html#mean-vs.-median",
    "title": "Introduction to Statistics in Python",
    "section": "Mean vs. median",
    "text": "Mean vs. median\nIn the video, you learned that the mean is the sum of all the data points divided by the total number of data points, and the median is the middle value of the dataset where 50% of the data is less than the median, and 50% of the data is greater than the median. In this exercise, you’ll compare these two measures of center.\npandas is loaded as pd, numpy is loaded as np, and food_consumption is available. ### Instructions - Import matplotlib.pyplot with the alias plt. - Subset food_consumption to get the rows where food_category is ‘rice’. - Create a histogram of co2_emission for rice and show the plot.\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Subset for food_category equals rice\nrice_consumption = food_consumption[food_consumption.food_category=='rice']\n\n# Histogram of co2_emission for rice and show plot\n\n#rice_consumption.plot(x='consumption', y = 'co2_emission',kind='bar')\nplt.hist(rice_consumption[\"co2_emission\"])\nplt.show()\n\n\n\npng\n\n\n\nUse .agg() to calculate the mean and median of co2_emission for rice.\n\n# Subset for food_category equals rice\nrice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n\n# Calculate mean and median of co2_emission with .agg()\nprint(rice_consumption.co2_emission.agg([\"mean\",\"median\"]))\nmean      37.591615\nmedian    15.200000\nName: co2_emission, dtype: float64",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#quartiles-quantiles-and-quintiles",
    "href": "5_Introduction_to_statistics_in_python.html#quartiles-quantiles-and-quintiles",
    "title": "Introduction to Statistics in Python",
    "section": "Quartiles, quantiles, and quintiles",
    "text": "Quartiles, quantiles, and quintiles\nQuantiles are a great way of summarizing numerical data since they can be used to measure center and spread, as well as to get a sense of where a data point stands in relation to the rest of the data set. For example, you might want to give a discount to the 10% most active users on a website.\nIn this exercise, you’ll calculate quartiles, quintiles, and deciles, which split up a dataset into 4, 5, and 10 pieces, respectively.\nBoth pandas as pd and numpy as np are loaded and food_consumption is available. ### Instructions - Calculate the quartiles of the co2_emission column of food_consumption. - Calculate the six quantiles that split up the data into 5 pieces (quintiles) of the co2_emission column of food_consumption. - Calculate the eleven quantiles of co2_emission that split up the data into ten pieces (deciles).\n# Calculate the quartiles of co2_emission\nprint(np.quantile(food_consumption.co2_emission,[0,0.25,0.5,0.75,1]))\n[   0.        5.21     16.53     62.5975 1712.    ]\n# Calculate the quintiles of co2_emission\nprint(np.quantile(food_consumption.co2_emission, np.linspace(0,1,6)))\n[   0.       3.54    11.026   25.59    99.978 1712.   ]\n# Calculate the deciles of co2_emission\nprint(np.quantile(food_consumption.co2_emission,np.linspace(0,1,11)))\n[0.00000e+00 6.68000e-01 3.54000e+00 7.04000e+00 1.10260e+01 1.65300e+01\n 2.55900e+01 4.42710e+01 9.99780e+01 2.03629e+02 1.71200e+03]",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#variance-and-standard-deviation",
    "href": "5_Introduction_to_statistics_in_python.html#variance-and-standard-deviation",
    "title": "Introduction to Statistics in Python",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\nVariance and standard deviation are two of the most common ways to measure the spread of a variable, and you’ll practice calculating these in this exercise. Spread is important since it can help inform expectations. For example, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10 products, there will probably be days where they sell 40 products, but also days where they only sell one or two. Information like this is important, especially when making predictions.\nBoth pandas as pd and numpy as np are loaded, and food_consumption is available. ### Instructions - Calculate the variance and standard deviation of co2_emission for each food_category by grouping and aggregating. - Import matplotlib.pyplot with alias plt. - Create a histogram of co2_emission for the beef food_category and show the plot. - Create a histogram of co2_emission for the eggs food_category and show the plot.\n# Print variance and sd of co2_emission for each food_category\nprint(food_consumption.groupby('food_category')['co2_emission'].agg(['var','std']))\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Create histogram of co2_emission for food_category 'beef'\nplt.hist(food_consumption[food_consumption.food_category=='beef'].co2_emission)\n# Show plot\nplt.show()\n\n# Create histogram of co2_emission for food_category 'eggs'\nplt.hist(food_consumption[food_consumption.food_category=='eggs'].co2_emission)\n# Show plot\nplt.show()\n                        var         std\nfood_category                          \nbeef           88748.408132  297.906710\ndairy          17671.891985  132.935669\neggs              21.371819    4.622966\nfish             921.637349   30.358481\nlamb_goat      16475.518363  128.356996\nnuts              35.639652    5.969895\npork            3094.963537   55.632396\npoultry          245.026801   15.653332\nrice            2281.376243   47.763754\nsoybeans           0.879882    0.938020\nwheat             71.023937    8.427570\n\n\n\npng\n\n\n\n\n\npng",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#finding-outliers-using-iqr",
    "href": "5_Introduction_to_statistics_in_python.html#finding-outliers-using-iqr",
    "title": "Introduction to Statistics in Python",
    "section": "Finding outliers using IQR",
    "text": "Finding outliers using IQR\nOutliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that’s less influenced by outliers. IQR is also often used to find outliers. If a value is less than Q1-1.5 x IQR or greater than Q1 + 1.5 x IQR, it’s considered an outlier. In fact, this is how the lengths of the whiskers in a matplotlib box plot are calculated.\nIn this exercise, you’ll calculate IQR and use it to find some outliers. pandas as pd and numpy as np are loaded and food_consumption is available.\n\nInstructions\n\nCalculate the total co2_emission per country by grouping by country and taking the sum of co2_emission. Store the resulting DataFrame as emissions_by_country.\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby(\"country\").agg({\"co2_emission\":\"sum\"})\n\nprint(emissions_by_country)\n\nemissions_by_country = food_consumption.groupby(\"country\")[\"co2_emission\"].sum()\n\nprint(emissions_by_country)\n           co2_emission\ncountry                \nAlbania         1777.85\nAlgeria          707.88\nAngola           412.99\nArgentina       2172.40\nArmenia         1109.93\n...                 ...\nUruguay         1634.91\nVenezuela       1104.10\nVietnam          641.51\nZambia           225.30\nZimbabwe         350.33\n\n[130 rows x 1 columns]\ncountry\nAlbania      1777.85\nAlgeria       707.88\nAngola        412.99\nArgentina    2172.40\nArmenia      1109.93\n              ...   \nUruguay      1634.91\nVenezuela    1104.10\nVietnam       641.51\nZambia        225.30\nZimbabwe      350.33\nName: co2_emission, Length: 130, dtype: float64\n\nCompute the first and third quartiles of emissions_by_country and store these as q1 and q3.\nCalculate the interquartile range of emissions_by_country and store it as iqr.\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quartiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country,0.25)\nq3 = np.quantile(emissions_by_country,0.75)\niqr = q3-q1\n\nCalculate the lower and upper cutoffs for outliers of emissions_by_country, and store these as lower and upper.\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quantiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5*iqr\nupper = q3 + 1.5*iqr\n\nSubset emissions_by_country to get countries with a total emission greater than the upper cutoff or a total emission less than the lower cutoff.\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\n\n# Compute the first and third quantiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = q3 - q1\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\n\n# Subset emissions_by_country to find outliers\noutliers = emissions_by_country[(emissions_by_country&lt;lower) | (emissions_by_country&gt;upper)]\nprint(outliers)\ncountry\nArgentina    2172.4\nName: co2_emission, dtype: float64",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#calculating-probabilities",
    "href": "5_Introduction_to_statistics_in_python.html#calculating-probabilities",
    "title": "Introduction to Statistics in Python",
    "section": "Calculating probabilities",
    "text": "Calculating probabilities\nYou’re in charge of the sales team, and it’s time for performance reviews, starting with Amir. As part of the review, you want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals.\n\nInstructions\n\nCount the number of deals Amir worked on for each product type and store in counts.\nCalculate the probability of selecting a deal for the different product types by dividing the counts by the total number of deals Amir worked on. Save this as probs.\n\n# Count the deals for each product\ncounts = amir_deals['product'].value_counts()\nprint(counts)\nProduct B    62\nProduct D    40\nProduct A    23\nProduct C    15\nProduct F    11\nProduct H     8\nProduct I     7\nProduct E     5\nProduct N     3\nProduct G     2\nProduct J     2\nName: product, dtype: int64\n# Count the deals for each product\ncounts = amir_deals['product'].value_counts()\n\n# Calculate probability of picking a deal with each product\nprobs = counts/np.sum(counts)\nprint(probs)\nProduct B    0.348315\nProduct D    0.224719\nProduct A    0.129213\nProduct C    0.084270\nProduct F    0.061798\nProduct H    0.044944\nProduct I    0.039326\nProduct E    0.028090\nProduct N    0.016854\nProduct G    0.011236\nProduct J    0.011236\nName: product, dtype: float64",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#sampling-deals",
    "href": "5_Introduction_to_statistics_in_python.html#sampling-deals",
    "title": "Introduction to Statistics in Python",
    "section": "Sampling deals",
    "text": "Sampling deals\nIn the previous exercise, you counted the deals Amir worked on. Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement.\nAdditionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed before sampling from the deals.\nBoth pandas as pd and numpy as np are loaded and amir_deals is available. ### Instructions - Set the random seed to 24. - Take a sample of 5 deals without replacement and store them as sample_without_replacement. - Take a sample of 5 deals with replacement and save as sample_with_replacement.\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals without replacement\nsample_without_replacement = amir_deals.sample(5, replace=False)\nprint(sample_without_replacement)\n     Unnamed: 0    product   client status   amount  num_users\n127         128  Product B  Current    Won  2070.25          7\n148         149  Product D  Current    Won  3485.48         52\n77           78  Product B  Current    Won  6252.30         27\n104         105  Product D  Current    Won  4110.98         39\n166         167  Product C      New   Lost  3779.86         11\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals with replacement\nsample_with_replacement = amir_deals.sample(5,replace=True)\nprint(sample_with_replacement)\n     Unnamed: 0    product   client status   amount  num_users\n162         163  Product D  Current    Won  6755.66         59\n131         132  Product B  Current    Won  6872.29         25\n87           88  Product C  Current    Won  3579.63          3\n145         146  Product A  Current    Won  4682.94         63\n145         146  Product A  Current    Won  4682.94         63",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#creating-a-probability-distribution",
    "href": "5_Introduction_to_statistics_in_python.html#creating-a-probability-distribution",
    "title": "Introduction to Statistics in Python",
    "section": "Creating a probability distribution",
    "text": "Creating a probability distribution\nA new restaurant opened a few months ago, and the restaurant’s management wants to optimize its seating space based on the size of the groups that come most often. On one night, there are 10 groups of people waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. In this exercise, you’ll investigate the probability of groups of different sizes getting picked first. Data on each of the ten groups is contained in the restaurant_groups DataFrame.\nRemember that expected value can be calculated by multiplying each possible outcome with its corresponding probability and taking the sum. The restaurant_groups data is available. pandas is loaded as pd, numpy is loaded as np, and matplotlib.pyplot is loaded as plt. ### Instructions - Create a histogram of the group_size column of restaurant_groups, setting bins to [2, 3, 4, 5, 6]. Remember to show the plot. - Count the number of each group_size in restaurant_groups, then divide by the number of rows in restaurant_groups to calculate the probability of randomly selecting a group of each size. Save as size_dist. - Reset the index of size_dist. - Rename the columns of size_dist to group_size and prob. - Calculate the expected value of the size_dist, which represents the expected group size, by multiplying the group_size by the prob and taking the sum. - Calculate the probability of randomly picking a group of 4 or more people by subsetting for groups of size 4 or more and summing the probabilities of selecting those groups.\nrestaurant_groups = pd.read_csv(\"datasets/resturant_groups.csv\")\nrestaurant_groups\n\n\n\n\n\n\n\n\n\n\n\ngroup_id\ngroup_size\n\n\n\n\n0\nA\n2\n\n\n1\nB\n4\n\n\n2\nC\n6\n\n\n3\nD\n2\n\n\n4\nE\n2\n\n\n5\nF\n2\n\n\n6\nG\n3\n\n\n7\nH\n2\n\n\n8\nI\n4\n\n\n9\nJ\n2\n\n\n\n\n# Create a histogram of restaurant_groups and show plot\nrestaurant_groups['group_size'].hist(bins=[2,3,4,5,6])\nplt.show()\n\n\n\npng\n\n\n# Create probability distribution\nsize_dist = restaurant_groups[\"group_size\"] / restaurant_groups.shape[0]\n\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = [\"group_size\", \"prob\"]\n\nprint(size_dist)\n   group_size  prob\n0           0   0.2\n1           1   0.4\n2           2   0.6\n3           3   0.2\n4           4   0.2\n5           5   0.2\n6           6   0.3\n7           7   0.2\n8           8   0.4\n9           9   0.2\n# Create probability distribution\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\n\n# Calculate expected value\nexpected_value = np.sum(size_dist.group_size*size_dist.prob)\nprint(expected_value)\n2.9000000000000004\n# Create probability distribution\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\n\n# Expected value\nexpected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n\n# Subset groups of size 4 or more\ngroups_4_or_more = size_dist[size_dist.group_size&gt;=4]\n\n# Sum the probabilities of groups_4_or_more\nprob_4_or_more = groups_4_or_more.prob.sum()\nprint(prob_4_or_more)\n0.30000000000000004",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#data-back-ups",
    "href": "5_Introduction_to_statistics_in_python.html#data-back-ups",
    "title": "Introduction to Statistics in Python",
    "section": "Data back-ups",
    "text": "Data back-ups\nThe sales software used at your company is set to automatically back itself up, but no one knows exactly what time the back-ups happen. It is known, however, that back-ups happen exactly every 30 minutes. Amir comes back from sales meetings at random times to update the data on the client he just met with. He wants to know how long he’ll have to wait for his newly-entered data to get backed up. Use your new knowledge of continuous uniform distributions to model this situation and answer Amir’s questions.\n\nInstructions\n\nTo model how long Amir will wait for a back-up using a continuous uniform distribution, save his lowest possible wait time as min_time and his longest possible wait time as max_time. Remember that back-ups happen every 30 minutes.\nImport uniform from scipy.stats and calculate the probability that Amir has to wait less than 5 minutes, and store in a variable called prob_less_than_5.\nCalculate the probability that Amir has to wait more than 5 minutes, and store in a variable called prob_greater_than_5.\nCalculate the probability that Amir has to wait between 10 and 20 minutes, and store in a variable called prob_between_10_and_20.\n\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Import uniform from scipy.stats\nfrom scipy.stats import uniform\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 = uniform.cdf(5,min_time,max_time)\nprint(prob_less_than_5)\n0.16666666666666666\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Import uniform from scipy.stats\nfrom scipy.stats import uniform\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 = 1-uniform.cdf(5,min_time,max_time)\nprint(prob_greater_than_5)\n0.8333333333333334\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Import uniform from scipy.stats\nfrom scipy.stats import uniform\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20,min_time,max_time) - uniform.cdf(10,min_time,max_time)\nprint(prob_between_10_and_20)\n0.3333333333333333",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#simulating-wait-times",
    "href": "5_Introduction_to_statistics_in_python.html#simulating-wait-times",
    "title": "Introduction to Statistics in Python",
    "section": "Simulating wait times",
    "text": "Simulating wait times\nTo give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.\nAs usual, pandas as pd, numpy as np, and matplotlib.pyplot as plt are loaded. ### Instructions - Set the random seed to 334. - Import uniform from scipy.stats. - Generate 1000 wait times from the continuous uniform distribution that models Amir’s wait time. Save this as wait_times. - Create a histogram of the simulated wait times and show the plot.\n# Set random seed to 334\nnp.random.seed(334)\n\n# Import uniform\nfrom scipy.stats import uniform\n\n# Generate 1000 wait times between 0 and 30 mins\nwait_times = uniform.rvs(0, 30, size=1000)\n\n# Create a histogram of simulated times and show plot\nplt.hist(wait_times,bins=10)\nplt.show()\n\n\n\npng",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#simulating-sales-deals",
    "href": "5_Introduction_to_statistics_in_python.html#simulating-sales-deals",
    "title": "Introduction to Statistics in Python",
    "section": "Simulating sales deals",
    "text": "Simulating sales deals\nAssume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance.\nnumpy is imported as np. ### Instructions - Import binom from scipy.stats and set the random seed to 10. - Simulate 1 deal worked on by Amir, who wins 30% of the deals he works on. - Simulate a typical week of Amir’s deals, or one week of 3 deals. - Simulate a year’s worth of Amir’s deals, or 52 weeks of 3 deals each, and store in deals. - Print the mean number of deals he won per week.\n# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate a single deal\nprint(binom.rvs(1, .3, size=1))\n[1]\n# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate 1 week of 3 deals\nprint(binom.rvs(3,.3,size=1))\n[1]\n# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate 52 weeks of 3 deals\ndeals = binom.rvs(3,.3,size=52)\n\n# Print mean deals won per week\nprint(np.mean(deals))\n0.8269230769230769",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#calculating-binomial-probabilities",
    "href": "5_Introduction_to_statistics_in_python.html#calculating-binomial-probabilities",
    "title": "Introduction to Statistics in Python",
    "section": "Calculating binomial probabilities",
    "text": "Calculating binomial probabilities\nJust as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution.\nbinom is imported from scipy.stats. ### Instructions - What’s the probability that Amir closes all 3 deals in a week? Save this as prob_3. - What’s the probability that Amir closes 1 or fewer deals in a week? Save this as prob_less_than_or_equal_1. - What’s the probability that Amir closes more than 1 deal? Save this as prob_greater_than_1.\n# Probability of closing 3 out of 3 deals\nprob_3 = binom.pmf(3,3,0.3)\n\nprint(prob_3)\n0.026999999999999996\n# Probability of closing &lt;= 1 deal out of 3 deals\nprob_less_than_or_equal_1 = binom.cdf(1,3,.3)\n\nprint(prob_less_than_or_equal_1)\n0.784\n# Probability of closing &gt; 1 deal out of 3 deals\nprob_greater_than_1 = 1-binom.cdf(1,3,.3)\n\nprint(prob_greater_than_1)\n0.21599999999999997",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#how-many-sales-will-be-won",
    "href": "5_Introduction_to_statistics_in_python.html#how-many-sales-will-be-won",
    "title": "Introduction to Statistics in Python",
    "section": "How many sales will be won?",
    "text": "How many sales will be won?\nNow Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. Recall from the video that the expected value of a binomial distribution can be calculated by nxp\n\nInstructions\n\nCalculate the expected number of sales out of the 3 he works on that Amir will win each week if he maintains his 30% win rate.\nCalculate the expected number of sales out of the 3 he works on that he’ll win if his win rate drops to 25%.\nCalculate the expected number of sales out of the 3 he works on that he’ll win if his win rate rises to 35%.\n\n# Expected number won with 30% win rate\nwon_30pct = 3 * .3\nprint(won_30pct)\n\n# Expected number won with 25% win rate\nwon_25pct = 3*.25\nprint(won_25pct)\n\n# Expected number won with 35% win rate\nwon_35pct = 3*.35\nprint(won_35pct)\n0.8999999999999999\n0.75\n1.0499999999999998",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#chapter-3-more-distributions-and-the-central-limit-theorem",
    "href": "5_Introduction_to_statistics_in_python.html#chapter-3-more-distributions-and-the-central-limit-theorem",
    "title": "Introduction to Statistics in Python",
    "section": "Chapter 3: More Distributions and the Central Limit Theorem",
    "text": "Chapter 3: More Distributions and the Central Limit Theorem\nIt’s time to explore one of the most important probability distributions in statistics, normal distribution. You’ll create histograms to plot normal distributions and gain an understanding of the central limit theorem, before expanding your knowledge of statistical functions by adding the Poisson, exponential, and t-distributions to your repertoire.",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#distribution-of-amirs-sales",
    "href": "5_Introduction_to_statistics_in_python.html#distribution-of-amirs-sales",
    "title": "Introduction to Statistics in Python",
    "section": "Distribution of Amir’s sales",
    "text": "Distribution of Amir’s sales\nSince each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals As part of Amir’s performance review, you want to be able to estimate the probability of him selling different amounts, but before you can do this, you’ll need to determine what kind of distribution the amount variable follows.\n\nInstructions\n\nCreate a histogram with 10 bins to visualize the distribution of the amount. Show the plot.\n\n# Histogram of amount with 10 bins and show plot\namir_deals[\"amount\"].hist(bins=10)\nplt.show()\n\n\n\npng",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#probabilities-from-the-normal-distribution",
    "href": "5_Introduction_to_statistics_in_python.html#probabilities-from-the-normal-distribution",
    "title": "Introduction to Statistics in Python",
    "section": "Probabilities from the normal distribution",
    "text": "Probabilities from the normal distribution\nSince each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals and follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars. As part of his performance metrics, you want to calculate the probability of Amir closing a deal worth various amounts.\n\nInstructions\n\nWhat’s the probability of Amir closing a deal worth less than $7500?\nWhat’s the probability of Amir closing a deal worth more than $1000?\nWhat’s the probability of Amir closing a deal worth between $3000 and $7000?\nWhat amount will 25% of Amir’s sales be less than?\n\nfrom scipy.stats import norm\n\n# Probability of deal &lt; 7500\nprob_less_7500 = norm.cdf(7500, 5000, 2000)\n\nprint(prob_less_7500)\n0.8943502263331446\n# Probability of deal &gt; 1000\nprob_over_1000 = 1- norm.cdf(1000,5000,2000)\n\nprint(prob_over_1000)\n0.9772498680518208\n# Probability of deal between 3000 and 7000\nprob_3000_to_7000 = norm.cdf(7000,5000,2000) - norm.cdf(3000,5000,2000)\n\nprint(prob_3000_to_7000)\n0.6826894921370859\n# Calculate amount that 25% of deals will be less than\npct_25 = norm.ppf(.25,5000,2000)\n\nprint(pct_25)\n3651.0204996078364",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#simulating-sales-under-new-market-conditions",
    "href": "5_Introduction_to_statistics_in_python.html#simulating-sales-under-new-market-conditions",
    "title": "Introduction to Statistics in Python",
    "section": "Simulating sales under new market conditions",
    "text": "Simulating sales under new market conditions\nThe company’s financial analyst is predicting that next quarter, the worth of each sale will increase by 20% and the volatility, or standard deviation, of each sale’s worth will increase by 30%. To see what Amir’s sales might look like next quarter under these new market conditions, you’ll simulate new sales amounts using the normal distribution and store these in the new_sales DataFrame, which has already been created for you.\nIn addition, norm from scipy.stats, pandas as pd, and matplotlib.pyplot as plt are loaded. ### Instructions - Currently, Amir’s average sale amount is $5000. Calculate what his new average amount will be if it increases by 20% and store this in new_mean. - Amir’s current standard deviation is $2000. Calculate what his new standard deviation will be if it increases by 30% and store this in new_sd. - Create a variable called new_sales, which contains 36 simulated amounts from a normal distribution with a mean of new_mean and a standard deviation of new_sd. - Plot the distribution of the new_sales amounts using a histogram and show the plot.\n# Calculate new average amount\nnew_mean = 5000*1.2\n\n# Calculate new standard deviation\nnew_sd = 2000*1.3\n\n# Simulate 36 new sales\nnew_sales = norm.rvs(new_mean,new_sd,size=36)\n\n# Create histogram and show\nplt.hist(new_sales,bins=10)\nplt.show()\n\n\n\npng",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#the-clt-in-action",
    "href": "5_Introduction_to_statistics_in_python.html#the-clt-in-action",
    "title": "Introduction to Statistics in Python",
    "section": "The CLT in action",
    "text": "The CLT in action\nThe central limit theorem states that a sampling distribution of a sample statistic approaches the normal distribution as you take more samples, no matter the original distribution being sampled from.\nIn this exercise, you’ll focus on the sample mean and see the central limit theorem in action while examining the num_users column of amir_deals more closely, which contains the number of people who intend to use the product Amir is selling.\n\nInstructions\n\nCreate a histogram of the num_users column of amir_deals and show the plot.\n\n# Create a histogram of num_users and show\namir_deals['num_users'].hist(bins=10)\nplt.show()\n\n\n\npng\n\n\n\nSet the seed to 104.\nTake a sample of size 20 with replacement from the num_users column of amir_deals, and take the mean.\n\n# Set seed to 104\nnp.random.seed(104)\n\n# Sample 20 num_users with replacement from amir_deals\nsamp_20 = amir_deals['num_users'].sample(20, replace=True)\n\n# Take mean of samp_20\nprint(np.mean(samp_20))\n32.0\n\nRepeat this 100 times using a for loop and store as sample_means. This will take 100 different samples and calculate the mean of each.\n\n# Set seed to 104\nnp.random.seed(104)\n\n# Sample 20 num_users with replacement from amir_deals and take mean\nsamp_20 = amir_deals['num_users'].sample(20, replace=True)\nnp.mean(samp_20)\n\nsample_means = []\n# Loop 100 times\nfor i in range(100):\n  # Take sample of 20 num_users\n  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n  # Calculate mean of samp_20\n  samp_20_mean = np.mean(samp_20)\n  # Append samp_20_mean to sample_means\n  sample_means.append(samp_20_mean)\n  \nprint(sample_means)\n[31.35, 45.05, 33.55, 38.15, 50.85, 31.85, 34.65, 36.25, 38.9, 44.05, 35.45, 37.6, 37.95, 28.85, 33.3, 31.65, 45.5, 43.2, 24.4, 41.05, 37.2, 39.3, 29.45, 33.55, 45.3, 45.1, 30.95, 36.25, 37.65, 42.55, 34.55, 41.1, 36.9, 42.45, 38.45, 45.9, 42.7, 38.4, 32.55, 30.25, 38.0, 38.75, 49.3, 39.55, 49.05, 42.05, 41.0, 40.6, 58.25, 34.55, 51.2, 34.15, 36.95, 42.45, 41.85, 33.2, 36.15, 37.55, 34.2, 29.75, 42.35, 43.75, 29.0, 32.05, 31.65, 44.6, 30.85, 29.6, 37.7, 33.1, 36.35, 40.65, 45.7, 33.8, 40.1, 39.9, 33.5, 32.65, 32.85, 42.85, 35.4, 31.7, 32.0, 33.85, 36.6, 44.35, 39.9, 37.0, 37.3, 42.5, 38.35, 42.8, 44.55, 30.3, 50.45, 42.35, 40.65, 29.85, 39.3, 33.1]\n\nConvert sample_means into a pd.Series, create a histogram of the sample_means, and show the plot.\n\n# Set seed to 104\nnp.random.seed(104)\n\nsample_means = []\n# Loop 100 times\nfor i in range(100):\n  # Take sample of 20 num_users\n  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n  # Calculate mean of samp_20\n  samp_20_mean = np.mean(samp_20)\n  # Append samp_20_mean to sample_means\n  sample_means.append(samp_20_mean)\n  \n# Convert to Series and plot histogram\nsample_means_series = pd.Series(sample_means)\nsample_means_series.hist(bins=10)\n# Show plot\nplt.show()\n\n\n\npng",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#the-mean-of-means",
    "href": "5_Introduction_to_statistics_in_python.html#the-mean-of-means",
    "title": "Introduction to Statistics in Python",
    "section": "The mean of means",
    "text": "The mean of means\nYou want to know what the average number of users (num_users) is per deal, but you want to know this number for the entire company so that you can see if Amir’s deals have more or fewer users than the company’s average deal. The problem is that over the past year, the company has worked on more than ten thousand deals, so it’s not realistic to compile all the data. Instead, you’ll estimate the mean by taking several random samples of deals, since this is much easier than collecting data from everyone in the company.\namir_deals is available and the user data for all the company’s deals is available in all_deals. Both pandas as pd and numpy as np are loaded.\n\nInstructions\n\nSet the random seed to 321.\nTake 30 samples (with replacement) of size 20 from all_deals[‘num_users’] and take the mean of each sample. Store the sample means in sample_means.\nPrint the mean of sample_means.\nPrint the mean of the num_users column of amir_deals.\n\n# Set seed to 321\nnp.random.seed(321)\n\nsample_means = []\n# Loop 30 times to take 30 means\nfor i in range(30):\n  # Take sample of size 20 from num_users col of all_deals with replacement\n  cur_sample = all_deals[\"num_users\"].sample(20,replace=True)\n  # Take mean of cur_sample\n  cur_mean = np.mean(cur_sample)\n  # Append cur_mean to sample_means\n  sample_means.append(cur_mean)\n\n# Print mean of sample_means\nprint(np.mean(sample_means))\n\n# Print mean of num_users in amir_deals\nprint(amir_deals.num_users.mean())",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#tracking-lead-responses",
    "href": "5_Introduction_to_statistics_in_python.html#tracking-lead-responses",
    "title": "Introduction to Statistics in Python",
    "section": "Tracking lead responses",
    "text": "Tracking lead responses\nYour company uses sales software to keep track of new sales leads. It organizes them into a queue so that anyone can follow up on one when they have a bit of free time. Since the number of lead responses is a countable outcome over a period of time, this scenario corresponds to a Poisson distribution. On average, Amir responds to 4 leads each day. In this exercise, you’ll calculate probabilities of Amir responding to different numbers of leads. ### Instructions - Import poisson from scipy.stats and calculate the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4. - Amir’s coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day? - What’s the probability that Amir responds to 2 or fewer leads in a day? - What’s the probability that Amir responds to more than 10 leads in a day?\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability of 5 responses\nprob_5 = poisson.pmf(5,4)\n\nprint(prob_5)\n0.1562934518505317\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability of 5 responses\nprob_coworker = poisson.pmf(5,5.5)\n\nprint(prob_coworker)\n0.17140068409793663\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability of 2 or fewer responses\nprob_2_or_less = poisson.cdf(2,4)\n\nprint(prob_2_or_less)\n0.23810330555354436\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability of &gt; 10 responses\nprob_over_10 = 1-poisson.cdf(10,4)\n\nprint(prob_over_10)\n0.0028397661205137315",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#modeling-time-between-leads",
    "href": "5_Introduction_to_statistics_in_python.html#modeling-time-between-leads",
    "title": "Introduction to Statistics in Python",
    "section": "Modeling time between leads",
    "text": "Modeling time between leads\nTo further evaluate Amir’s performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, he responds to 1 request every 2.5 hours. In this exercise, you’ll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response. ### Instructions - Import expon from scipy.stats. What’s the probability it takes Amir less than an hour to respond to a lead? - What’s the probability it takes Amir more than 4 hours to respond to a lead? - What’s the probability it takes Amir 3-4 hours to respond to a lead?\n# Import expon from scipy.stats\nfrom scipy.stats import expon\n\n# Print probability response takes &lt; 1 hour\nprint(expon.cdf(1, scale=2.5))\n0.3296799539643607\n# Import expon from scipy.stats\nfrom scipy.stats import expon\n\n# Print probability response takes &gt; 4 hours\nprint(1-expon.cdf(4,scale=2.5))\n0.20189651799465536\n# Import expon from scipy.stats\nfrom scipy.stats import expon\n\n# Print probability response takes 3-4 hours\nprint(expon.cdf(4,scale=2.5) - expon.cdf(3,scale=2.5))\n0.09929769391754684",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#relationships-between-variables",
    "href": "5_Introduction_to_statistics_in_python.html#relationships-between-variables",
    "title": "Introduction to Statistics in Python",
    "section": "Relationships between variables",
    "text": "Relationships between variables\nIn this chapter, you’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country.\nIn this exercise, you’ll examine the relationship between a country’s life expectancy (life_exp) and happiness score (happiness_score) both visually and quantitatively. seaborn as sns, matplotlib.pyplot as plt, and pandas as pd are loaded and world_happiness is available. ### Instructions - Create a scatterplot of happiness_score vs. life_exp (without a trendline) using seaborn. - Show the plot.\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n# Create a scatterplot of happiness_score vs. life_exp and show\nsns.scatterplot(y=\"happiness_score\", x = \"life_exp\", data= world_happiness)\n\n# Show plot\nplt.show()\n\n\n\npng\n\n\n\nCreate a scatterplot of happiness_score vs. life_exp with a linear trendline using seaborn, setting ci to None.\nShow the plot.\n\n# Create scatterplot of happiness_score vs life_exp with trendline\n#sns.scatterplot(x=world_happiness[\"life_exp\"],y = world_happiness[\"happiness_score\"],ci=None)\nsns.lmplot(x=\"life_exp\",y = \"happiness_score\",data=world_happiness,ci=None)\n# Show plot\nplt.show()\n\n\n\npng\n\n\n\nCalculate the correlation between life_exp and happiness_score. Save this as cor.\n\n# Create scatterplot of happiness_score vs life_exp with trendline\nsns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)\n\n# Show plot\nplt.show()\n\n# Correlation between life_exp and happiness_score\ncor = world_happiness.life_exp.corr(world_happiness.happiness_score)\n\nprint(cor)\n\n\n\npng\n\n\n0.7802249053272062",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#what-cant-correlation-measure",
    "href": "5_Introduction_to_statistics_in_python.html#what-cant-correlation-measure",
    "title": "Introduction to Statistics in Python",
    "section": "What can’t correlation measure?",
    "text": "What can’t correlation measure?\nWhile the correlation coefficient is a convenient way to quantify the strength of a relationship between two variables, it’s far from perfect. In this exercise, you’ll explore one of the caveats of the correlation coefficient by examining the relationship between a country’s GDP per capita (gdp_per_cap) and happiness score.\npandas as pd, matplotlib.pyplot as plt, and seaborn as sns are imported, and world_happiness is loaded. ### Instructions - Create a seaborn scatterplot (without a trendline) showing the relationship between gdp_per_cap (on the x-axis) and life_exp (on the y-axis). - Show the plot\n# Scatterplot of gdp_per_cap and life_exp\nsns.scatterplot(x='gdp_per_cap',y = 'life_exp', data=world_happiness)\n\n# Show plot\nplt.show()\n\n\n\npng\n\n\n\nCalculate the correlation between gdp_per_cap and life_exp and store as cor.\n\n# Scatterplot of gdp_per_cap and life_exp\nsns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)\n\n# Show plot\nplt.show()\n  \n# Correlation between gdp_per_cap and life_exp\ncor = world_happiness.gdp_per_cap.corr(world_happiness.life_exp)\n\nprint(cor)\n\n\n\npng\n\n\n0.7019547642148012",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#transforming-variables",
    "href": "5_Introduction_to_statistics_in_python.html#transforming-variables",
    "title": "Introduction to Statistics in Python",
    "section": "Transforming variables",
    "text": "Transforming variables\nWhen variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. In this exercise, you’ll perform a transformation yourself.\npandas as pd, numpy as np, matplotlib.pyplot as plt, and seaborn as sns are imported, and world_happiness is loaded. ### Instructions - Create a scatterplot of happiness_score versus gdp_per_cap and calculate the correlation between them. - Add a new column to world_happiness called log_gdp_per_cap that contains the log of gdp_per_cap. - Create a seaborn scatterplot of happiness_score versus log_gdp_per_cap. - Calculate the correlation between log_gdp_per_cap and happiness_score.\n# Scatterplot of happiness_score vs. gdp_per_cap\nsns.scatterplot(x=\"gdp_per_cap\",y = 'happiness_score', data = world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness.gdp_per_cap.corr(world_happiness.happiness_score)\nprint(cor)\n\n\n\npng\n\n\n0.727973301222298\n# Create log_gdp_per_cap column\nworld_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'] )\n\n# Scatterplot of happiness_score vs. log_gdp_per_cap\nsns.scatterplot(x=\"log_gdp_per_cap\", y = \"happiness_score\", data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness[\"log_gdp_per_cap\"].corr(world_happiness[\"happiness_score\"])\nprint(cor)\n\n\n\npng\n\n\n0.8043146004918288",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "5_Introduction_to_statistics_in_python.html#does-sugar-improve-happiness",
    "href": "5_Introduction_to_statistics_in_python.html#does-sugar-improve-happiness",
    "title": "Introduction to Statistics in Python",
    "section": "Does sugar improve happiness?",
    "text": "Does sugar improve happiness?\nA new column has been added to world_happiness called grams_sugar_per_day, which contains the average amount of sugar eaten per person per day in each country. In this exercise, you’ll examine the effect of a country’s average sugar consumption on its happiness score.\npandas as pd, matplotlib.pyplot as plt, and seaborn as sns are imported, and world_happiness is loaded. ### Instructions - Create a seaborn scatterplot showing the relationship between log_gdp_per_cap (on the x-axis) and happiness_score (on the y-axis). - Calculate the correlation between log_gdp_per_cap and happiness_score.\nworld_happiness.head(3)\n\n\n\n\n\n\n\n\n\n\n\nUnnamed: 0\n\n\n\n\ncountry\n\n\n\n\nsocial_support\n\n\n\n\nfreedom\n\n\n\n\ncorruption\n\n\n\n\ngenerosity\n\n\n\n\ngdp_per_cap\n\n\n\n\nlife_exp\n\n\n\n\nhappiness_score\n\n\n\n\nlog_gdp_per_cap\n\n\n\n\n\n\n\n\n0\n\n\n\n\n1\n\n\n\n\nFinland\n\n\n\n\n2.0\n\n\n\n\n5.0\n\n\n\n\n4.0\n\n\n\n\n47.0\n\n\n\n\n42400\n\n\n\n\n81.8\n\n\n\n\n155\n\n\n\n\n10.654904\n\n\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\nDenmark\n\n\n\n\n4.0\n\n\n\n\n6.0\n\n\n\n\n3.0\n\n\n\n\n22.0\n\n\n\n\n48300\n\n\n\n\n81.0\n\n\n\n\n154\n\n\n\n\n10.785187\n\n\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\nNorway\n\n\n\n\n3.0\n\n\n\n\n3.0\n\n\n\n\n8.0\n\n\n\n\n11.0\n\n\n\n\n66300\n\n\n\n\n82.6\n\n\n\n\n153\n\n\n\n\n11.101945\n\n\n\n\n\n\n# Scatterplot of grams_sugar_per_day and happiness_score\nsns.scatterplot(y=\"happiness_score\", x = \"log_gdp_per_cap\", data = world_happiness)\nplt.show()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor = world_happiness[\"happiness_score\"].corr(world_happiness.log_gdp_per_cap)\nprint(cor)\n\n\n\npng\n\n\n0.8043146004918289",
    "crumbs": [
      "Introduction to Statistics in Python"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "",
    "text": "Explore Datasets\nUse the DataFrames imported in the first cell to explore the data and practice your skills! - Using austin_weather and seattle_weather, create a Figure with an array of two Axes objects that share a y-axis range (MONTHS in this case). Plot Seattle’s and Austin’s MLY-TAVG-NORMAL (for average temperature) in the top Axes and plot their MLY-PRCP-NORMAL (for average precipitation) in the bottom axes. The cities should have different colors and the line style should be different between precipitation and temperature. Make sure to label your viz! - Using climate_change, create a twin Axes object with the shared x-axis as time. There should be two lines of different colors not sharing a y-axis: co2 and relative_temp. Only include dates from the 2000s and annotate the first date at which co2 exceeded 400. - Create a scatter plot from medals comparing the number of Gold medals vs the number of Silver medals with each point labeled with the country name. - Explore if the distribution of Age varies in different sports by creating histograms from summer_2016. - Try out the different Matplotlib styles available and save your visualizations as a PNG file.",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#using-the-matplotlib.pyplot-interface",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#using-the-matplotlib.pyplot-interface",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Using the matplotlib.pyplot interface",
    "text": "Using the matplotlib.pyplot interface\nThere are many ways to use Matplotlib. In this course, we will focus on the pyplot interface, which provides the most flexibility in creating and customizing data visualizations.\nInitially, we will use the pyplot interface to create two kinds of objects: Figure objects and Axes objects.\nThis course introduces a lot of new concepts, so if you ever need a quick refresher, download the Matplotlib Cheat Sheet and keep it handy!\n\n\n\n\n\n\nInstructions\n\n\n\n\nImport the matplotlib.pyplot API, using the conventional name plt. - Create Figure and Axes objects using the plt.subplots function. - Show the results, an empty set of axes, using the plt.show function.\n\n\n\n\n# Import the matplotlib.pyplot submodule and name it plt\nimport matplotlib.pyplot  as plt\n\n# Create a Figure and an Axes with plt.subplots\nfig, ax = plt.subplots()\n\n# Call the show function to show the result\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#adding-data-to-an-axes-object",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#adding-data-to-an-axes-object",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Adding data to an Axes object",
    "text": "Adding data to an Axes object\nAdding data to a figure is done by calling methods of the Axes object. In this exercise, we will use the plot method to add data about rainfall in two American cities: Seattle, WA and Austin, TX.\nThe data are stored in two pandas DataFrame objects that are already loaded into memory: seattle_weather stores information about the weather in Seattle, and austin_weather stores information about the weather in Austin. Each of the DataFrames has a “MONTH” column that stores the three-letter name of the months. Each also has a column named “MLY-PRCP-NORMAL” that stores the average rainfall in each month during a ten-year period.\nIn this exercise, you will create a visualization that will allow you to compare the rainfall in these two cities.\n\n\n\n\n\n\nInstructions\n\n\n\n\nImport the matplotlib.pyplot submodule as plt. - Create a Figure and an Axes object by calling plt.subplots. - Add data from the seattle_weather DataFrame by calling the Axes plot method. - Add data from the austin_weather DataFrame in a similar manner and call plt.show to show the results.\n\n\n\n\n# Import the matplotlib.pyplot submodule and name it plt\nimport matplotlib.pyplot as plt\n\n# Create a Figure and an Axes with plt.subplots\nfig, ax = plt.subplots()\n\n# Plot MLY-PRCP-NORMAL from seattle_weather against the MONTH\nax.plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-PRCP-NORMAL\"])\n\n# Plot MLY-PRCP-NORMAL from austin_weather against MONTH\nax.plot(austin_weather['MONTH'], austin_weather['MLY-PRCP-NORMAL'])\n\n# Call the show function\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#customizing-data-appearance",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#customizing-data-appearance",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Customizing data appearance",
    "text": "Customizing data appearance\nWe can customize the appearance of data in our plots, while adding the data to the plot, using key-word arguments to the plot command.\nIn this exercise, you will customize the appearance of the markers, the linestyle that is used, and the color of the lines and markers for your data.\nAs before, the data is already provided in pandas DataFrame objects loaded into memory: seattle_weather and austin_weather. These each have a “MONTHS” column and a “MLY-PRCP-NORMAL” that you will plot against each other.\nIn addition, a Figure object named fig and an Axes object named ax have already been created for you.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCall ax.plot to plot “MLY-PRCP-NORMAL” against “MONTHS” in both DataFrames. - Pass the color key-word arguments to these commands to set the color of the Seattle data to blue (‘b’) and the Austin data to red (‘r’). - Pass the marker key-word arguments to these commands to set the Seattle data to circle markers (‘o’) and the Austin markers to triangles pointing downwards (‘v’). - Pass the linestyle key-word argument to use dashed lines for the data from both cities (‘–’).\n\n\n\n\n#Plot Seattle data, setting data appearance\nfig,ax=plt.subplots()\nax.plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-PRCP-NORMAL\"],marker='o', linestyle='--',color='b')\n\n# Plot Austin data, setting data appearance\nax.plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-PRCP-NORMAL\"],marker='v', linestyle='--',color='r')\n\n# Call show to display the resulting plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#customizing-axis-labels-and-adding-titles",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#customizing-axis-labels-and-adding-titles",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Customizing axis labels and adding titles",
    "text": "Customizing axis labels and adding titles\nCustomizing the axis labels requires using the set_xlabel and set_ylabel methods of the Axes object. Adding a title uses the set_title method.\nIn this exercise, you will customize the content of the axis labels and add a title to a plot.\nAs before, the data is already provided in pandas DataFrame objects loaded into memory: seattle_weather and austin_weather. These each have a “MONTH” column and a “MLY-PRCP-NORMAL” column. These data are plotted against each other in the first two lines of the sample code provided.\nIn addition, a Figure object named fig and an Axes object named ax have already been created for you.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the set_xlabel method to add the label: “Time (months)”. - Use the set_ylabel method to add the label: “Precipitation (inches)”. - Use the set_title method to add the title: “Weather patterns in Austin and Seattle”.\n\n\n\n\nfig,ax=plt.subplots()\nax.plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-PRCP-NORMAL\"])\nax.plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-PRCP-NORMAL\"])\n\n# Customize the x-axis label\nax.set_xlabel(\"Time (months)\")\n\n# Customize the y-axis label\nax.set_ylabel(\"Precipitation (inches)\")\n\n# Add the title\nax.set_title(\"Weather patterns in Austin and Seattle\")\n\n# Display the figure\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#creating-small-multiples-with-plt.subplots",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#creating-small-multiples-with-plt.subplots",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Creating small multiples with plt.subplots",
    "text": "Creating small multiples with plt.subplots\nSmall multiples are used to plot several datasets side-by-side. In Matplotlib, small multiples can be created using the plt.subplots() function. The first argument is the number of rows in the array of Axes objects generate and the second argument is the number of columns. In this exercise, you will use the Austin and Seattle data to practice creating and populating an array of subplots.\nThe data is given to you in DataFrames: seattle_weather and austin_weather. These each have a “MONTH” column and “MLY-PRCP-NORMAL” (for average precipitation), as well as “MLY-TAVG-NORMAL” (for average temperature) columns. In this exercise, you will plot in a separate subplot the monthly average precipitation and average temperatures in each city. Instructions - Create a Figure and an array of subplots with 2 rows and 2 columns. - Addressing the top left Axes as index 0, 0, plot the Seattle precipitation. - In the top right (index 0,1), plot Seattle temperatures. - In the bottom left (1, 0) and bottom right (1, 1) plot Austin precipitations and temperatures.\n\n# Create a Figure and an array of subplots with 2 rows and 2 columns\nfig, ax = plt.subplots(2, 2)\n\n# Addressing the top left Axes as index 0, 0, plot month and Seattle precipitation\nax[0, 0].plot(seattle_weather['MONTH'], seattle_weather[\"MLY-PRCP-NORMAL\"])\n\n# In the top right (index 0,1), plot month and Seattle temperatures\nax[0, 1].plot(seattle_weather['MONTH'], seattle_weather[\"MLY-TAVG-NORMAL\"])\n\n# In the bottom left (1, 0) plot month and Austin precipitations\nax[1,0].plot(austin_weather['MONTH'], austin_weather[\"MLY-PRCP-NORMAL\"])\n\n# In the bottom right (1, 1) plot month and Austin temperatures\nax[1,1].plot(austin_weather['MONTH'], austin_weather[\"MLY-TAVG-NORMAL\"])\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#small-multiples-with-shared-y-axis",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#small-multiples-with-shared-y-axis",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Small multiples with shared y axis",
    "text": "Small multiples with shared y axis\nWhen creating small multiples, it is often preferable to make sure that the different plots are displayed with the same scale used on the y-axis. This can be configured by setting the sharey key-word to True.\nIn this exercise, you will create a Figure with two Axes objects that share their y-axis. As before, the data is provided in seattle_weather and austin_weather DataFrames.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a Figure with an array of two Axes objects that share their y-axis range. - Plot Seattle’s “MLY-PRCP-NORMAL” in a solid blue line in the top Axes. - Add Seattle’s “MLY-PRCP-25PCTL” and “MLY-PRCP-75PCTL” in dashed blue lines to the top Axes. - Plot Austin’s “MLY-PRCP-NORMAL” in a solid red line in the bottom Axes and the “MLY-PRCP-25PCTL” and “MLY-PRCP-75PCTL” in dashed red lines.\n\n\n\n\n# Create a figure and an array of axes: 2 rows, 1 column with shared y axis\nfig, ax = plt.subplots(2, 1, sharey=True)\n\n# Plot Seattle precipitation data in the top axes\nax[0].plot(seattle_weather['MONTH'], seattle_weather[\"MLY-PRCP-NORMAL\"], color = 'b')\nax[0].plot(seattle_weather['MONTH'], seattle_weather[\"MLY-PRCP-25PCTL\"], color = 'b', linestyle = '--')\nax[0].plot(seattle_weather['MONTH'], seattle_weather[\"MLY-PRCP-75PCTL\"], color = 'b', linestyle = '--')\n\n# Plot Austin precipitation data in the bottom axes\nax[1].plot(austin_weather['MONTH'], austin_weather[\"MLY-PRCP-NORMAL\"], color = 'r')\nax[1].plot(austin_weather['MONTH'], austin_weather[\"MLY-PRCP-25PCTL\"], color = 'r', linestyle = '--')\nax[1].plot(austin_weather['MONTH'], austin_weather[\"MLY-PRCP-75PCTL\"], color = 'r', linestyle = '--')\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#read-data-with-a-time-index",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#read-data-with-a-time-index",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Read data with a time index",
    "text": "Read data with a time index\npandas DataFrame objects can have an index that denotes time. This is useful because Matplotlib recognizes that these measurements represent time and labels the values on the axis accordingly.\nIn this exercise, you will read data from a CSV file called climate_change.csv that contains measurements of CO2 levels and temperatures made on the 6th of every month from 1958 until 2016. You will use pandas’ read_csv function.\nTo designate the index as a DateTimeIndex, you will use the parse_dates and index_col key-word arguments both to parse this column as a variable that contains dates and also to designate it as the index for this DataFrame.\nBy the way, if you haven’t downloaded it already, check out the Matplotlib Cheat Sheet. It includes an overview of the most important concepts, functions and methods and might come in handy if you ever need a quick refresher!\n\n\n\n\n\n\nInstructions\n\n\n\n\nImport the pandas library as pd. - Read in the data from a CSV file called ‘climate_change.csv’ using pd.read_csv. - Use the parse_dates key-word argument to parse the “date” column as dates. - Use the index_col key-word argument to set the “date” column as the index.\n\n\n\n\n# Import pandas as pd\nimport pandas as pd\n\n# Read the data from file using read_csv\nclimate_change = pd.read_csv(\"datasets/climate_change.csv\", parse_dates=['date'], index_col=\"date\")",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#plot-time-series-data",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#plot-time-series-data",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Plot time-series data",
    "text": "Plot time-series data\nTo plot time-series data, we use the Axes object plot command. The first argument to this method are the values for the x-axis and the second argument are the values for the y-axis.\nThis exercise provides data stored in a DataFrame called climate_change. This variable has a time-index with the dates of measurements and two data columns: “co2” and “relative_temp”.\nIn this case, the index of the DataFrame would be used as the x-axis values and we will plot the values stored in the “relative_temp” column as the y-axis values. We will also properly label the x-axis and y-axis.\n\n\n\n\n\n\nInstructions\n\n\n\n\nAdd the data from climate_change to the plot: use the DataFrame index for the x value and the “relative_temp” column for the y values. - Set the x-axis label to ‘Time’. - Set the y-axis label to ‘Relative temperature (Celsius)’. - Show the figure.\n\n\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\n\n# Add the time-series for \"relative_temp\" to the plot\nax.plot(climate_change.index,climate_change[\"relative_temp\"])\n\n# Set the x-axis label\nax.set_xlabel(\"Time\")\n\n# Set the y-axis label\nax.set_ylabel('Relative temperature (Celsius)')\n\n# Show the figure\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#using-a-time-index-to-zoom-in",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#using-a-time-index-to-zoom-in",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Using a time index to zoom in",
    "text": "Using a time index to zoom in\nWhen a time-series is represented with a time index, we can use this index for the x-axis when plotting. We can also select a range of dates to zoom in on a particular period within the time-series using pandas’ indexing facilities. In this exercise, you will select a portion of a time-series dataset and you will plot that period.\nThe data to use is stored in a DataFrame called climate_change, which has a time-index with dates of measurements and two data columns: “co2” and “relative_temp”.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse plt.subplots to create a Figure with one Axes called fig and ax, respectively. - Create a variable called seventies that includes all the data between “1970-01-01” and “1979-12-31”. - Add the data from seventies to the plot: use the DataFrame index for the x value and the “co2” column for the y values.\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Use plt.subplots to create fig and ax\nfig, ax = plt.subplots()\n\n# Create variable seventies with data from \"1970-01-01\" to \"1979-12-31\"\nseventies = climate_change[\"1970-01-01\" : \"1979-12-31\"]\n\n# Add the time-series for \"co2\" data from seventies to the plot\nax.plot(seventies.index, seventies[\"co2\"])\n\n# Show the figure\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#plotting-two-variables",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#plotting-two-variables",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Plotting two variables",
    "text": "Plotting two variables\nIf you want to plot two time-series variables that were recorded at the same times, you can add both of them to the same subplot.\nIf the variables have very different scales, you’ll want to make sure that you plot them in different twin Axes objects. These objects can share one axis (for example, the time, or x-axis) while not sharing the other (the y-axis).\nTo create a twin Axes object that shares the x-axis, we use the twinx method.\nIn this exercise, you’ll have access to a DataFrame that has the climate_change data loaded into it. This DataFrame was loaded with the “date” column set as a DateTimeIndex, and it has a column called “co2” with carbon dioxide measurements and a column called “relative_temp” with temperature measurements.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse plt.subplots to create a Figure and Axes objects called fig and ax, respectively. - Plot the carbon dioxide variable in blue using the Axes plot method. - Use the Axes twinx method to create a twin Axes that shares the x-axis. - Plot the relative temperature variable in red on the twin Axes using its plot method.\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Initalize a Figure and Axes\nfig, ax = plt.subplots()\n\n# Plot the CO2 variable in blue\nax.plot(climate_change.index, climate_change['co2'], color='blue')\n\n# Create a twin Axes that shares the x-axis\nax2 = ax.twinx()\n\n# Plot the relative temperature in red\nax2.plot(climate_change.index, climate_change['relative_temp'], color='red')\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#defining-a-function-that-plots-time-series-data",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#defining-a-function-that-plots-time-series-data",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Defining a function that plots time-series data",
    "text": "Defining a function that plots time-series data\nOnce you realize that a particular section of code that you have written is useful, it is a good idea to define a function that saves that section of code for you, rather than copying it to other parts of your program where you would like to use this code.\nHere, we will define a function that takes inputs such as a time variable and some other variable and plots them as x and y inputs. Then, it sets the labels on the x- and y-axis and sets the colors of the y-axis label, the y-axis ticks and the tick labels.\n\n\n\n\n\n\nInstructions\n\n\n\n\nDefine a function called plot_timeseries that takes as input an Axes object (axes), data (x,y), a string with the name of a color and strings for x- and y-axis labels. - Plot y as a function of in the color provided as the input color. - Set the x- and y-axis labels using the provided input xlabel and ylabel, setting the y-axis label color using color. - Set the y-axis tick parameters using the tick_params method of the Axes object, setting the colors key-word to color.\n\n\n\n\n# Define a function called plot_timeseries\ndef plot_timeseries(axes, x, y, color, xlabel, ylabel):\n\n  # Plot the inputs x,y in the provided color\n  axes.plot(x, y, color=color)\n\n  # Set the x-axis label\n  axes.set_xlabel(xlabel)\n\n  # Set the y-axis label\n  axes.set_ylabel(ylabel, color=color)\n\n  # Set the colors tick params for y-axis\n  axes.tick_params('y',colors=color)",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#using-a-plotting-function",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#using-a-plotting-function",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Using a plotting function",
    "text": "Using a plotting function\nDefining functions allows us to reuse the same code without having to repeat all of it. Programmers sometimes say “Don’t repeat yourself”.\nIn the previous exercise, you defined a function called plot_timeseries:\nplot_timeseries(axes, x, y, color, xlabel, ylabel)\nthat takes an Axes object (as the argument axes), time-series data (as x and y arguments) the name of a color (as a string, provided as the color argument) and x-axis and y-axis labels (as xlabel and ylabel arguments). In this exercise, the function plot_timeseries is already defined and provided to you.\nUse this function to plot the climate_change time-series data, provided as a pandas DataFrame object that has a DateTimeIndex with the dates of the measurements and co2 and relative_temp columns.\n\n\n\n\n\n\nInstructions\n\n\n\n\nIn the provided ax object, use the function plot_timeseries to plot the “co2” column in blue, with the x-axis label “Time (years)” and y-axis label “CO2 levels”. - Use the ax.twinx method to add an Axes object to the figure that shares the x-axis with ax. - Use the function plot_timeseries to add the data in the “relative_temp” column in red to the twin Axes object, with the x-axis label “Time (years)” and y-axis label “Relative temperature (Celsius)”.\n\n\n\n\nfig, ax = plt.subplots()\n\n# Plot the CO2 levels time-series in blue\nplot_timeseries(ax,climate_change.index, climate_change['co2'], \"blue\", 'Time (years)', 'CO2 levels')\n\n# Create a twin Axes object that shares the x-axis\nax2 = ax.twinx()\n\n# Plot the relative temperature data in red\nplot_timeseries(ax2, climate_change.index, climate_change[\"relative_temp\"], \"red\", \"Time (years)\", \"Relative temperature (Celsius)\")\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#annotating-a-plot-of-time-series-data",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#annotating-a-plot-of-time-series-data",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Annotating a plot of time-series data",
    "text": "Annotating a plot of time-series data\nAnnotating a plot allows us to highlight interesting information in the plot. For example, in describing the climate change dataset, we might want to point to the date at which the relative temperature first exceeded 1 degree Celsius.\nFor this, we will use the annotate method of the Axes object. In this exercise, you will have the DataFrame called climate_change loaded into memory. Using the Axes methods, plot only the relative temperature column as a function of dates, and annotate the data.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the ax.plot method to plot the DataFrame index against the relative_temp column. - Use the annotate method to add the text ‘&gt;1 degree’ in the location (pd.Timestamp(‘2015-10-06’), 1).\n\n\n\n\nfig, ax = plt.subplots()\n\n# Plot the relative temperature data\nax.plot(climate_change.index, climate_change.relative_temp)\n\n# Annotate the date at which temperatures exceeded 1 degree\nax.annotate(\"&gt;1 degree\", xy=(pd.Timestamp(\"2015-10-06\"),1))\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#plotting-time-series-putting-it-all-together",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#plotting-time-series-putting-it-all-together",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Plotting time-series: putting it all together",
    "text": "Plotting time-series: putting it all together\nIn this exercise, you will plot two time-series with different scales on the same Axes, and annotate the data from one of these series.\nThe CO2/temperatures data is provided as a DataFrame called climate_change. You should also use the function that we have defined before, called plot_timeseries, which takes an Axes object (as the axes argument) plots a time-series (provided as x and y arguments), sets the labels for the x-axis and y-axis and sets the color for the data, and for the y tick/axis labels:\nplot_timeseries(axes, x, y, color, xlabel, ylabel)\nThen, you will annotate with text an important time-point in the data: on 2015-10-06, when the temperature first rose to above 1 degree over the average.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the plot_timeseries function to plot CO2 levels against time. Set xlabel to “Time (years)” ylabel to “CO2 levels” and color to ‘blue’. - Create ax2, as a twin of the first Axes. - In ax2, plot temperature against time, setting the color ylabel to “Relative temp (Celsius)” and color to ‘red’. - Annotate the data using the ax2.annotate method. Place the text “&gt;1 degree” in x=pd.Timestamp(‘2008-10-06’), y=-0.2 pointing with a gray thin arrow to x=pd.Timestamp(‘2015-10-06’), y = 1.\n\n\n\n\nfig, ax = plt.subplots()\n\n# Plot the CO2 levels time-series in blue\nplot_timeseries(ax, climate_change.index, climate_change['co2'], 'blue', \"Time (years)\", \"CO2 levels\")\n\n# Create an Axes object that shares the x-axis\nax2 = ax.twinx()\n\n# Plot the relative temperature data in red\nplot_timeseries(ax2, climate_change.index, climate_change['relative_temp'], 'red', \"Time (years)\", \"Relative temp (Celsius)\")\n\n# Annotate point with relative temperature &gt;1 degree\nax2.annotate(\"&gt;1 degree\", xy=(pd.Timestamp(\"2015-10-06\"),1), \n            xytext=(pd.Timestamp(\"2008-10-06\"),-0.2), arrowprops={'arrowstyle': '-&gt;', 'color': 'gray'})\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#bar-chart",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#bar-chart",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Bar chart",
    "text": "Bar chart\nBar charts visualize data that is organized according to categories as a series of bars, where the height of each bar represents the values of the data in this category.\nFor example, in this exercise, you will visualize the number of gold medals won by each country in the provided medals DataFrame. The DataFrame contains the countries as the index, and a column called “Gold” that contains the number of gold medals won by each country, according to their rows.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCall the ax.bar method to plot the “Gold” column as a function of the country. - Use the ax.set_xticklabels to set the x-axis tick labels to be the country names. - In the call to ax.set_xticklabels rotate the x-axis tick labels by 90 degrees by using the rotation key-word argument. - Set the y-axis label to “Number of medals”.\n\n\n\n\nfig, ax = plt.subplots()\n\n# Plot a bar-chart of gold medals as a function of country\nax.bar(medals.index,medals['Gold'])\n\n# Set the x-axis tick labels to the country names\nax.set_xticklabels(medals.index, rotation=90)\n\n# Set the y-axis label\nax.set_ylabel(\"Number of medals\")\n\nplt.show()\n\n/var/folders/53/yp3kynfd7rn5y13c2wwfm33rmgtrfb/T/ipykernel_38133/1944855953.py:7: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(medals.index, rotation=90)",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#stacked-bar-chart",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#stacked-bar-chart",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Stacked bar chart",
    "text": "Stacked bar chart\nA stacked bar chart contains bars, where the height of each bar represents values. In addition, stacked on top of the first variable may be another variable. The additional height of this bar represents the value of this variable. And you can add more bars on top of that.\nIn this exercise, you will have access to a DataFrame called medals that contains an index that holds the names of different countries, and three columns: “Gold”, “Silver” and “Bronze”. You will also have a Figure, fig, and Axes, ax, that you can add data to.\nYou will create a stacked bar chart that shows the number of gold, silver, and bronze medals won by each country, and you will add labels and create a legend that indicates which bars represent which medals.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCall the ax.bar method to add the “Gold” medals. Call it with the label set to “Gold”. - Call the ax.bar method to stack “Silver” bars on top of that, using the bottom key-word argument so the bottom of the bars will be on top of the gold medal bars, and label to add the label “Silver”. - Use ax.bar to add “Bronze” bars on top of that, using the bottom key-word and label it as “Bronze”.\n\n\n\n\nfig,ax=plt.subplots()\n# Add bars for \"Gold\" with the label \"Gold\"\nax.bar(medals.index, medals.Gold, label=\"Gold\")\n\n# Stack bars for \"Silver\" on top with label \"Silver\"\nax.bar(medals.index, medals.Silver, bottom=medals.Gold, label=\"Silver\")\n\n# Stack bars for \"Bronze\" on top of that with label \"Bronze\"\nax.bar(medals.index, medals.Bronze, bottom=medals.Gold+medals.Silver, label=\"Bronze\")\n\n# Display the legend\nax.legend()\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#creating-histograms",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#creating-histograms",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Creating histograms",
    "text": "Creating histograms\nHistograms show the full distribution of a variable. In this exercise, we will display the distribution of weights of medalists in gymnastics and in rowing in the 2016 Olympic games for a comparison between them.\nYou will have two DataFrames to use. The first is called mens_rowing and includes information about the medalists in the men’s rowing events. The other is called mens_gymnastics and includes information about medalists in all of the Gymnastics events.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the ax.hist method to add a histogram of the “Weight” column from the mens_rowing DataFrame. - Use ax.hist to add a histogram of “Weight” for the mens_gymnastics DataFrame. - Set the x-axis label to “Weight (kg)” and the y-axis label to “# of observations”.\n\n\n\n\nmens_rowing = summer_2016[summer_2016['Sport']==\"Rowing\"]\nmens_gymnastics=summer_2016[summer_2016['Sport']==\"Gymnastics\"]\n\n\nfig, ax = plt.subplots()\n# Plot a histogram of \"Weight\" for mens_rowing\nax.hist(mens_rowing.Weight)\n\n# Compare to histogram of \"Weight\" for mens_gymnastics\nax.hist(mens_gymnastics.Weight)\n\n# Set the x-axis label to \"Weight (kg)\"\nax.set_xlabel(\"Weight (kg)\")\n\n# Set the y-axis label to \"# of observations\"\nax.set_ylabel(\"# of observations\")\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#step-histogram",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#step-histogram",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "“Step” histogram",
    "text": "“Step” histogram\nHistograms allow us to see the distributions of the data in different groups in our data. In this exercise, you will select groups from the Summer 2016 Olympic Games medalist dataset to compare the height of medalist athletes in two different sports.\nThe data is stored in a pandas DataFrame object called summer_2016_medals that has a column “Height”. In addition, you are provided a pandas GroupBy object that has been grouped by the sport.\nIn this exercise, you will visualize and label the histograms of two sports: “Gymnastics” and “Rowing” and see the marked difference between medalists in these two sports.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the hist method to display a histogram of the “Weight” column from the mens_rowing DataFrame, label this as “Rowing”. - Use hist to display a histogram of the “Weight” column from the mens_gymnastics DataFrame, and label this as “Gymnastics”. - For both histograms, use the histtype argument to visualize the data using the ‘step’ type and set the number of bins to use to 5. - Add a legend to the figure, before it is displayed.\n\n\n\n\nfig, ax = plt.subplots()\n\n# Plot a histogram of \"Weight\" for mens_rowing\nax.hist(mens_rowing.Weight,label='Rowing',histtype='step',bins=5)\n\n# Compare to histogram of \"Weight\" for mens_gymnastics\nax.hist(mens_gymnastics.Weight,label='Gymnastics',histtype='step',bins=5)\n\nax.set_xlabel(\"Weight (kg)\")\nax.set_ylabel(\"# of observations\")\n\n# Add the legend and show the Figure\nax.legend()\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#adding-error-bars-to-a-bar-chart",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#adding-error-bars-to-a-bar-chart",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Adding error-bars to a bar chart",
    "text": "Adding error-bars to a bar chart\nStatistical plotting techniques add quantitative information for comparisons into the visualization. For example, in this exercise, we will add error bars that quantify not only the difference in the means of the height of medalists in the 2016 Olympic Games, but also the standard deviation of each of these groups, as a way to assess whether the difference is substantial relative to the variability within each group.\nFor the purpose of this exercise, you will have two DataFrames: mens_rowing holds data about the medalists in the rowing events and mens_gymnastics will hold information about the medalists in the gymnastics events.\n\n\n\n\n\n\nInstructions\n\n\n\n\nAdd a bar with size equal to the mean of the “Height” column in the mens_rowing DataFrame and an error-bar of its standard deviation. - Add another bar for the mean of the “Height” column in mens_gymnastics with an error-bar of its standard deviation. - Add a label to the the y-axis: “Height (cm)”.\n\n\n\n\nfig, ax = plt.subplots()\n\n# Add a bar for the rowing \"Height\" column mean/std\nax.bar(\"Rowing\", mens_rowing.Height.mean(), yerr=mens_rowing.Height.std())\n\n# Add a bar for the gymnastics \"Height\" column mean/std\nax.bar(\"Gymnastics\", mens_gymnastics.Height.mean(), yerr=mens_gymnastics.Height.std())\n\n\n# Label the y-axis\nax.set_ylabel(\"Height (cm)\")\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#adding-error-bars-to-a-plot",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#adding-error-bars-to-a-plot",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Adding error-bars to a plot",
    "text": "Adding error-bars to a plot\nAdding error-bars to a plot is done by using the errorbar method of the Axes object.\nHere, you have two DataFrames loaded: seattle_weather has data about the weather in Seattle and austin_weather has data about the weather in Austin. Each DataFrame has a column “MONTH” that has the names of the months, a column “MLY-TAVG-NORMAL” that has the average temperature in each month and a column “MLY-TAVG-STDDEV” that has the standard deviation of the temperatures across years.\nIn the exercise, you will plot the mean temperature across months and add the standard deviation at each point as y errorbars.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse the ax.errorbar method to add the Seattle data: the “MONTH” column as x values, the “MLY-TAVG-NORMAL” as y values and “MLY-TAVG-STDDEV” as yerr values. Add the Austin data: the “MONTH” column as x values, the “MLY-TAVG-NORMAL” as y values and “MLY-TAVG-STDDEV” as yerr values. Set the y-axis label as “Temperature (Fahrenheit)”.\n\n\n\n\nfig, ax = plt.subplots()\n\nax.errorbar(seattle_weather.MONTH,seattle_weather[\"MLY-TAVG-NORMAL\"],\n           yerr=seattle_weather[\"MLY-TAVG-STDDEV\"])\nax.errorbar(austin_weather.MONTH,austin_weather[\"MLY-TAVG-NORMAL\"],\n           yerr=austin_weather[\"MLY-TAVG-STDDEV\"])\nax.set_ylabel(\"Temperature (Fahrenheit)\")\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#creating-boxplots",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#creating-boxplots",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Creating boxplots",
    "text": "Creating boxplots\nBoxplots provide additional information about the distribution of the data that they represent. They tell us what the median of the distribution is, what the inter-quartile range is and also what the expected range of approximately 99% of the data should be. Outliers beyond this range are particularly highlighted.\nIn this exercise, you will use the data about medalist heights that you previously visualized as histograms, and as bar charts with error bars, and you will visualize it as boxplots.\nAgain, you will have the mens_rowing and mens_gymnastics DataFrames available to you, and both of these DataFrames have columns called “Height” that you will compare.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a boxplot that contains the “Height” column for mens_rowing on the left and mens_gymnastics on the right. - Add x-axis tick labels: “Rowing” and “Gymnastics”. - Add a y-axis label: “Height (cm)”.\n\n\n\n\nfig, ax = plt.subplots()\n\nax.boxplot([mens_rowing.Height,mens_gymnastics.Height])\nax.set_xticklabels(([\"Rowing\", \"Gymnastics\"]))\nax.set_ylabel(\"Height (cm)\")\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#simple-scatter-plot",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#simple-scatter-plot",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Simple scatter plot",
    "text": "Simple scatter plot\nScatter are a bi-variate visualization technique. They plot each record in the data as a point. The location of each point is determined by the value of two variables: the first variable determines the distance along the x-axis and the second variable determines the height along the y-axis.\nIn this exercise, you will create a scatter plot of the climate_change data. This DataFrame, which is already loaded, has a column “co2” that indicates the measurements of carbon dioxide every month and another column, “relative_temp” that indicates the temperature measured at the same time.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUsing the ax.scatter method, add the data to the plot: “co2” on the x-axis and “relative_temp” on the y-axis. - Set the x-axis label to “CO2 (ppm)”. - Set the y-axis label to “Relative temperature (C)”.\n\n\n\n\nfig, ax = plt.subplots()\n\n# Add data: \"co2\" on x-axis, \"relative_temp\" on y-axis\nax.scatter(climate_change.co2,climate_change.relative_temp)\n\n# Set the x-axis label to \"CO2 (ppm)\"\nax.set_xlabel(\"CO2 (ppm)\")\n\n# Set the y-axis label to \"Relative temperature (C)\"\nax.set_ylabel(\"Relative temperature (C)\")\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#encoding-time-by-color",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#encoding-time-by-color",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Encoding time by color",
    "text": "Encoding time by color\nThe screen only has two dimensions, but we can encode another dimension in the scatter plot using color. Here, we will visualize the climate_change dataset, plotting a scatter plot of the “co2” column, on the x-axis, against the “relative_temp” column, on the y-axis. We will encode time using the color dimension, with earlier times appearing as darker shades of blue and later times appearing as brighter shades of yellow.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUsing the ax.scatter method add a scatter plot of the “co2” column (x-axis) against the “relative_temp” column. - Use the c key-word argument to pass in the index of the DataFrame as input to color each point according to its date. - Set the x-axis label to “CO2 (ppm)” and the y-axis label to “Relative temperature (C)”.\n\n\n\n\nfig, ax = plt.subplots()\n\n# Add data: \"co2\" on x-axis, \"relative_temp\" on y-axis\nax.scatter(climate_change.co2,climate_change.relative_temp,c =climate_change.index)\n\n# Set the x-axis label to \"CO2 (ppm)\"\nax.set_xlabel(\"CO2 (ppm)\")\n\n# Set the y-axis label to \"Relative temperature (C)\"\nax.set_ylabel(\"Relative temperature (C)\")\n\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#switching-between-styles",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#switching-between-styles",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Switching between styles",
    "text": "Switching between styles\nSelecting a style to use affects all of the visualizations that are created after this style is selected.\nHere, you will practice plotting data in two different styles. The data you will use is the same weather data we used in the first lesson: you will have available to you the DataFrame seattle_weather and the DataFrame austin_weather, both with records of the average temperature in every month.\n\n\n\n\n\n\nInstructions\n\n\n\n\nSelect the ‘ggplot’ style, create a new Figure called fig, and a new Axes object called ax with plt.subplots. - Select the ‘Solarize_Light2’ style, create a new Figure called fig, and a new Axes object called ax with plt.subplots.\n\n\n\n\n# Use the \"ggplot\" style and create new Figure/Axes\nplt.style.use(\"ggplot\")\nfig,ax = plt.subplots()\nax.plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-TAVG-NORMAL\"])\nplt.show()\n\n# Use the \"Solarize_Light2\" style and create new Figure/Axes\nplt.style.use(\"Solarize_Light2\")\nfig,ax = plt.subplots()\nax.plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-TAVG-NORMAL\"])\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#saving-a-file-several-times",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#saving-a-file-several-times",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Saving a file several times",
    "text": "Saving a file several times\nIf you want to share your visualizations with others, you will need to save them into files. Matplotlib provides as way to do that, through the savefig method of the Figure object. In this exercise, you will save a figure several times. Each time setting the parameters to something slightly different. We have provided and already created Figure object.\n\n\n\n\n\n\nInstructions\n\n\n\n\nExamine the figure by calling the plt.show() function. - Save the figure into the file my_figure.png, using the default resolution. - Save the figure into the file my_figure_300dpi.png and set the resolution to 300 dpi.\n\n\n\n\n# Set figure dimensions and save as a PNG\nplt.show()\nfig.set_size_inches([3,5])\nfig.savefig('figure_3_5.png')\n# Save as a PNG file\nfig.savefig('my_figure.png')\n\n# Save as a PNG file with 300 dpi\nfig.savefig('my_figure_300dpi.png',dpi=300)",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#save-a-figure-with-different-sizes",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#save-a-figure-with-different-sizes",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Save a figure with different sizes",
    "text": "Save a figure with different sizes\nBefore saving your visualization, you might want to also set the size that the figure will have on the page. To do so, you can use the Figure object’s set_size_inches method. This method takes a sequence of two values. The first sets the width and the second sets the height of the figure.\nHere, you will again have a Figure object called fig already provided (you can run plt.show if you want to see its contents). Use the Figure methods set_size_inches and savefig to change its size and save two different versions of this figure.\n\n\n\n\n\n\nInstructions\n\n\n\n\nSet the figure size as width of 3 inches and height of 5 inches and save it as ‘figure_3_5.png’ with default resolution. - Set the figure size to width of 5 inches and height of 3 inches and save it as ‘figure_5_3.png’ with default settings.\n\n\n\n\n# Set figure dimensions and save as a PNG\nfig.set_size_inches([3,5])\nfig.savefig('figure_3_5.png')\n# Set figure dimensions and save as a PNG\nfig.set_size_inches([5,3])\nfig.savefig('figure_5_3.png')",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#unique-values-of-a-column",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#unique-values-of-a-column",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Unique values of a column",
    "text": "Unique values of a column\nOne of the main strengths of Matplotlib is that it can be automated to adapt to the data that it receives as input. For example, if you receive data that has an unknown number of categories, you can still create a bar plot that has bars for each category.\nIn this exercise and the next, you will be visualizing the weight of athletes in the 2016 summer Olympic Games again, from a dataset that has some unknown number of branches of sports in it. This will be loaded into memory as a pandas DataFrame object called summer_2016_medals, which has a column called “Sport” that tells you to which branch of sport each row corresponds. There is also a “Weight” column that tells you the weight of each athlete.\nIn this exercise, we will extract the unique values of the “Sport” column\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a variable called sports_column that holds the data from the “Sport” column of the DataFrame object. - Use the unique method of this variable to find all the unique different sports that are present in this data, and assign these values into a new variable called sports. - Print the sports variable to the console.\n\n\n\n\n# Extract the \"Sport\" column\nsports_column = summer_2016['Sport']\n\n# Find the unique values of the \"Sport\" column\nsports = sports_column.unique()\n\n# Print out the unique sports values\nprint(sports)\n\n['Rowing' 'Taekwondo' 'Handball' 'Wrestling' 'Gymnastics' 'Swimming'\n 'Basketball' 'Boxing' 'Volleyball' 'Athletics' 'Rugby Sevens' 'Judo'\n 'Rhythmic Gymnastics' 'Weightlifting' 'Equestrianism' 'Badminton'\n 'Water Polo' 'Football' 'Fencing' 'Shooting' 'Sailing' 'Beach Volleyball'\n 'Canoeing' 'Hockey' 'Cycling' 'Tennis' 'Diving' 'Table Tennis'\n 'Triathlon' 'Archery' 'Synchronized Swimming' 'Modern Pentathlon'\n 'Trampolining' 'Golf']",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#automate-your-visualization",
    "href": "6_Introduction_to_Data_Visualization_with_Matplotlib.html#automate-your-visualization",
    "title": "Introduction to Data Visualization with Matplotlib",
    "section": "Automate your visualization",
    "text": "Automate your visualization\nOne of the main strengths of Matplotlib is that it can be automated to adapt to the data that it receives as input. For example, if you receive data that has an unknown number of categories, you can still create a bar plot that has bars for each category.\nThis is what you will do in this exercise. You will be visualizing data about medal winners in the 2016 summer Olympic Games again, but this time you will have a dataset that has some unknown number of branches of sports in it. This will be loaded into memory as a pandas DataFrame object called summer_2016_medals, which has a column called “Sport” that tells you to which branch of sport each row corresponds. There is also a “Weight” column that tells you the weight of each athlete. Instructions - Iterate over the values of sports setting sport as your loop variable. - In each iteration, extract the rows where the “Sport” column is equal to sport. - Add a bar to the provided ax object, labeled with the sport name, with the mean of the “Weight” column as its height, and the standard deviation as a y-axis error bar. - Save the figure into the file “sports_weights.png”.\n\nfig, ax = plt.subplots()\n\n# Loop over the different sports branches\nfor sport in sports:\n  # Extract the rows only for this sport\n  sport_df = summer_2016[summer_2016.Sport==sport]\n  # Add a bar for the \"Weight\" mean with std y error bar\n  ax.bar(sport,sport_df.Weight.mean(),yerr=sport_df.Weight.std())\n\nax.set_ylabel(\"Weight\")\nax.set_xticklabels(sports, rotation=90)\n\n# Save the figure to file\nfig.savefig(\"sports_weights.png\")\n\n/var/folders/53/yp3kynfd7rn5y13c2wwfm33rmgtrfb/T/ipykernel_38133/4221101768.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels(sports, rotation=90)",
    "crumbs": [
      "Introduction to Data Visualization with Matplotlib"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "",
    "text": "Explore Datasets\nUse the DataFrames imported in the first cell to explore the data and practice your skills! - From country_data, create a scatter plot to look at the relationship between GDP and Literacy. Use color to segment the data points by region. - Use mpg to create a line plot with model_year on the x-axis and weight on the y-axis. Create differentiating lines for each country of origin (origin). - Create a box plot from student_data to explore the relationship between the number of failures (failures) and the average final grade (G3). - Create a bar plot from survey to compare how Loneliness differs across values for Internet usage. Format it to have two subplots for gender. - Make sure to add titles and labels to your plots and adjust their format for readability!",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#making-a-scatter-plot-with-lists",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#making-a-scatter-plot-with-lists",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Making a scatter plot with lists",
    "text": "Making a scatter plot with lists\nIn this exercise, we’ll use a dataset that contains information about 227 countries. This dataset has lots of interesting information on each country, such as the country’s birth rates, death rates, and its gross domestic product (GDP). GDP is the value of all the goods and services produced in a year, expressed as dollars per person.\nWe’ve created three lists of data from this dataset to get you started. gdp is a list that contains the value of GDP per country, expressed as dollars per person. phones is a list of the number of mobile phones per 1,000 people in that country. Finally, percent_literate is a list that contains the percent of each country’s population that can read and write.\n\nInstructions\n\nImport Matplotlib and Seaborn using the standard naming convention.\nUse Seaborn to create a count plot with region on the y-axis.\nDisplay the plot.\n\n\nregion=['ASIA (EX. NEAR EAST)', 'EASTERN EUROPE', 'NORTHERN AFRICA', 'OCEANIA', 'WESTERN EUROPE', 'SUB-SAHARAN AFRICA', 'LATIN AMER. & CARIB', 'LATIN AMER. & CARIB', 'LATIN AMER. & CARIB', 'C.W. OF IND. STATES', 'LATIN AMER. & CARIB', 'OCEANIA',\n 'WESTERN EUROPE', 'C.W. OF IND. STATES', 'LATIN AMER. & CARIB', 'NEAR EAST', 'ASIA (EX. NEAR EAST)', 'LATIN AMER. & CARIB',\n 'C.W. OF IND. STATES',  'WESTERN EUROPE',  'LATIN AMER. & CARIB',  'SUB-SAHARAN AFRICA',  'NORTHERN AMERICA',  'ASIA (EX. NEAR EAST)',  'LATIN AMER. & CARIB',  'EASTERN EUROPE',  'SUB-SAHARAN AFRICA',  'LATIN AMER. & CARIB',\n 'LATIN AMER. & CARIB',  'ASIA (EX. NEAR EAST)',  'EASTERN EUROPE',  'SUB-SAHARAN AFRICA',  'ASIA (EX. NEAR EAST)',  'SUB-SAHARAN AFRICA', 'ASIA (EX. NEAR EAST)', 'SUB-SAHARAN AFRICA', 'NORTHERN AMERICA', 'SUB-SAHARAN AFRICA', 'LATIN AMER. & CARIB', 'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA', 'LATIN AMER. & CARIB', 'ASIA (EX. NEAR EAST)', 'LATIN AMER. & CARIB', 'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA', 'OCEANIA', 'LATIN AMER. & CARIB', 'SUB-SAHARAN AFRICA', 'EASTERN EUROPE', 'LATIN AMER. & CARIB', 'NEAR EAST', 'EASTERN EUROPE', 'WESTERN EUROPE', 'SUB-SAHARAN AFRICA', 'LATIN AMER. & CARIB', 'LATIN AMER. & CARIB', 'ASIA (EX. NEAR EAST)', 'LATIN AMER. & CARIB', 'NORTHERN AFRICA', 'LATIN AMER. & CARIB', 'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA', 'BALTICS', 'SUB-SAHARAN AFRICA', 'WESTERN EUROPE', 'OCEANIA', 'WESTERN EUROPE', 'WESTERN EUROPE', 'LATIN AMER. & CARIB', 'OCEANIA',\n 'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA', 'NEAR EAST', 'C.W. OF IND. STATES', 'WESTERN EUROPE',\n 'SUB-SAHARAN AFRICA', 'WESTERN EUROPE', 'WESTERN EUROPE',\n 'NORTHERN AMERICA', 'LATIN AMER. & CARIB', 'LATIN AMER. & CARIB',\n 'OCEANIA', 'LATIN AMER. & CARIB', 'WESTERN EUROPE',\n 'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA', 'LATIN AMER. & CARIB',\n 'LATIN AMER. & CARIB', 'LATIN AMER. & CARIB', 'ASIA (EX. NEAR EAST)',\n 'EASTERN EUROPE', 'WESTERN EUROPE', 'ASIA (EX. NEAR EAST)',\n 'ASIA (EX. NEAR EAST)', 'ASIA (EX. NEAR EAST)', 'NEAR EAST', 'WESTERN EUROPE',\n 'WESTERN EUROPE', 'NEAR EAST', 'WESTERN EUROPE', 'LATIN AMER. & CARIB',\n 'ASIA (EX. NEAR EAST)', 'WESTERN EUROPE', 'NEAR EAST', 'C.W. OF IND. STATES', 'SUB-SAHARAN AFRICA',\n 'OCEANIA', 'ASIA (EX. NEAR EAST)', 'ASIA (EX. NEAR EAST)', 'NEAR EAST', 'C.W. OF IND. STATES',\n 'ASIA (EX. NEAR EAST)', 'BALTICS', 'NEAR EAST', 'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA', 'NORTHERN AFRICA',\n 'WESTERN EUROPE', 'BALTICS', 'WESTERN EUROPE', 'ASIA (EX. NEAR EAST)', 'EASTERN EUROPE', 'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA',  'ASIA (EX. NEAR EAST)', 'ASIA (EX. NEAR EAST)', 'SUB-SAHARAN AFRICA', 'WESTERN EUROPE', 'OCEANIA', 'LATIN AMER. & CARIB',\n 'SUB-SAHARAN AFRICA',  'SUB-SAHARAN AFRICA',  'SUB-SAHARAN AFRICA',  'LATIN AMER. & CARIB',  'OCEANIA',  'C.W. OF IND. STATES',\n 'WESTERN EUROPE',  'ASIA (EX. NEAR EAST)',  'LATIN AMER. & CARIB',  'NORTHERN AFRICA',  'SUB-SAHARAN AFRICA',  'SUB-SAHARAN AFRICA',  'OCEANIA',  'ASIA (EX. NEAR EAST)',  'WESTERN EUROPE',  'LATIN AMER. & CARIB', 'OCEANIA',\n 'OCEANIA',  'LATIN AMER. & CARIB',  'SUB-SAHARAN AFRICA', 'SUB-SAHARAN AFRICA', 'OCEANIA', 'WESTERN EUROPE', 'NEAR EAST',\n 'ASIA (EX. NEAR EAST)',\n 'OCEANIA',\n 'LATIN AMER. & CARIB',\n 'OCEANIA',\n 'LATIN AMER. & CARIB',\n 'LATIN AMER. & CARIB',\n 'ASIA (EX. NEAR EAST)',\n 'EASTERN EUROPE',\n 'WESTERN EUROPE',\n 'LATIN AMER. & CARIB',\n 'NEAR EAST',\n 'SUB-SAHARAN AFRICA',\n 'EASTERN EUROPE',\n 'C.W. OF IND. STATES',\n 'SUB-SAHARAN AFRICA',\n 'SUB-SAHARAN AFRICA',\n 'LATIN AMER. & CARIB',\n 'LATIN AMER. & CARIB',\n 'NORTHERN AMERICA',\n 'LATIN AMER. & CARIB',\n 'OCEANIA',\n 'WESTERN EUROPE',\n 'SUB-SAHARAN AFRICA',\n 'NEAR EAST',\n 'SUB-SAHARAN AFRICA',\n 'EASTERN EUROPE',\n 'SUB-SAHARAN AFRICA',\n 'SUB-SAHARAN AFRICA',\n 'ASIA (EX. NEAR EAST)',\n 'EASTERN EUROPE',\n 'EASTERN EUROPE',\n 'OCEANIA',\n 'SUB-SAHARAN AFRICA',\n 'SUB-SAHARAN AFRICA',\n 'WESTERN EUROPE',\n 'ASIA (EX. NEAR EAST)',\n 'SUB-SAHARAN AFRICA',\n 'LATIN AMER. & CARIB',\n 'SUB-SAHARAN AFRICA',\n 'WESTERN EUROPE',\n 'WESTERN EUROPE',\n 'NEAR EAST',\n 'ASIA (EX. NEAR EAST)',\n 'C.W. OF IND. STATES',\n 'SUB-SAHARAN AFRICA',\n 'ASIA (EX. NEAR EAST)',\n 'SUB-SAHARAN AFRICA',\n 'OCEANIA',\n 'LATIN AMER. & CARIB',\n 'NORTHERN AFRICA',\n 'NEAR EAST',\n 'C.W. OF IND. STATES',\n 'LATIN AMER. & CARIB',\n 'OCEANIA',\n 'SUB-SAHARAN AFRICA',\n 'C.W. OF IND. STATES',\n 'NEAR EAST',\n 'WESTERN EUROPE',\n 'NORTHERN AMERICA',\n 'LATIN AMER. & CARIB',\n 'C.W. OF IND. STATES',\n 'OCEANIA',\n 'LATIN AMER. & CARIB',\n 'ASIA (EX. NEAR EAST)',\n 'LATIN AMER. & CARIB',\n 'OCEANIA',\n 'NEAR EAST',\n 'NORTHERN AFRICA',\n 'NEAR EAST',\n 'SUB-SAHARAN AFRICA',\n 'SUB-SAHARAN AFRICA']\n\n\n# Import Matplotlib and Seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Create count plot with region on the y-axis\nsns.countplot(y=region)\n\n# Show plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#tidy-vs.-untidy-data",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#tidy-vs.-untidy-data",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "“Tidy” vs. “untidy” data",
    "text": "“Tidy” vs. “untidy” data\nHere, we have a sample dataset from a survey of children about their favorite animals. But can we use this dataset as-is with Seaborn? Let’s use pandas to import the csv file with the data collected from the survey and determine whether it is tidy, which is essential to having it work well with Seaborn. ### Instructions - Read the csv file located at csv_filepath into a DataFrame named df. - Print the head of df to show the first five rows.\n\n# Import pandas\nimport pandas as pd\n\n# Create a DataFrame from csv file\ndf = pd.read_csv(\"datasets/young-people-survey-responses.csv\")\n\n# Print the head of df\nprint(df.head())\n\n   Unnamed: 0  Music  Techno  Movies  History  Mathematics  Pets  Spiders  \\\n0           0    5.0     1.0     5.0      1.0          3.0   4.0      1.0   \n1           1    4.0     1.0     5.0      1.0          5.0   5.0      1.0   \n2           2    5.0     1.0     5.0      1.0          5.0   5.0      1.0   \n3           3    5.0     2.0     5.0      4.0          4.0   1.0      5.0   \n4           4    5.0     2.0     5.0      3.0          2.0   1.0      1.0   \n\n   Loneliness  Parents Advice   Internet usage  Finances   Age  Siblings  \\\n0         3.0             4.0  few hours a day       3.0  20.0       1.0   \n1         2.0             2.0  few hours a day       3.0  19.0       2.0   \n2         5.0             3.0  few hours a day       2.0  20.0       2.0   \n3         5.0             2.0  most of the day       2.0  22.0       1.0   \n4         3.0             3.0  few hours a day       4.0  20.0       1.0   \n\n   Gender Village - town  \n0  female        village  \n1  female           city  \n2  female           city  \n3  female           city  \n4  female        village",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#making-a-count-plot-with-a-dataframe",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#making-a-count-plot-with-a-dataframe",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Making a count plot with a DataFrame",
    "text": "Making a count plot with a DataFrame\nIn this exercise, we’ll look at the responses to a survey sent out to young people. Our primary question here is: how many young people surveyed report being scared of spiders? Survey participants were asked to agree or disagree with the statement “I am afraid of spiders”. Responses vary from 1 to 5, where 1 is “Strongly disagree” and 5 is “Strongly agree”. ### Instructions - Import Matplotlib, pandas, and Seaborn using the standard names. - Create a DataFrame named df from the csv file located at csv_filepath. - Use the countplot() function with the x= and data= arguments to create a count plot with the “Spiders” column values on the x-axis. - Display the plot.\n\n# Import Matplotlib, pandas, and Seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ncsv_filepath = \"datasets/young-people-survey-responses.csv\"\n\n# Create a DataFrame from csv file\ndf = pd.read_csv(csv_filepath)\n\n# Create a count plot with \"Spiders\" on the x-axis\nsns.countplot(x=\"Spiders\", data=df)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nHue and scatter plots\nIn the prior video, we learned how hue allows us to easily make subgroups within Seaborn plots. Let’s try it out by exploring data from students in secondary school. We have a lot of information about each student like their age, where they live, their study habits and their extracurricular activities.\nFor now, we’ll look at the relationship between the number of absences they have in school and their final grade in the course, segmented by where the student lives (rural vs. urban area). ### Instructions - Fill in the palette_colors dictionary to map the “Rural” location value to the color “green” and the “Urban” location value to the color “blue”. - Create a count plot with “school” on the x-axis using the student_data DataFrame. - Add subgroups to the plot using “location” variable and use the palette_colors dictionary to make the location subgroups green and blue.\n\n# Import Matplotlib and Seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nstudent_data = pd.read_csv(\"datasets/student-alcohol-consumption.csv\")\n\n# Create a dictionary mapping subgroup values to colors\npalette_colors = {\"Rural\": \"green\", \"Urban\": \"blue\"}\n\n# Create a count plot of school with location subgroups\n\nsns.countplot(x=\"school\",hue=\"location\", palette=palette_colors, data=student_data)\n\n\n# Display plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCreating subplots with col and row\nWe’ve seen in prior exercises that students with more absences (“absences”) tend to have lower final grades (“G3”). Does this relationship hold regardless of how much time students study each week?\nTo answer this, we’ll look at the relationship between the number of absences that a student has in school and their final grade in the course, creating separate subplots based on each student’s weekly study time (“study_time”). ### Instructions - Use relplot() to create a scatter plot with “G1” on the x-axis and “G3” on the y-axis, using the student_data DataFrame. - Create column subplots based on whether the student received support from the school (“schoolsup”), ordered so that “yes” comes before “no”. - Add row subplots based on whether the student received support from the family (“famsup”), ordered so that “yes” comes before “no”. This will result in subplots based on two factors.\n\n# Create a scatter plot of G1 vs. G3\nsns.relplot(x=\"G1\", y = \"G3\", data=student_data,kind=\"scatter\")\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Create column subplots based on whether the student received support from the school (\"schoolsup\"), ordered so that \"yes\" comes before \"no\".\n# Adjust to add subplots based on school support\nsns.relplot(x=\"G1\", y=\"G3\", \n            data=student_data,col=\"schoolsup\",col_order=[\"yes\",\"no\"],\n            kind=\"scatter\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Add row subplots based on whether the student received support from the family (\"famsup\"), ordered so that \"yes\" comes before \"no\". This will result in subplots based on two factors.\n# Adjust further to add subplots based on family support\nsns.relplot(x=\"G1\", y=\"G3\", \n            data=student_data,\n            kind=\"scatter\", \n            col=\"schoolsup\",\n            col_order=[\"yes\", \"no\"],\n            row=\"famsup\",\n            row_order=[\"yes\",\"no\"])\n\n# Show plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#changing-the-size-of-scatter-plot-points",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#changing-the-size-of-scatter-plot-points",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Changing the size of scatter plot points",
    "text": "Changing the size of scatter plot points\nIn this exercise, we’ll explore Seaborn’s mpg dataset, which contains one row per car model and includes information such as the year the car was made, the number of miles per gallon (“M.P.G.”) it achieves, the power of its engine (measured in “horsepower”), and its country of origin.\nWhat is the relationship between the power of a car’s engine (“horsepower”) and its fuel efficiency (“mpg”)? And how does this relationship vary by the number of cylinders (“cylinders”) the car has? Let’s find out.\nLet’s continue to use relplot() instead of scatterplot() since it offers more flexibility. ### Instructions - Use relplot() and the mpg DataFrame to create a scatter plot with “horsepower” on the x-axis and “mpg” on the y-axis. Vary the size of the points by the number of cylinders in the car (“cylinders”). - To make this plot easier to read, use hue to vary the color of the points by the number of cylinders in the car (“cylinders”).\n\n# Import Matplotlib and Seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create scatter plot of horsepower vs. mpg\n\nsns.relplot(x=\"horsepower\", y=\"mpg\",kind=\"scatter\",size=\"cylinders\", data=mpg)\n\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#To make this plot easier to read, use hue to vary the color of the points by the number of cylinders in the car (\"cylinders\").\n# Create scatter plot of horsepower vs. mpg\nsns.relplot(x=\"horsepower\", y=\"mpg\", \n            data=mpg, kind=\"scatter\", hue=\"cylinders\",\n            size=\"cylinders\")\n\n# Show plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#changing-the-style-of-scatter-plot-points",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#changing-the-style-of-scatter-plot-points",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Changing the style of scatter plot points",
    "text": "Changing the style of scatter plot points\nLet’s continue exploring Seaborn’s mpg dataset by looking at the relationship between how fast a car can accelerate (“acceleration”) and its fuel efficiency (“mpg”). Do these properties vary by country of origin (“origin”)?\nNote that the “acceleration” variable is the time to accelerate from 0 to 60 miles per hour, in seconds. Higher values indicate slower acceleration. ### Instructions - Use relplot() and the mpg DataFrame to create a scatter plot with “acceleration” on the x-axis and “mpg” on the y-axis. Vary the style and color of the plot points by country of origin (“origin”).\n\n# Create a scatter plot of acceleration vs. mpg\nsns.relplot(x=\"acceleration\",y=\"mpg\", data=mpg,kind='scatter', style=\"origin\",hue=\"origin\")\n# Show plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#interpreting-line-plots",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#interpreting-line-plots",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Interpreting line plots",
    "text": "Interpreting line plots\nIn this exercise, we’ll continue to explore Seaborn’s mpg dataset, which contains one row per car model and includes information such as the year the car was made, its fuel efficiency (measured in “miles per gallon” or “M.P.G”), and its country of origin (USA, Europe, or Japan).\nHow has the average miles per gallon achieved by these cars changed over time? Let’s use line plots to find out! ### Instructions - Use relplot() and the mpg DataFrame to create a line plot with “model_year” on the x-axis and “mpg” on the y-axis.\n\n#Use relplot() and the mpg DataFrame to create a line plot with \"model_year\" on the x-axis and \"mpg\" on the y-axis.\n# Create line plot\nsns.relplot(x=\"model_year\",y=\"mpg\", data= mpg, kind=\"line\")\n# Show plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#visualizing-standard-deviation-with-line-plots",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#visualizing-standard-deviation-with-line-plots",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Visualizing standard deviation with line plots",
    "text": "Visualizing standard deviation with line plots\nIn the last exercise, we looked at how the average miles per gallon achieved by cars has changed over time. Now let’s use a line plot to visualize how the distribution of miles per gallon has changed over time. ### Instructions - Change the plot so the shaded area shows the standard deviation instead of the confidence interval for the mean.\n\n#Change the plot so the shaded area shows the standard deviation instead of the confidence interval for the mean.\n# Make the shaded area show the standard deviation\nsns.relplot(x=\"model_year\", y=\"mpg\",ci=\"sd\",\n            data=mpg, kind=\"line\")\n\n# Show plot\nplt.show()\n\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/axisgrid.py:854: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar='sd'` for the same effect.\n\n  func(*plot_args, **plot_kwargs)",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#plotting-subgroups-in-line-plots",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#plotting-subgroups-in-line-plots",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Plotting subgroups in line plots",
    "text": "Plotting subgroups in line plots\nLet’s continue to look at the mpg dataset. We’ve seen that the average miles per gallon for cars has increased over time, but how has the average horsepower for cars changed over time? And does this trend differ by country of origin? ### Instructions - Use relplot() and the mpg DataFrame to create a line plot with “model_year” on the x-axis and “horsepower” on the y-axis. Turn off the confidence intervals on the plot. - Create different lines for each country of origin (“origin”) that vary in both line style and color. - Add markers for each data point to the lines. - Use the dashes parameter to use solid lines for all countries, while still allowing for different marker styles for each line.\n\n#Use relplot() and the mpg DataFrame to create a line plot with \"model_year\" on the x-axis and \"horsepower\" on the y-axis. Turn off the confidence intervals on the plot.\n# Create line plot of model year vs. horsepower\nsns.relplot(x=\"model_year\", y = \"horsepower\", data=mpg,kind=\"line\", ci=None)\n# Show plot\nplt.show()\n\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/axisgrid.py:854: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\n#Create different lines for each country of origin (\"origin\") that vary in both line style and color.\n# Change to create subgroups for country of origin\nsns.relplot(x=\"model_year\", y=\"horsepower\", \n            data=mpg, kind=\"line\", style=\"origin\", hue=\"origin\",\n            ci=None)\n\n# Show plot\nplt.show()\n\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/axisgrid.py:854: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\n# Add markers for each data point to the lines.\n# Use the dashes parameter to use solid lines for all countries, while still allowing for different marker styles for each line.\n# Add markers and make each line have the same style\nsns.relplot(x=\"model_year\", y=\"horsepower\", \n            data=mpg, kind=\"line\", \n            ci=None, style=\"origin\", markers=True,dashes=False,\n            hue=\"origin\")\n\n# Show plot\nplt.show()\n\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/axisgrid.py:854: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#count-plots",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#count-plots",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Count plots",
    "text": "Count plots\nIn this exercise, we’ll return to exploring our dataset that contains the responses to a survey sent out to young people. We might suspect that young people spend a lot of time on the internet, but how much do they report using the internet each day? Let’s use a count plot to break down the number of survey responses in each category and then explore whether it changes based on age.\nAs a reminder, to create a count plot, we’ll use the catplot() function and specify the name of the categorical variable to count (x=____), the pandas DataFrame to use (data=____), and the type of plot (kind=“count”).\n\nInstructions\n\nUse sns.catplot() to create a count plot using the survey_data DataFrame with “Internet usage” on the x-axis.\nMake the bars horizontal instead of vertical.\nSeparate this plot into two side-by-side column subplots based on “Age Category”, which separates respondents into those that are younger than 21 vs. 21 and older.\n\n\n# Use sns.catplot() to create a count plot using the survey_data DataFrame with \"Internet usage\" on the x-axis.\n# Create count plot of internet usage\nsns.catplot(x=\"Internet usage\",data=survey_data,kind=\"count\")\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Make the bars horizontal instead of vertical.\n# Change the orientation of the plot\nsns.catplot(y=\"Internet usage\", data=survey_data,\n            kind=\"count\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Separate this plot into two side-by-side column subplots based on \"Age Category\", which separates respondents into those that are younger than 21 vs. 21 and older.\n# Separate into column subplots based on age category\nsns.catplot(y=\"Internet usage\", data=survey_data,\n            kind=\"count\",col=\"Age Category\")\n\n# Show plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#bar-plots-with-percentages",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#bar-plots-with-percentages",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Bar plots with percentages",
    "text": "Bar plots with percentages\nLet’s continue exploring the responses to a survey sent out to young people. The variable “Interested in Math” is True if the person reported being interested or very interested in mathematics, and False otherwise. What percentage of young people report being interested in math, and does this vary based on gender? Let’s use a bar plot to find out.\nAs a reminder, we’ll create a bar plot using the catplot() function, providing the name of categorical variable to put on the x-axis (x=____), the name of the quantitative variable to summarize on the y-axis (y=____), the pandas DataFrame to use (data=____), and the type of categorical plot (kind=“bar”). ### Instructions - Use the survey_data DataFrame and sns.catplot() to create a bar plot with “Gender” on the x-axis and “Interested in Math” on the y-axis. -\n\n#Use the survey_data DataFrame and sns.catplot() to create a bar plot with \"Gender\" on the x-axis and \"Interested in Math\" on the y-axis.\n# Create a bar plot of interest in math, separated by gender\n\nsns.catplot(x=\"Gender\",y=\"Interested in Math\",data=survey_data,kind=\"bar\")\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nCustomizing bar plots\nIn this exercise, we’ll explore data from students in secondary school. The “study_time” variable records each student’s reported weekly study time as one of the following categories: “&lt;2 hours”, “2 to 5 hours”, “5 to 10 hours”, or “&gt;10 hours”. Do students who report higher amounts of studying tend to get better final grades? Let’s compare the average final grade among students in each category using a bar plot. ### Instructions - Use sns.catplot() to create a bar plot with “study_time” on the x-axis and final grade (“G3”) on the y-axis, using the student_data DataFrame. - Using the order parameter and the category_order list that is provided, rearrange the bars so that they are in order from lowest study time to highest. - Update the plot so that it no longer displays confidence intervals.\n\n#Use sns.catplot() to create a bar plot with \"study_time\" on the x-axis and final grade (\"G3\") on the y-axis, using the student_data DataFrame.\n# Create bar plot of average final grade in each study category\nsns.catplot(x=\"study_time\",y=\"G3\",kind=\"bar\", data=student_data )\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Using the order parameter and the category_order list that is provided, rearrange the bars so that they are in order from lowest study time to highest.\n# List of categories from lowest to highest\ncategory_order = [\"&lt;2 hours\", \n                  \"2 to 5 hours\", \n                  \"5 to 10 hours\", \n                  \"&gt;10 hours\"]\n\n# Rearrange the categories\nsns.catplot(x=\"study_time\", y=\"G3\",\n            data=student_data,order=category_order,\n            kind=\"bar\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Update the plot so that it no longer displays confidence intervals.\n# List of categories from lowest to highest\ncategory_order = [\"&lt;2 hours\", \n                  \"2 to 5 hours\", \n                  \"5 to 10 hours\", \n                  \"&gt;10 hours\"]\n\n# Turn off the confidence intervals\nsns.catplot(x=\"study_time\", y=\"G3\",\n            data=student_data,\n            kind=\"bar\",ci=None,\n            order=category_order)\n\n# Show plot\nplt.show()\n\n/var/folders/53/yp3kynfd7rn5y13c2wwfm33rmgtrfb/T/ipykernel_38232/451744100.py:9: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.catplot(x=\"study_time\", y=\"G3\",\n\n\n\n\n\n\n\n\n\n\n\nCreate and interpret a box plot\nLet’s continue using the student_data dataset. In an earlier exercise, we explored the relationship between studying and final grade by using a bar plot to compare the average final grade (“G3”) among students in different categories of “study_time”.\nIn this exercise, we’ll try using a box plot look at this relationship instead. As a reminder, to create a box plot you’ll need to use the catplot() function and specify the name of the categorical variable to put on the x-axis (x=____), the name of the quantitative variable to summarize on the y-axis (y=____), the pandas DataFrame to use (data=____), and the type of plot (kind=“box”). ### Instructions - Use sns.catplot() and the student_data DataFrame to create a box plot with “study_time” on the x-axis and “G3” on the y-axis. Set the ordering of the categories to study_time_order.\n\n#Use sns.catplot() and the student_data DataFrame to create a box plot with \"study_time\" on the x-axis and \"G3\" on the y-axis. Set the ordering of the categories to study_time_order.\n#Specify the category ordering\nstudy_time_order = [\"&lt;2 hours\", \"2 to 5 hours\", \n                    \"5 to 10 hours\", \"&gt;10 hours\"]\n\n# Create a box plot and set the order of the categories\n\nsns.catplot(x=\"study_time\", y=\"G3\", data=student_data,order=study_time_order, kind=\"box\")\n\n\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOmitting outliers\nNow let’s use the student_data dataset to compare the distribution of final grades (“G3”) between students who have internet access at home and those who don’t. To do this, we’ll use the “internet” variable, which is a binary (yes/no) indicator of whether the student has internet access at home.\nSince internet may be less accessible in rural areas, we’ll add subgroups based on where the student lives. For this, we can use the “location” variable, which is an indicator of whether a student lives in an urban (“Urban”) or rural (“Rural”) location. ### Instructions - Use sns.catplot() to create a box plot with the student_data DataFrame, putting “internet” on the x-axis and “G3” on the y-axis. - Add subgroups so each box plot is colored based on “location”. - Do not display the outliers.\n\n# Create a box plot with subgroups and omit the outliers\nsns.catplot(x=\"internet\",y=\"G3\", data=student_data,kind=\"box\",hue=\"location\")\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAdjusting the whiskers\nIn the lesson we saw that there are multiple ways to define the whiskers in a box plot. In this set of exercises, we’ll continue to use the student_data dataset to compare the distribution of final grades (“G3”) between students who are in a romantic relationship and those that are not. We’ll use the “romantic” variable, which is a yes/no indicator of whether the student is in a romantic relationship.\nLet’s create a box plot to look at this relationship and try different ways to define the whiskers. ### Instructions - Adjust the code to make the box plot whiskers to extend to 0.5 * IQR. Recall: the IQR is the interquartile range. - Change the code to set the whiskers to extend to the 5th and 95th percentiles. - Change the code to set the whiskers to extend to the min and max values.\n\n# Adjust the code to make the box plot whiskers to extend to 0.5 * IQR. Recall: the IQR is the interquartile range.\n# Set the whiskers to 0.5 * IQR\nsns.catplot(x=\"romantic\", y=\"G3\",\n            data=student_data,whis=0.5,\n            kind=\"box\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Change the code to set the whiskers to extend to the 5th and 95th percentiles.\n#Extend the whiskers to the 5th and 95th percentile\nsns.catplot(x=\"romantic\", y=\"G3\",\n            data=student_data,\n            kind=\"box\",\n            whis=[5,95])\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n# Change the code to set the whiskers to extend to the min and max values.\n# Set the whiskers at the min and max values\nsns.catplot(x=\"romantic\", y=\"G3\",\n            data=student_data,\n            kind=\"box\",\n            whis=[0, 100])\n\n# Show plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#customizing-point-plots",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#customizing-point-plots",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Customizing point plots",
    "text": "Customizing point plots\nLet’s continue to look at data from students in secondary school, this time using a point plot to answer the question: does the quality of the student’s family relationship influence the number of absences the student has in school? Here, we’ll use the “famrel” variable, which describes the quality of a student’s family relationship from 1 (very bad) to 5 (very good).\nAs a reminder, to create a point plot, use the catplot() function and specify the name of the categorical variable to put on the x-axis (x=____), the name of the quantitative variable to summarize on the y-axis (y=____), the pandas DataFrame to use (data=____), and the type of categorical plot (kind=“point”). ### Instructions - Use sns.catplot() and the student_data DataFrame to create a point plot with “famrel” on the x-axis and number of absences (“absences”) on the y-axis. - Add “caps” to the end of the confidence intervals with size 0.2. - Remove the lines joining the points in each category.\n\n#Use sns.catplot() and the student_data DataFrame to create a point plot with \"famrel\" on the x-axis and number of absences (\"absences\") on the y-axis.\n# Create a point plot of family relationship vs. absences\nsns.catplot(x=\"famrel\",y=\"absences\",data=student_data,kind=\"point\")\n          \n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Add \"caps\" to the end of the confidence intervals with size 0.2.\n# Add caps to the confidence interval\nsns.catplot(x=\"famrel\", y=\"absences\",\n            data=student_data,capsize=0.2,\n            kind=\"point\")\n        \n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Remove the lines joining the points in each category.\n# Remove the lines joining the points\nsns.catplot(x=\"famrel\", y=\"absences\",\n            data=student_data,\n            kind=\"point\",join=False,\n            capsize=0.2)\n            \n# Show plot\nplt.show()\n\n/var/folders/53/yp3kynfd7rn5y13c2wwfm33rmgtrfb/T/ipykernel_38232/2125230447.py:3: UserWarning: \n\nThe `join` parameter is deprecated and will be removed in v0.15.0. You can remove the line between points with `linestyle='none'`.\n\n  sns.catplot(x=\"famrel\", y=\"absences\",",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#point-plots-with-subgroups",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#point-plots-with-subgroups",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Point plots with subgroups",
    "text": "Point plots with subgroups\nLet’s continue exploring the dataset of students in secondary school. This time, we’ll ask the question: is being in a romantic relationship associated with higher or lower school attendance? And does this association differ by which school the students attend? Let’s find out using a point plot. ### Instructions - Use sns.catplot() and the student_data DataFrame to create a point plot with relationship status (“romantic”) on the x-axis and number of absences (“absences”) on the y-axis. Color the points based on the school that they attend (“school”). - Turn off the confidence intervals for the plot. - Since there may be outliers of students with many absences, use the median function that we’ve imported from numpy to display the median number of absences instead of the average.\n\n#Use sns.catplot() and the student_data DataFrame to create a point plot with relationship status (\"romantic\") on the x-axis and number of absences (\"absences\") on the y-axis. Color the points based on the school that they attend (\"school\").\n# Create a point plot that uses color to create subgroups\nsns.catplot(x=\"romantic\",y=\"absences\", data=student_data,kind=\"point\",hue=\"school\")\n\n\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Turn off the confidence intervals for the plot.\n# Turn off the confidence intervals for this plot\nsns.catplot(x=\"romantic\", y=\"absences\",\n            data=student_data,\n            kind=\"point\",ci=None,\n            hue=\"school\")\n\n# Show plot\nplt.show()\n\n/var/folders/53/yp3kynfd7rn5y13c2wwfm33rmgtrfb/T/ipykernel_38232/1627080475.py:3: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.catplot(x=\"romantic\", y=\"absences\",\n\n\n\n\n\n\n\n\n\n\n#Since there may be outliers of students with many absences, use the median function that we've imported from numpy to display the median number of absences instead of the average.\n# Import median function from numpy\nfrom numpy import median\n\n# Plot the median number of absences instead of the mean\nsns.catplot(x=\"romantic\", y=\"absences\",\n            data=student_data,\n            kind=\"point\",\n            hue=\"school\",estimator=median,\n            ci=None)\n\n# Show plot\nplt.show()\n\n/var/folders/53/yp3kynfd7rn5y13c2wwfm33rmgtrfb/T/ipykernel_38232/2085041250.py:6: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  sns.catplot(x=\"romantic\", y=\"absences\",",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#changing-style-and-palette",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#changing-style-and-palette",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Changing style and palette",
    "text": "Changing style and palette\nLet’s return to our dataset containing the results of a survey given to young people about their habits and preferences. We’ve provided the code to create a count plot of their responses to the question “How often do you listen to your parents’ advice?”. Now let’s change the style and palette to make this plot easier to interpret. ### Instructions - Set the style to “whitegrid” to help the audience determine the number of responses in each category. - Set the color palette to the sequential palette named “Purples”. - Change the color palette to the diverging palette named “RdBu”.\n\n# Mapping numerical values to string categories\ncategory_mapping = {1: 'Never', 2: 'Rarely', 3: 'Sometimes', 4: 'Often', 5: 'Always'}\n\n# Applying the mapping to the \"Parents Advice\" column\nsurvey_data[\"Parents Advice\"] = survey_data[\"Parents Advice\"].map(category_mapping)\n\n\n#Set the style to \"whitegrid\" to help the audience determine the number of responses in each category.\n# Set the style to \"whitegrid\"\nsns.set_style(\"whitegrid\")\n\n# Create a count plot of survey responses\ncategory_order = [\"Never\", \"Rarely\", \"Sometimes\", \n                  \"Often\", \"Always\"]\n\nsns.catplot(x=\"Parents Advice\", \n            data=survey_data, \n            kind=\"count\", \n            order=category_order)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Set the color palette to the sequential palette named \"Purples\".\n# Set the color palette to \"Purples\"\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"Purples\")\n\n\n# Create a count plot of survey responses\ncategory_order = [\"Never\", \"Rarely\", \"Sometimes\", \n                  \"Often\", \"Always\"]\n\nsns.catplot(x=\"Parents Advice\", \n            data=survey_data, \n            kind=\"count\", \n            order=category_order)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n#Change the color palette to the diverging palette named \"RdBu\".\n# Change the color palette to \"RdBu\"\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"RdBu\")\n\n# Create a count plot of survey responses\ncategory_order = [\"Never\", \"Rarely\", \"Sometimes\", \n                  \"Often\", \"Always\"]\n\nsns.catplot(x=\"Parents Advice\", \n            data=survey_data, \n            kind=\"count\", \n            order=category_order)\n\n# Show plot\nplt.show()",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "7_Introduction_to_Data_Visualization_with_Seaborn.html#changing-the-scale",
    "href": "7_Introduction_to_Data_Visualization_with_Seaborn.html#changing-the-scale",
    "title": "Introduction to Data Visualization with Seaborn",
    "section": "Changing the scale",
    "text": "Changing the scale\nIn this exercise, we’ll continue to look at the dataset containing responses from a survey of young people. Does the percentage of people reporting that they feel lonely vary depending on how many siblings they have? Let’s find out using a bar plot, while also exploring Seaborn’s four different plot scales (“contexts”). ### Instructions - Set the scale (“context”) to “paper”, which is the smallest of the scale options. - Change the context to “notebook” to increase the scale. - Change the context to “talk” to increase the scale. - Change the context to “poster”, which is the largest scale available.\n\n#Set the scale (\"context\") to \"paper\", which is the smallest of the scale options.\n# Set the context to \"paper\"\nsns.set_context(\"paper\")\n# Create a count plot of survey responses\ncategory_order = [\"Never\", \"Rarely\", \"Sometimes\", \n                  \"Often\", \"Always\"]\n\nsns.catplot(x=\"Parents Advice\", \n            data=survey_data, \n            kind=\"count\", \n            order=category_order)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n# Set the context to \"notebook\"\nsns.set_context(\"notebook\")\n# Create a count plot of survey responses\ncategory_order = [\"Never\", \"Rarely\", \"Sometimes\", \n                  \"Often\", \"Always\"]\n\nsns.catplot(x=\"Parents Advice\", \n            data=survey_data, \n            kind=\"count\", \n            order=category_order)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n# Set the context to \"talk\"\nsns.set_context(\"talk\")\n# Create a count plot of survey responses\ncategory_order = [\"Never\", \"Rarely\", \"Sometimes\", \n                  \"Often\", \"Always\"]\n\nsns.catplot(x=\"Parents Advice\", \n            data=survey_data, \n            kind=\"count\", \n            order=category_order)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n# Set the context to \"poster\"\nsns.set_context(\"poster\")\n# Create a count plot of survey responses\ncategory_order = [\"Never\", \"Rarely\", \"Sometimes\", \n                  \"Often\", \"Always\"]\n\nsns.catplot(x=\"Parents Advice\", \n            data=survey_data, \n            kind=\"count\", \n            order=category_order)\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nsurvey_data[\"Parents Advice\"]\n\n0           Often\n1          Rarely\n2       Sometimes\n3          Rarely\n4       Sometimes\n          ...    \n1005        Often\n1006        Often\n1007        Often\n1008    Sometimes\n1009    Sometimes\nName: Parents Advice, Length: 1010, dtype: object\n\n\n\nsurvey_data[\"Parents Advice\"].unique()\n\narray(['Often', 'Rarely', 'Sometimes', 'Never', 'Always', nan],\n      dtype=object)\n\n\n\nsurvey_data.columns\n\nIndex(['Unnamed: 0', 'Music', 'Techno', 'Movies', 'History', 'Mathematics',\n       'Pets', 'Spiders', 'Loneliness', 'Parents Advice', 'Internet usage',\n       'Finances', 'Age', 'Siblings', 'Gender', 'Village - town',\n       'Age Category', 'Interested in Math'],\n      dtype='object')",
    "crumbs": [
      "Introduction to Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html",
    "href": "8_Python_Data_Science_Toolbox_Part1.html",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "",
    "text": "Explore Datasets\nUse the DataFrame imported in the first cell to explore the data and practice your skills! - Write a function that takes a timestamp (see column timestamp_ms) and returns the text of any tweet published at that timestamp. Additionally, make it so that users can pass column names as flexible arguments (*args) so that the function can print out any other columns users want to see. - In a filter() call, write a lambda function to return tweets created on a Tuesday. Tip: look at the first three characters of the created_at column. - Make sure to add error handling on the functions you’ve created!",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#write-a-simple-function",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#write-a-simple-function",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Write a simple function",
    "text": "Write a simple function\nIn the last video, Hugo described the basics of how to define a function. You will now write your own function!\nDefine a function, shout(), which simply prints out a string with three exclamation marks ‘!!!’ at the end. The code for the square() function that we wrote earlier is found below. You can use it as a pattern to define shout().\n`def square():\nnew_value = 4 ** 2\n\nreturn new_value\n` ### Instructions - Complete the function header by adding the appropriate function name, shout. - In the function body, concatenate the string, ‘congratulations’ with another string, ‘!!!’. Assign the result to shout_word. - Print the value of shout_word. - Call the shout function.\n# Define the function shout\ndef shout():\n    \"\"\"Print a string with three exclamation marks\"\"\"\n    # Concatenate the strings: shout_word\n    shout_word='congratulations' + '!!!'\n\n    # Print shout_word\n    print(shout_word)\n\n# Call shout\nshout()",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#single-parameter-functions",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#single-parameter-functions",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Single-parameter functions",
    "text": "Single-parameter functions\nIn the previous exercise, you defined and called the function shout(), which printed out a string concatenated with ‘!!!’. You will now update shout() by adding a parameter so that it can accept and process any string argument passed to it. Also note that shout(word), the part of the header that specifies the function name and parameter(s), is known as the signature of the function. You may encounter this term in the wild! ### Instructions - Complete the function header by adding the parameter name, word. - Assign the result of concatenating word with ‘!!!’ to shout_word. - Print the value of shout_word. - Call the shout() function, passing to it the string, ‘congratulations’.\n# Define shout with the parameter, word\ndef shout(word):\n    \"\"\"Print a string with three exclamation marks\"\"\"\n    # Concatenate the strings: shout_word\n    shout_word = word + '!!!'\n\n    # Print shout_word\n    print(shout_word)\n\n# Call shout with the string 'congratulations'\nshout('congratulations')",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#functions-that-return-single-values",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#functions-that-return-single-values",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Functions that return single values",
    "text": "Functions that return single values\nTry your hand at another modification to the shout() function so that it now returns a single value instead of printing within the function. Recall that the return keyword lets you return values from functions. Parts of the function shout(), which you wrote earlier, are shown. Returning values is generally more desirable than printing them out because, as you saw earlier, a print() call assigned to a variable has type NoneType. ### Instructions - In the function body, concatenate the string in word with ‘!!!’ and assign to shout_word. - Replace the print() statement with the appropriate return statement. - Call the shout() function, passing to it the string, ‘congratulations’, and assigning the call to the variable, yell. - To check if yell contains the value returned by shout(), print the value of yell.\n# Define shout with the parameter, word\ndef shout(word):\n    \"\"\"Return a string with three exclamation marks\"\"\"\n    # Concatenate the strings: shout_word\n    shout_word = word + '!!!'\n\n    # Replace print with return\n    return shout_word\n\n# Pass 'congratulations' to shout: yell\nyell = shout('congratulations')\n\n# Print yell\nprint(yell)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-multiple-parameters",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-multiple-parameters",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Functions with multiple parameters",
    "text": "Functions with multiple parameters\nYou are now going to use what you’ve learned to modify the shout() function further. Here, you will modify shout() to accept two arguments. Parts of the function shout(), which you wrote earlier, are shown. ### Instructions - Modify the function header such that it accepts two parameters, word1 and word2, in that order. - Concatenate each of word1 and word2 with ‘!!!’ and assign to shout1 and shout2, respectively. - Concatenate shout1 and shout2 together, in that order, and assign to new_shout. - Pass the strings ‘congratulations’ and ‘you’, in that order, to a call to shout(). Assign the return value to yell.\n# Define shout with parameters word1 and word2\ndef shout(word1, word2):\n    \"\"\"Concatenate strings with three exclamation marks\"\"\"\n    # Concatenate word1 with '!!!': shout1\n    shout1 = word1 + '!!!'\n    \n    # Concatenate word2 with '!!!': shout2\n    \n    shout2 = word2 + '!!!'\n    # Concatenate shout1 with shout2: new_shout\n    new_shout = shout1 + shout2\n\n    # Return new_shout\n    return new_shout\n\n# Pass 'congratulations' and 'you' to shout(): yell\nyell = shout(\"congratulations\",'you')\n\n# Print yell\nprint(yell)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#a-brief-introduction-to-tuples",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#a-brief-introduction-to-tuples",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "A brief introduction to tuples",
    "text": "A brief introduction to tuples\nAlongside learning about functions, you’ve also learned about tuples! Here, you will practice what you’ve learned about tuples: how to construct, unpack, and access tuple elements. Recall how Hugo unpacked the tuple even_nums in the video:\na, b, c = even_nums\nA three-element tuple named nums has been preloaded for this exercise. Before completing the script, perform the following:\n\nPrint out the value of nums in the IPython shell. Note the elements in the tuple.\nIn the IPython shell, try to change the first element of nums to the value 2 by doing an assignment: nums[0] = 2. What happens?\n\n\nInstructions\n\nUnpack nums to the variables num1, num2, and num3.\nConstruct a new tuple, even_nums composed of the same elements in nums, but with the 1st element replaced with the value, 2.\n\nnums=(1,4,8)\n# Unpack nums into num1, num2, and num3\nnum1,num2,num3 = nums\n\n# Construct even_nums\neven_nums = (2,num2,num3)\n\n\nFunctions that return multiple values\nIn the previous exercise, you constructed tuples, assigned tuples to variables, and unpacked tuples. Here you will return multiple values from a function using tuples. Let’s now update our shout() function to return multiple values. Instead of returning just one string, we will return two strings with the string !!! concatenated to each.\nNote that the return statement return x, y has the same result as return (x, y): the former actually packs x and y into a tuple under the hood! ## Instructions - Modify the function header such that the function name is now shout_all, and it accepts two parameters, word1 and word2, in that order. - Concatenate the string ‘!!!’ to each of word1 and word2 and assign to shout1 and shout2, respectively. - Construct a tuple shout_words, composed of shout1 and shout2. - Call shout_all() with the strings ‘congratulations’ and ‘you’ and assign the result to yell1 and yell2 (remember, shout_all() returns 2 variables!).\n# Define shout_all with parameters word1 and word2\ndef shout_all(word1, word2):\n    \n    # Concatenate word1 with '!!!': shout1\n    shout1= word1+\"!!!\"\n    \n    # Concatenate word2 with '!!!': shout2\n    shout2= word2+\"!!!\"\n    \n    # Construct a tuple with shout1 and shout2: shout_words\n    shout_words=(shout1,shout2)\n\n    # Return shout_words\n    return shout_words\n\n# Pass 'congratulations' and 'you' to shout_all(): yell1, yell2\nyell1,yell2 = shout_all('congratulations' , 'you')\n\n# Print yell1 and yell2\nprint(yell1)\nprint(yell2)\n\n\nBringing it all together (1)\nYou’ve got your first taste of writing your own functions in the previous exercises. You’ve learned how to add parameters to your own function definitions, return a value or multiple values with tuples, and how to call the functions you’ve defined.\nIn this and the following exercise, you will bring together all these concepts and apply them to a simple data science problem. You will load a dataset and develop functionalities to extract simple insights from the data.\nFor this exercise, your goal is to recall how to load a dataset into a DataFrame. The dataset contains Twitter data and you will iterate over entries in a column to build a dictionary in which the keys are the names of languages and the values are the number of tweets in the given language. The file tweets.csv is available in your current directory.\nBe aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data). ### Instructions - Import the pandas package with the alias pd. - Import the file ‘tweets.csv’ using the pandas function read_csv(). Assign the resulting DataFrame to df. - Complete the for loop by iterating over col, the ‘lang’ column in the DataFrame df. - Complete the bodies of the if-else statements in the for loop: if the key is in the dictionary langs_count, add 1 to the value corresponding to this key in the dictionary, else add the key to langs_count and set the corresponding value to 1. Use the loop variable entry in your code.\n# Import pandas\nimport pandas as pd\n\n# Import Twitter data as DataFrame: df\ndf = pd.read_csv(\"datasets/tweets.csv\")\n\n# Initialize an empty dictionary: langs_count\nlangs_count = {}\n\n# Extract column from DataFrame: col\ncol = df['lang']\n\n# Iterate over lang column in DataFrame\nfor entry in col:\n\n    # If the language is in langs_count, add 1 \n    if entry in langs_count.keys():\n        langs_count[entry] +=1\n    # Else add the language to langs_count, set the value to 1\n    else:\n        langs_count[entry]=1\n\n# Print the populated dictionary\nprint(langs_count)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-2",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-2",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Bringing it all together (2)",
    "text": "Bringing it all together (2)\nGreat job! You’ve now defined the functionality for iterating over entries in a column and building a dictionary with keys the names of languages and values the number of tweets in the given language.\nIn this exercise, you will define a function with the functionality you developed in the previous exercise, return the resulting dictionary from within the function, and call the function with the appropriate arguments.\nFor your convenience, the pandas package has been imported as pd and the ‘tweets.csv’ file has been imported into the tweets_df variable. ### Instructions - Define the function count_entries(), which has two parameters. The first parameter is df for the DataFrame and the second is col_name for the column name. - Complete the bodies of the if-else statements in the for loop: if the key is in the dictionary langs_count, add 1 to its current value, else add the key to langs_count and set its value to 1. Use the loop variable entry in your code. - Return the langs_count dictionary from inside the count_entries() function. - Call the count_entries() function by passing to it tweets_df and the name of the column, ‘lang’. Assign the result of the call to the variable result.\ntweets_df=pd.read_csv('datasets/tweets.csv')\n# Define count_entries()\ndef count_entries(df, col_name):\n    \"\"\"Return a dictionary with counts of \n    occurrences as value for each key.\"\"\"\n\n    # Initialize an empty dictionary: langs_count\n    langs_count = {}\n    \n    # Extract column from DataFrame: col\n    col = df[col_name]\n    \n    # Iterate over lang column in DataFrame\n    for entry in col:\n\n        # If the language is in langs_count, add 1\n        if entry in langs_count.keys():\n            langs_count[entry] +=1\n        # Else add the language to langs_count, set the value to 1\n        else:\n            langs_count[entry] =1\n\n    # Return the langs_count dictionary\n    return langs_count\n    \n\n# Call count_entries(): result\nresult=count_entries(tweets_df,'lang')\n\n# Print the result\nprint(result)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#the-keyword-global",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#the-keyword-global",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "The keyword global",
    "text": "The keyword global\nLet’s work more on your mastery of scope. In this exercise, you will use the keyword global within a function to alter the value of a variable defined in the global scope. ### Instructions - Use the keyword global to alter the object team in the global scope. - Change the value of team in the global scope to the string “justice league”. Assign the result to team. - Hit the Submit button to see how executing your newly defined function change_team() changes the value of the name team!\n# Create a string: team\nteam = \"teen titans\"\n\n# Define change_team()\ndef change_team():\n    \"\"\"Change the value of the global variable team.\"\"\"\n\n    # Use team in global scope\n    global team\n\n    # Change the value of team in global: team\n    team = \"justice league\"\n# Print team\nprint(team)\n\n# Call change_team()\nchange_team()\n\n# Print team\nprint(team)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#nested-functions-i",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#nested-functions-i",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Nested Functions I",
    "text": "Nested Functions I\nYou’ve learned in the last video about nesting functions within functions. One reason why you’d like to do this is to avoid writing out the same computations within functions repeatedly. There’s nothing new about defining nested functions: you simply define it as you would a regular function with def and embed it inside another function!\nIn this exercise, inside a function three_shouts(), you will define a nested function inner() that concatenates a string object with !!!. three_shouts() then returns a tuple of three elements, each a string concatenated with !!! using inner(). Go for it! ### Instructions - Complete the function header of the nested function with the function name inner() and a single parameter word. - Complete the return value: each element of the tuple should be a call to inner(), passing in the parameters from three_shouts() as arguments to each call.\n# Define three_shouts\ndef three_shouts(word1, word2, word3):\n    \"\"\"Returns a tuple of strings\n    concatenated with '!!!'.\"\"\"\n\n    # Define inner\n    def inner(word):\n        \"\"\"Returns a string concatenated with '!!!'.\"\"\"\n        return word + '!!!'\n\n    # Return a tuple of strings\n    return (inner(word1), inner(word2), inner(word3))\n\n# Call three_shouts() and print\nprint(three_shouts('a', 'b', 'c'))",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#nested-functions-ii",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#nested-functions-ii",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Nested Functions II",
    "text": "Nested Functions II\nGreat job, you’ve just nested a function within another function. One other pretty cool reason for nesting functions is the idea of a closure. This means that the nested or inner function remembers the state of its enclosing scope when called. Thus, anything defined locally in the enclosing scope is available to the inner function even when the outer function has finished execution.\nLet’s move forward then! In this exercise, you will complete the definition of the inner function inner_echo() and then call echo() a couple of times, each with a different argument. Complete the exercise and see what the output will be! ### Instructions - Complete the function header of the inner function with the function name inner_echo() and a single parameter word1. - Complete the function echo() so that it returns inner_echo. - We have called echo(), passing 2 as an argument, and assigned the resulting function to twice. Your job is to call echo(), passing 3 as an argument. Assign the resulting function to thrice. - Hit Submit to call twice() and thrice() and print the results.\n# Define echo\ndef echo(n):\n    \"\"\"Return the inner_echo function.\"\"\"\n\n    # Define inner_echo\n    def inner_echo(word1):\n        \"\"\"Concatenate n copies of word1.\"\"\"\n        echo_word = word1 * n\n        return echo_word\n\n    # Return inner_echo\n    return inner_echo\n\n# Call echo: twice\ntwice = echo(2)\n\n# Call echo: thrice\nthrice = echo(3)\n\n# Call twice() and thrice() then print\nprint(twice('hello'), thrice('hello'))",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#the-keyword-nonlocal-and-nested-functions",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#the-keyword-nonlocal-and-nested-functions",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "The keyword nonlocal and nested functions",
    "text": "The keyword nonlocal and nested functions\nLet’s once again work further on your mastery of scope! In this exercise, you will use the keyword nonlocal within a nested function to alter the value of a variable defined in the enclosing scope. ### Instructions - Assign to echo_word the string word, concatenated with itself. - Use the keyword nonlocal to alter the value of echo_word in the enclosing scope. - Alter echo_word to echo_word concatenated with ‘!!!’. - Call the function echo_shout(), passing it a single argument ‘hello’.\n# Define echo_shout()\ndef echo_shout(word):\n    \"\"\"Change the value of a nonlocal variable\"\"\"\n    \n    # Concatenate word with itself: echo_word\n    echo_word = word + word\n    \n    # Print echo_word\n    print(echo_word)\n    \n    # Define inner function shout()\n    def shout():\n        \"\"\"Alter a variable in the enclosing scope\"\"\"    \n        # Use echo_word in nonlocal scope\n        nonlocal echo_word\n        \n        # Change echo_word to echo_word concatenated with '!!!'\n        echo_word = echo_word + \"!!!\"\n    \n    # Call function shout()\n    shout()\n    \n    # Print echo_word\n    print(echo_word)\n\n# Call function echo_shout() with argument 'hello'\necho_shout('hello')",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-one-default-argument",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-one-default-argument",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Functions with one default argument",
    "text": "Functions with one default argument\nIn the previous chapter, you’ve learned to define functions with more than one parameter and then calling those functions by passing the required number of arguments. In the last video, Hugo built on this idea by showing you how to define functions with default arguments. You will practice that skill in this exercise by writing a function that uses a default argument and then calling the function a couple of times. ### Instructions - Complete the function header with the function name shout_echo. It accepts an argument word1 and a default argument echo with default value 1, in that order. - Use the * operator to concatenate echo copies of word1. Assign the result to echo_word. - Call shout_echo() with just the string, “Hey”. Assign the result to no_echo. - Call shout_echo() with the string “Hey” and the value 5 for the default argument, echo. Assign the result to with_echo.\n# Define shout_echo\ndef shout_echo(word1, echo=1):\n    \"\"\"Concatenate echo copies of word1 and three\n     exclamation marks at the end of the string.\"\"\"\n\n    # Concatenate echo copies of word1 using *: echo_word\n    echo_word = word1*echo\n\n    # Concatenate '!!!' to echo_word: shout_word\n    shout_word = echo_word + '!!!'\n\n    # Return shout_word\n    return shout_word\n\n# Call shout_echo() with \"Hey\": no_echo\nno_echo = shout_echo(\"Hey\")\n\n# Call shout_echo() with \"Hey\" and echo=5: with_echo\nwith_echo = shout_echo(\"Hey\",echo=5)\n\n# Print no_echo and with_echo\nprint(no_echo)\nprint(with_echo)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-multiple-default-arguments",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-multiple-default-arguments",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Functions with multiple default arguments",
    "text": "Functions with multiple default arguments\nYou’ve now defined a function that uses a default argument - don’t stop there just yet! You will now try your hand at defining a function with more than one default argument and then calling this function in various ways.\nAfter defining the function, you will call it by supplying values to all the default arguments of the function. Additionally, you will call the function by not passing a value to one of the default arguments - see how that changes the output of your function! ### Instructions - Complete the function header with the function name shout_echo. It accepts an argument word1, a default argument echo with default value 1 and a default argument intense with default value False, in that order. - In the body of the if statement, make the string object echo_word upper case by applying the method .upper() on it. - Call shout_echo() with the string, “Hey”, the value 5 for echo and the value True for intense. Assign the result to with_big_echo. - Call shout_echo() with the string “Hey” and the value True for intense. Assign the result to big_no_echo.\n# Define shout_echo\ndef shout_echo(word1, echo=1, intense=False):\n    \"\"\"Concatenate echo copies of word1 and three\n    exclamation marks at the end of the string.\"\"\"\n\n    # Concatenate echo copies of word1 using *: echo_word\n    echo_word = word1 * echo\n\n    # Make echo_word uppercase if intense is True\n    if intense is True:\n        # Make uppercase and concatenate '!!!': echo_word_new\n        echo_word_new = str.upper(echo_word) + '!!!'\n    else:\n        # Concatenate '!!!' to echo_word: echo_word_new\n        echo_word_new = echo_word + '!!!'\n\n    # Return echo_word_new\n    return echo_word_new\n\n# Call shout_echo() with \"Hey\", echo=5 and intense=True: with_big_echo\nwith_big_echo = shout_echo(\"Hey\",echo=5,intense=True)\n\n# Call shout_echo() with \"Hey\" and intense=True: big_no_echo\nbig_no_echo = shout_echo(\"Hey\",intense=True)\n\n# Print values\nprint(with_big_echo)\nprint(big_no_echo)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-variable-length-arguments-args",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-variable-length-arguments-args",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Functions with variable-length arguments (*args)",
    "text": "Functions with variable-length arguments (*args)\nFlexible arguments enable you to pass a variable number of arguments to a function. In this exercise, you will practice defining a function that accepts a variable number of string arguments.\nThe function you will define is gibberish() which can accept a variable number of string values. Its return value is a single string composed of all the string arguments concatenated together in the order they were passed to the function call. You will call the function with a single string argument and see how the output changes with another call using more than one string argument. Recall from the previous video that, within the function definition, args is a tuple. ### Instructions - Complete the function header with the function name gibberish. It accepts a single flexible argument *args. - Initialize a variable hodgepodge to an empty string. - Return the variable hodgepodge at the end of the function body. - Call gibberish() with the single string, “luke”. Assign the result to one_word. - Hit the Submit button to call gibberish() with multiple arguments and to print the value to the Shell.\n# Define gibberish\ndef gibberish(*args):\n    \"\"\"Concatenate strings in *args together.\"\"\"\n\n    # Initialize an empty string: hodgepodge\n    hodgepodge=\"\"\n\n    # Concatenate the strings in args\n    for word in args:\n        hodgepodge += word\n\n    # Return hodgepodge\n    return hodgepodge\n\n# Call gibberish() with one string: one_word\none_word = gibberish('luke')\n\n# Call gibberish() with five strings: many_words\nmany_words = gibberish(\"luke\", \"leia\", \"han\", \"obi\", \"darth\")\n\n# Print one_word and many_words\nprint(one_word)\nprint(many_words)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-variable-length-keyword-arguments-kwargs",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#functions-with-variable-length-keyword-arguments-kwargs",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Functions with variable-length keyword arguments (**kwargs)",
    "text": "Functions with variable-length keyword arguments (**kwargs)\nLet’s push further on what you’ve learned about flexible arguments - you’ve used *args, you’re now going to use **kwargs! What makes **kwargs different is that it allows you to pass a variable number of keyword arguments to functions. Recall from the previous video that, within the function definition, kwargs is a dictionary.\nTo understand this idea better, you’re going to use **kwargs in this exercise to define a function that accepts a variable number of keyword arguments. The function simulates a simple status report system that prints out the status of a character in a movie. ### Instructions - Complete the function header with the function name report_status. It accepts a single flexible argument **kwargs. - Iterate over the key-value pairs of kwargs to print out the keys and values, separated by a colon ‘:’. - In the first call to report_status(), pass the following keyword-value pairs: name=“luke”, affiliation=“jedi” and status=“missing”. - In the second call to report_status(), pass the following keyword-value pairs: name=“anakin”, affiliation=“sith lord” and status=“deceased”.\n# Define report_status\ndef report_status(**kwargs):\n    \"\"\"Print out the status of a movie character.\"\"\"\n\n    print(\"\\nBEGIN: REPORT\\n\")\n\n    # Iterate over the key-value pairs of kwargs\n    for key, value in kwargs.items():\n        # Print out the keys and values, separated by a colon ':'\n        print(key + \": \" + value)\n\n    print(\"\\nEND REPORT\")\n\n# First call to report_status()\nreport_status(name=\"luke\", affiliation=\"jedi\" , status=\"missing\")\n\n# Second call to report_status()\nreport_status(name=\"anakin\", affiliation=\"sith lord\" , status=\"deceased\")",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-1-1",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-1-1",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Bringing it all together (1)",
    "text": "Bringing it all together (1)\nRecall the Bringing it all together exercise in the previous chapter where you did a simple Twitter analysis by developing a function that counts how many tweets are in certain languages. The output of your function was a dictionary that had the language as the keys and the counts of tweets in that language as the value.\nIn this exercise, we will generalize the Twitter language analysis that you did in the previous chapter. You will do that by including a default argument that takes a column name.\nFor your convenience, pandas has been imported as pd and the ‘tweets.csv’ file has been imported into the DataFrame tweets_df. Parts of the code from your previous work are also provided. ### Instructions - Complete the function header by supplying the parameter for a DataFrame df and the parameter col_name with a default value of ‘lang’ for the DataFrame column name. - Call count_entries() by passing the tweets_df DataFrame and the column name ‘lang’. Assign the result to result1. Note that since ‘lang’ is the default value of the col_name parameter, you don’t have to specify it here. - Call count_entries() by passing the tweets_df DataFrame and the column name ‘source’. Assign the result to result2.\ntweets_df = pd.read_csv('datasets/tweets.csv')\n# Define count_entries()\ndef count_entries(df, col_name='lang'):\n    \"\"\"Return a dictionary with counts of\n    occurrences as value for each key.\"\"\"\n\n    # Initialize an empty dictionary: cols_count\n    cols_count = {}\n\n    # Extract column from DataFrame: col\n    col = df[col_name]\n    \n    # Iterate over the column in DataFrame\n    for entry in col:\n\n        # If entry is in cols_count, add 1\n        if entry in cols_count.keys():\n            cols_count[entry] += 1\n\n        # Else add the entry to cols_count, set the value to 1\n        else:\n            cols_count[entry] = 1\n\n    # Return the cols_count dictionary\n    return cols_count\n\n# Call count_entries(): result1\nresult1 = count_entries(tweets_df)\n\n# Call count_entries(): result2\nresult2 = count_entries(tweets_df,'source')\n\n# Print result1 and result2\nprint(result1)\nprint(result2)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-2-1",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-2-1",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Bringing it all together (2)",
    "text": "Bringing it all together (2)\nWow, you’ve just generalized your Twitter language analysis that you did in the previous chapter to include a default argument for the column name. You’re now going to generalize this function one step further by allowing the user to pass it a flexible argument, that is, in this case, as many column names as the user would like!\nOnce again, for your convenience, pandas has been imported as pd and the ‘tweets.csv’ file has been imported into the DataFrame tweets_df. Parts of the code from your previous work are also provided. ### Instructions - Complete the function header by supplying the parameter for the DataFrame df and the flexible argument *args. - Complete the for loop within the function definition so that the loop occurs over the tuple args. - Call count_entries() by passing the tweets_df DataFrame and the column name ‘lang’. Assign the result to result1. - Call count_entries() by passing the tweets_df DataFrame and the column names ‘lang’ and ‘source’. Assign the result to result2.\n# Define count_entries()\ndef count_entries(df, *args):\n    \"\"\"Return a dictionary with counts of\n    occurrences as value for each key.\"\"\"\n    \n    #Initialize an empty dictionary: cols_count\n    cols_count = {}\n    \n    # Iterate over column names in args\n    for col_name in args:\n    \n        # Extract column from DataFrame: col\n        col = df[col_name]\n    \n        # Iterate over the column in DataFrame\n        for entry in col:\n    \n            # If entry is in cols_count, add 1\n            if entry in cols_count.keys():\n                cols_count[entry] += 1\n    \n            # Else add the entry to cols_count, set the value to 1\n            else:\n                cols_count[entry] = 1\n\n    # Return the cols_count dictionary\n    return cols_count\n\n# Call count_entries(): result1\nresult1 = count_entries(tweets_df, 'lang')\n\n# Call count_entries(): result2\nresult2 = count_entries(tweets_df, 'lang', 'source')\n\n# Print result1 and result2\nprint(result1)\nprint(result2)\nimport pandas as pd\ntweets_df=pd.read_csv('datasets/tweets.csv')",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#writing-a-lambda-function-you-already-know",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#writing-a-lambda-function-you-already-know",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Writing a lambda function you already know",
    "text": "Writing a lambda function you already know\nSome function definitions are simple enough that they can be converted to a lambda function. By doing this, you write less lines of code, which is pretty awesome and will come in handy, especially when you’re writing and maintaining big programs. In this exercise, you will use what you know about lambda functions to convert a function that does a simple task into a lambda function. Take a look at this function definition: def echo_word(word1, echo):     \"\"\"Concatenate echo copies of word1.\"\"\"     words = word1 * echo     return words The function echo_word takes 2 parameters: a string value, word1 and an integer value, echo. It returns a string that is a concatenation of echo copies of word1. Your task is to convert this simple function into a lambda function. ### Instructions - Define the lambda function echo_word using the variables word1 and echo. Replicate what the original function definition for echo_word() does above. - Call echo_word() with the string argument ‘hey’ and the value 5, in that order. Assign the call to result.\n# Define echo_word as a lambda function: echo_word\necho_word = (lambda word1, echo: word1*echo)\n\n# Call echo_word: result\nresult = echo_word('hey',5)\n\n# Print result\nprint(result)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#map-and-lambda-functions",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#map-and-lambda-functions",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Map() and lambda functions",
    "text": "Map() and lambda functions\nSo far, you’ve used lambda functions to write short, simple functions as well as to redefine functions with simple functionality. The best use case for lambda functions, however, are for when you want these simple functionalities to be anonymously embedded within larger expressions. What that means is that the functionality is not stored in the environment, unlike a function defined with def. To understand this idea better, you will use a lambda function in the context of the map() function.\nRecall from the video that map() applies a function over an object, such as a list. Here, you can use lambda functions to define the function that map() will use to process the object. For example:\n`nums = [2, 4, 6, 8, 10]\nresult = map(lambda a: a ** 2, nums)`\nYou can see here that a lambda function, which raises a value a to the power of 2, is passed to map() alongside a list of numbers, nums. The map object that results from the call to map() is stored in result. You will now practice the use of lambda functions with map(). For this exercise, you will map the functionality of the add_bangs() function you defined in previous exercises over a list of strings. ### Instructions - In the map() call, pass a lambda function that concatenates the string ‘!!!’ to a string item; also pass the list of strings, spells. Assign the resulting map object to shout_spells. - Convert shout_spells to a list and print out the list.\n# Create a list of strings: spells\nspells = [\"protego\", \"accio\", \"expecto patronum\", \"legilimens\"]\n\n# Use map() to apply a lambda function over spells: shout_spells\nshout_spells = map(lambda item: item+'!!!', spells)\n\n# Convert shout_spells to a list: shout_spells_list\nshout_spells_list = list(shout_spells)\n\n# Print the result\nprint(shout_spells_list)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#filter-and-lambda-functions",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#filter-and-lambda-functions",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Filter() and lambda functions",
    "text": "Filter() and lambda functions\nIn the previous exercise, you used lambda functions to anonymously embed an operation within map(). You will practice this again in this exercise by using a lambda function with filter(), which may be new to you! The function filter() offers a way to filter out elements from a list that don’t satisfy certain criteria.\nYour goal in this exercise is to use filter() to create, from an input list of strings, a new list that contains only strings that have more than 6 characters. ### Instructions - In the filter() call, pass a lambda function and the list of strings, fellowship. The lambda function should check if the number of characters in a string member is greater than 6; use the len() function to do this. Assign the resulting filter object to result. - Convert result to a list and print out the list.\n# Create a list of strings: fellowship\nfellowship = ['frodo', 'samwise', 'merry', 'pippin', 'aragorn', 'boromir', 'legolas', 'gimli', 'gandalf']\n\n# Use filter() to apply a lambda function over fellowship: result\nresult = filter(lambda member: len(member)&gt;6, fellowship )\n\n# Convert result to a list: result_list\nresult_list = list(result)\n\n# Print result_list\nprint(result_list)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#reduce-and-lambda-functions",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#reduce-and-lambda-functions",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Reduce() and lambda functions",
    "text": "Reduce() and lambda functions\nYou’re getting very good at using lambda functions! Here’s one more function to add to your repertoire of skills. The reduce() function is useful for performing some computation on a list and, unlike map() and filter(), returns a single value as a result. To use reduce(), you must import it from the functools module.\nRemember gibberish() from a few exercises back? \\# Define gibberish def gibberish(*args):     \"\"\"Concatenate strings in *args together.\"\"\"     hodgepodge = ''     for word in args:         hodgepodge += word     return hodgepodge gibberish() simply takes a list of strings as an argument and returns, as a single-value result, the concatenation of all of these strings. In this exercise, you will replicate this functionality by using reduce() and a lambda function that concatenates strings together. ### Instructions - Import the reduce function from the functools module. - In the reduce() call, pass a lambda function that takes two string arguments item1 and item2 and concatenates them; also pass the list of strings, stark. Assign the result to result. The first argument to reduce() should be the lambda function and the second argument is the list stark.\n# Import reduce from functools\nfrom functools import reduce\n\n# Create a list of strings: stark\nstark = ['robb', 'sansa', 'arya', 'brandon', 'rickon']\n\n# Use reduce() to apply a lambda function over stark: result\nresult = reduce(lambda item1, item2: item1 + item2, stark)\n\n# Print the result\nprint(result)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#error-handling-with-try-except",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#error-handling-with-try-except",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Error handling with try-except",
    "text": "Error handling with try-except\nA good practice in writing your own functions is also anticipating the ways in which other people (or yourself, if you accidentally misuse your own function) might use the function you defined.\nAs in the previous exercise, you saw that the len() function is able to handle input arguments such as strings, lists, and tuples, but not int type ones and raises an appropriate error and error message when it encounters invalid input arguments. One way of doing this is through exception handling with the try-except block.\nIn this exercise, you will define a function as well as use a try-except block for handling cases when incorrect input arguments are passed to the function.\nRecall the shout_echo() function you defined in previous exercises; parts of the function definition are provided in the sample code. Your goal is to complete the exception handling code in the function definition and provide an appropriate error message when raising an error. ### Instructions - Initialize the variables echo_word and shout_words to empty strings. - Add the keywords try and except in the appropriate locations for the exception handling block. - Use the * operator to concatenate echo copies of word1. Assign the result to echo_word. - Concatenate the string ‘!!!’ to echo_word. Assign the result to shout_words.\n# Define shout_echo\ndef shout_echo(word1, echo=1):\n    \"\"\"Concatenate echo copies of word1 and three\n    exclamation marks at the end of the string.\"\"\"\n\n    # Initialize empty strings: echo_word, shout_words\n    echo_word = \"\"\n    shout_words = \"\"\n    \n\n    # Add exception handling with try-except\n    try:\n        # Concatenate echo copies of word1 using *: echo_word\n        echo_word = word1*echo\n\n        # Concatenate '!!!' to echo_word: shout_words\n        shout_words = echo_word+'!!!'\n    except:\n        # Print error message\n        print(\"word1 must be a string and echo must be an integer.\")\n\n    # Return shout_words\n    return shout_words\n\n# Call shout_echo\nshout_echo(\"particle\", echo=\"accelerator\")",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#error-handling-by-raising-an-error",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#error-handling-by-raising-an-error",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Error handling by raising an error",
    "text": "Error handling by raising an error\nAnother way to raise an error is by using raise. In this exercise, you will add a raise statement to the shout_echo() function you defined before to raise an error message when the value supplied by the user to the echo argument is less than 0.\nThe call to shout_echo() uses valid argument values. To test and see how the raise statement works, simply change the value for the echo argument to a negative value. Don’t forget to change it back to valid values to move on to the next exercise! ### Instructions - Complete the if statement by checking if the value of echo is less than 0. - In the body of the if statement, add a raise statement that raises a ValueError with message ‘echo must be greater than or equal to 0’ when the value supplied by the user to echo is less than 0.\n# Define shout_echo\ndef shout_echo(word1, echo=1):\n    \"\"\"Concatenate echo copies of word1 and three\n    exclamation marks at the end of the string.\"\"\"\n\n    # Raise an error with raise\n    if echo&lt;0:\n        raise  ValueError(\"echo must be greater than or equal to 0\")\n\n    # Concatenate echo copies of word1 using *: echo_word\n    echo_word = word1 * echo\n\n    # Concatenate '!!!' to echo_word: shout_word\n    shout_word = echo_word + '!!!'\n\n    # Return shout_word\n    return shout_word\n\n# Call shout_echo\nshout_echo(\"particle\", echo=5)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-1-2",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-1-2",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Bringing it all together (1)",
    "text": "Bringing it all together (1)\nThis is awesome! You have now learned how to write anonymous functions using lambda, how to pass lambda functions as arguments to other functions such as map(), filter(), and reduce(), as well as how to write errors and output custom error messages within your functions. You will now put together these learnings to good use by working with a Twitter dataset. Before practicing your new error handling skills; in this exercise, you will write a lambda function and use filter() to select retweets, that is, tweets that begin with the string ‘RT’.\nTo help you accomplish this, the Twitter data has been imported into the DataFrame, tweets_df. Go for it! ### Instructions - In the filter() call, pass a lambda function and the sequence of tweets as strings, tweets_df[‘text’]. The lambda function should check if the first 2 characters in a tweet x are ‘RT’. Assign the resulting filter object to result. To get the first 2 characters in a tweet x, use x[0:2]. To check equality, use a Boolean filter with ==. - Convert result to a list and print out the list.\n# Select retweets from the Twitter DataFrame: result\nresult = filter(lambda x: x.startswith(\"RT\") , tweets_df['text'])\n\n# Create list from filter object result: res_list\nres_list = list(result)\n\n# Print all retweets in res_list\nfor tweet in res_list:\n    print(tweet)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-2-2",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-2-2",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Bringing it all together (2)",
    "text": "Bringing it all together (2)\nSometimes, we make mistakes when calling functions - even ones you made yourself. But don’t fret! In this exercise, you will improve on your previous work with the count_entries() function in the last chapter by adding a try-except block to it. This will allow your function to provide a helpful message when the user calls your count_entries() function but provides a column name that isn’t in the DataFrame.\nOnce again, for your convenience, pandas has been imported as pd and the ‘tweets.csv’ file has been imported into the DataFrame tweets_df. Parts of the code from your previous work are also provided. ### Instructions - Add a try block so that when the function is called with the correct arguments, it processes the DataFrame and returns a dictionary of results. - Add an except block so that when the function is called incorrectly, it displays the following error message: ‘The DataFrame does not have a’ + col_name + ’ column.’.\n# Define count_entries()\ndef count_entries(df, col_name='lang'):\n    \"\"\"Return a dictionary with counts of\n    occurrences as value for each key.\"\"\"\n\n    # Initialize an empty dictionary: cols_count\n    cols_count = {}\n\n    # Add try block\n    try:\n        # Extract column from DataFrame: col\n        col = df[col_name]\n        \n        # Iterate over the column in DataFrame\n        for entry in col:\n    \n            # If entry is in cols_count, add 1\n            if entry in cols_count.keys():\n                cols_count[entry] += 1\n            # Else add the entry to cols_count, set the value to 1\n            else:\n                cols_count[entry] = 1\n    \n        # Return the cols_count dictionary\n        return cols_count\n\n    # Add except block\n    except:\n        print('The DataFrame does not have a ' + col_name + ' column.')\n\n# Call count_entries(): result1\nresult1 = count_entries(tweets_df, 'lang')\n\n# Print result1\nprint(result1)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-3",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-3",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Bringing it all together (3)",
    "text": "Bringing it all together (3)\nIn the previous exercise, you built on your function count_entries() to add a try-except block. This was so that users would get helpful messages when calling your count_entries() function and providing a column name that isn’t in the DataFrame. In this exercise, you’ll instead raise a ValueError in the case that the user provides a column name that isn’t in the DataFrame.\nOnce again, for your convenience, pandas has been imported as pd and the ‘tweets.csv’ file has been imported into the DataFrame tweets_df. Parts of the code from your previous work are also provided. ### Instructions - If col_name is not a column in the DataFrame df, raise a ValueError ‘The DataFrame does not have a’ + col_name + ’ column.’. - Call your new function count_entries() to analyze the ‘lang’ column of tweets_df. Store the result in result1. - Print result1. This has been done for you, so hit ‘Submit Answer’ to check out the result. In the next exercise, you’ll see that it raises the necessary ValueErrors.\n# Define count_entries()\ndef count_entries(df, col_name='lang'):\n    \"\"\"Return a dictionary with counts of\n    occurrences as value for each key.\"\"\"\n    \n    # Raise a ValueError if col_name is NOT in DataFrame\n    if col_name not in df.columns:\n        raise ValueError('The DataFrame does not have a ' + col_name + ' column.')\n\n    # Initialize an empty dictionary: cols_count\n    cols_count = {}\n    \n    # Extract column from DataFrame: col\n    col = df[col_name]\n    \n    # Iterate over the column in DataFrame\n    for entry in col:\n\n        # If entry is in cols_count, add 1\n        if entry in cols_count.keys():\n            cols_count[entry] += 1\n            # Else add the entry to cols_count, set the value to 1\n        else:\n            cols_count[entry] = 1\n        \n        # Return the cols_count dictionary\n    return cols_count\n\n# Call count_entries(): result1\nresult1=count_entries(tweets_df,'lang')\n\n# Print result1\nprint(result1)",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-testing-your-error-handling-skills",
    "href": "8_Python_Data_Science_Toolbox_Part1.html#bringing-it-all-together-testing-your-error-handling-skills",
    "title": "Python Data Science Toolbox (Part 1)",
    "section": "Bringing it all together: testing your error handling skills",
    "text": "Bringing it all together: testing your error handling skills\nYou have just written error handling into your count_entries() function so that, when the user passes the function a column (as 2nd argument) NOT contained in the DataFrame (1st argument), a ValueError is thrown. You’re now going to play with this function: it is loaded into pre-exercise code, as is the DataFrame tweets_df. Try calling count_entries(tweets_df, ‘lang’) to confirm that the function behaves as it should. Then call count_entries(tweets_df, ‘lang1’): what is the last line of the output?\ncount_entries(tweets_df, 'lang1')",
    "crumbs": [
      "Python Data Science Toolbox (Part 1)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html",
    "href": "9_Python_Data_Science_Toolbox_Part2.html",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "",
    "text": "Explore Datasets\nUse the DataFrames imported in the first cell to explore the data and practice your skills! - Create a zip object containing the CountryName and CountryCode columns in world_ind. Unpack the resulting zip object and print the tuple values. - Use a list comprehension to extract the first 25 characters of the text column of the tweets DataFrame provided that the tweet is not a retweet (i.e., starts with “RT”). - Create an iterable reader object so that you can use next() to read datasets/world_ind_pop_data.csv in chunks of 20.",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#iterators-vs.-iterables",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#iterators-vs.-iterables",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Iterators vs. Iterables",
    "text": "Iterators vs. Iterables\nLet’s do a quick recall of what you’ve learned about iterables and iterators. Recall from the video that an iterable is an object that can return an iterator, while an iterator is an object that keeps state and produces the next value when you call next() on it.",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#iterating-over-iterables-1",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#iterating-over-iterables-1",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Iterating over iterables (1)",
    "text": "Iterating over iterables (1)\nGreat, you’re familiar with what iterables and iterators are! In this exercise, you will reinforce your knowledge about these by iterating over and printing from iterables and iterators.\nYou are provided with a list of strings flash. You will practice iterating over the list by using a for loop. You will also create an iterator for the list and access the values from the iterator. ### Instructions - Create a for loop to loop over flash and print the values in the list. Use person as the loop variable. - Create an iterator for the list flash and assign the result to superhero. - Print each of the items from superhero using next() 4 times.\n# Create a list of strings: flash\nflash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']\n\n# Print each list item in flash using a for loop\nfor item in flash:\n    print(item)\n\n\n# Create an iterator for flash: superhero\nsuperhero= iter(flash)\n\n# Print each item from the iterator\nprint(next(superhero))\nprint(next(superhero))\nprint(next(superhero))\nprint(next(superhero))",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#iterating-over-iterables-2",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#iterating-over-iterables-2",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Iterating over iterables (2)",
    "text": "Iterating over iterables (2)\nOne of the things you learned about in this chapter is that not all iterables are actual lists. A couple of examples that we looked at are strings and the use of the range() function. In this exercise, we will focus on the range() function.\nYou can use range() in a for loop as if it’s a list to be iterated over:\n`for i in range(5):\nprint(i)`\nRecall that range() doesn’t actually create the list; instead, it creates a range object with an iterator that produces the values until it reaches the limit (in the example, until the value 4). If range() created the actual list, calling it with a value of may not work, especially since a number as big as that may go over a regular computer’s memory. The value\nis actually what’s called a Googol which is a 1 followed by a hundred 0s. That’s a huge number!\nYour task for this exercise is to show that calling range() with\nwon’t actually pre-create the list. ### Instructions - Create an iterator object small_value over range(3) using the function iter(). - Using a for loop, iterate over range(3), printing the value for every iteration. Use num as the loop variable. - Create an iterator object googol over range(10 ** 100).\n# Loop over range(3) and print the values\nfor i in range(3):\n    print(i)\n\n\n# Create an iterator for range(10 ** 100): googol\ngoogol = iter(range(10**100))\n\n# Print the first 5 values from googol\nprint(next(googol))\nprint(next(googol))\nprint(next(googol))\nprint(next(googol))\nprint(next(googol))",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#iterators-as-function-arguments",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#iterators-as-function-arguments",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Iterators as function arguments",
    "text": "Iterators as function arguments\nYou’ve been using the iter() function to get an iterator object, as well as the next() function to retrieve the values one by one from the iterator object.\nThere are also functions that take iterators and iterables as arguments. For example, the list() and sum() functions return a list and the sum of elements, respectively.\nIn this exercise, you will use these functions by passing an iterable from range() and then printing the results of the function calls. ### Instructions - Create a range object that would produce the values from 10 to 20 using range(). Assign the result to values. - Use the list() function to create a list of values from the range object values. Assign the result to values_list. - Use the sum() function to get the sum of the values from 10 to 20 from the range object values. Assign the result to values_sum.\n# Create a range object: values\nvalues = range(10,21)\n\n# Print the range object\nprint(values)\n\n# Create a list of integers: values_list\nvalues_list = list(values)\n\n# Print values_list\nprint(values_list)\n\n# Get the sum of values: values_sum\nvalues_sum = sum(values)\n\n# Print values_sum\nprint(values_sum)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#using-enumerate",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#using-enumerate",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Using enumerate",
    "text": "Using enumerate\nYou’re really getting the hang of using iterators, great job!\nYou’ve just gained several new ideas on iterators from the last video and one of them is the enumerate() function. Recall that enumerate() returns an enumerate object that produces a sequence of tuples, and each of the tuples is an index-value pair.\nIn this exercise, you are given a list of strings mutants and you will practice using enumerate() on it by printing out a list of tuples and unpacking the tuples using a for loop. ### Instructions - Create a list of tuples from mutants and assign the result to mutant_list. Make sure you generate the tuples using enumerate() and turn the result from it into a list using list(). - Complete the first for loop by unpacking the tuples generated by calling enumerate() on mutants. Use index1 for the index and value1 for the value when unpacking the tuple. - Complete the second for loop similarly as with the first, but this time change the starting index to start from 1 by passing it in as an argument to the start parameter of enumerate(). Use index2 for the index and value2 for the value when unpacking the tuple.\n# Create a list of strings: mutants\nmutants = ['charles xavier', \n            'bobby drake', \n            'kurt wagner', \n            'max eisenhardt', \n            'kitty pryde']\n\n# Create a list of tuples: mutant_list\nmutant_list = list(enumerate(mutants))\n\n# Print the list of tuples\nprint(mutant_list)\n\n# Unpack and print the tuple pairs\nfor index1, value1 in enumerate(mutants):\n    print(index1, value1)\n\n# Change the start index\nfor index2, value2 in enumerate(mutants, start=1):\n    print(index2, value2)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#using-zip",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#using-zip",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Using zip",
    "text": "Using zip\nAnother interesting function that you’ve learned is zip(), which takes any number of iterables and returns a zip object that is an iterator of tuples. If you wanted to print the values of a zip object, you can convert it into a list and then print it. Printing just a zip object will not return the values unless you unpack it first. In this exercise, you will explore this for yourself.\nThree lists of strings are pre-loaded: mutants, aliases, and powers. First, you will use list() and zip() on these lists to generate a list of tuples. Then, you will create a zip object using zip(). Finally, you will unpack this zip object in a for loop to print the values in each tuple. Observe the different output generated by printing the list of tuples, then the zip object, and finally, the tuple values in the for loop. ### Instructions - Using zip() with list(), create a list of tuples from the three lists mutants, aliases, and powers (in that order) and assign the result to mutant_data. - Using zip(), create a zip object called mutant_zip from the three lists mutants, aliases, and powers. - Complete the for loop by unpacking the zip object you created and printing the tuple values. Use value1, value2, value3 for the values from each of mutants, aliases, and powers, in that order.\nmutants =['charles xavier', 'bobby drake', 'kurt wagner', 'max eisenhardt', 'kitty pryde']\naliases = ['prof x', 'iceman', 'nightcrawler', 'magneto', 'shadowcat']\npowers = ['telepathy',  'thermokinesis', 'teleportation', 'magnetokinesis', 'intangibility']\n# Create a list of tuples: mutant_data\nmutant_data = list(zip(mutants, aliases, powers ))\n\n# Print the list of tuples\nprint(mutant_data)\n\n# Create a zip object using the three lists: mutant_zip\nmutant_zip = zip(mutants, aliases, powers )\n\n# Print the zip object\nprint(mutant_zip)\n\n# Unpack the zip object and print the tuple values\nfor value1,value2,value3 in mutant_zip:\n    print(value1, value2, value3)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#using-and-zip-to-unzip",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#using-and-zip-to-unzip",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Using * and zip to ‘unzip’",
    "text": "Using * and zip to ‘unzip’\nYou know how to use zip() as well as how to print out values from a zip object. Excellent!\nLet’s play around with zip() a little more. There is no unzip function for doing the reverse of what zip() does. We can, however, reverse what has been zipped together by using zip() with a little help from ! unpacks an iterable such as a list or a tuple into positional arguments in a function call.\nIn this exercise, you will use * in a call to zip() to unpack the tuples produced by zip().\nTwo tuples of strings, mutants and powers have been pre-loaded. ### Instructions - Create a zip object by using zip() on mutants and powers, in that order. Assign the result to z1. - Print the tuples in z1 by unpacking them into positional arguments using the * operator in a print() call. - Because the previous print() call would have exhausted the elements in z1, recreate the zip object you defined earlier and assign the result again to z1. - ‘Unzip’ the tuples in z1 by unpacking them into positional arguments using the * operator in a zip() call. Assign the results to result1 and result2, in that order. - The last print() statements prints the output of comparing result1 to mutants and result2 to powers. Click Submit Answer to see if the unpacked result1 and result2 are equivalent to mutants and powers, respectively.\nmutants = ('charles xavier', 'bobby drake', 'kurt wagner', 'max eisenhardt', 'kitty pryde')\n\npowers = ('telepathy', 'thermokinesis', 'teleportation', 'magnetokinesis', 'intangibility')\n# Create a zip object from mutants and powers: z1\nz1 = zip(mutants,powers)\n\n# Print the tuples in z1 by unpacking with *\nprint(*z1)\n\n# Re-create a zip object from mutants and powers: z1\nz1 = zip(mutants,powers)\n\n# 'Unzip' the tuples in z1 by unpacking with * and zip(): result1, result2\nresult1, result2 = zip(*z1)\n\n# Check if unpacked tuples are equivalent to original tuples\nprint(result1 == mutants)\nprint(result2 == powers)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#processing-large-amounts-of-twitter-data",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#processing-large-amounts-of-twitter-data",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Processing large amounts of Twitter data",
    "text": "Processing large amounts of Twitter data\nSometimes, the data we have to process reaches a size that is too much for a computer’s memory to handle. This is a common problem faced by data scientists. A solution to this is to process an entire data source chunk by chunk, instead of a single go all at once.\nIn this exercise, you will do just that. You will process a large csv file of Twitter data in the same way that you processed ‘tweets.csv’ in Bringing it all together exercises of the prequel course, but this time, working on it in chunks of 10 entries at a time.\nIf you are interested in learning how to access Twitter data so you can work with it on your own system, refer to Part 2 of the DataCamp course on Importing Data in Python.\nThe pandas package has been imported as pd and the file ‘tweets.csv’ is in your current directory for your use.\nBe aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data). ### Instructions - Initialize an empty dictionary counts_dict for storing the results of processing the Twitter data. - Iterate over the ‘tweets.csv’ file by using a for loop. Use the loop variable chunk and iterate over the call to pd.read_csv() with a chunksize of 10. - In the inner loop, iterate over the column ‘lang’ in chunk by using a for loop. Use the loop variable entry.\n# Initialize an empty dictionary: counts_dict\ncounts_dict={}\n\n# Iterate over the file chunk by chunk\nfor chunk in pd.read_csv('datasets/tweets.csv',chunksize=10):\n\n    # Iterate over the column in DataFrame\n    for entry in chunk.lang:\n        if entry in counts_dict.keys():\n            counts_dict[entry] += 1\n        else:\n            counts_dict[entry] = 1\n\n# Print the populated dictionary\nprint(counts_dict)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#extracting-information-for-large-amounts-of-twitter-data",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#extracting-information-for-large-amounts-of-twitter-data",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Extracting information for large amounts of Twitter data",
    "text": "Extracting information for large amounts of Twitter data\nGreat job chunking out that file in the previous exercise. You now know how to deal with situations where you need to process a very large file and that’s a very useful skill to have!\nIt’s good to know how to process a file in smaller, more manageable chunks, but it can become very tedious having to write and rewrite the same code for the same task each time. In this exercise, you will be making your code more reusable by putting your work in the last exercise in a function definition.\nThe pandas package has been imported as pd and the file ‘tweets.csv’ is in your current directory for your use. ### Instructions - Define the function count_entries(), which has 3 parameters. The first parameter is csv_file for the filename, the second is c_size for the chunk size, and the last is colname for the column name. - Iterate over the file in csv_file file by using a for loop. Use the loop variable chunk and iterate over the call to pd.read_csv(), passing c_size to chunksize. - In the inner loop, iterate over the column given by colname in chunk by using a for loop. Use the loop variable entry. - Call the count_entries() function by passing to it the filename ‘tweets.csv’, the size of chunks 10, and the name of the column to count, ‘lang’. Assign the result of the call to the variable result_counts.\n# Define count_entries()\ndef count_entries(csv_file,c_size,colname):\n    \"\"\"Return a dictionary with counts of\n    occurrences as value for each key.\"\"\"\n    \n    # Initialize an empty dictionary: counts_dict\n    counts_dict = {}\n\n    # Iterate over the file chunk by chunk\n    for chunk in pd.read_csv(csv_file,chunksize=c_size):\n\n        # Iterate over the column in DataFrame\n        for entry in chunk[colname]:\n            if entry in counts_dict.keys():\n                counts_dict[entry] += 1\n            else:\n                counts_dict[entry] = 1\n\n    # Return counts_dict\n    return counts_dict\n\n# Call count_entries(): result_counts\nresult_counts = count_entries('datasets/tweets.csv',10,'lang')\n\n# Print result_counts\nprint(result_counts)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#write-a-basic-list-comprehension",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#write-a-basic-list-comprehension",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Write a basic list comprehension",
    "text": "Write a basic list comprehension\nIn this exercise, you will practice what you’ve learned from the video about writing list comprehensions. You will write a list comprehension and identify the output that will be produced.\nThe following list has been pre-loaded in the environment.\ndoctor = ['house', 'cuddy', 'chase', 'thirteen', 'wilson']\nHow would a list comprehension that produces a list of the first character of each string in doctor look like? Note that the list comprehension uses doc as the iterator variable. What will the output be?\ndoctor = ['house', 'cuddy', 'chase', 'thirteen', 'wilson']\n[doc[0] for doc in doctor]",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#list-comprehension-over-iterables",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#list-comprehension-over-iterables",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "List comprehension over iterables",
    "text": "List comprehension over iterables\nYou know that list comprehensions can be built over iterables. Given the following objects below, which of these can we build list comprehensions over?\ndoctor = ['house', 'cuddy', 'chase', 'thirteen', 'wilson']\nrange(50)\nunderwood = 'After all, we are nothing more or less than what we choose to reveal.'\njean = '24601'\nflash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']\nvaljean = 24601\n\nAnswer:\nAll but valjean = 24601",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-list-comprehensions",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-list-comprehensions",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing list comprehensions",
    "text": "Writing list comprehensions\nYou now have all the knowledge necessary to begin writing list comprehensions! Your job in this exercise is to write a list comprehension that produces a list of the squares of the numbers ranging from 0 to 9. ### Instructions - Using the range of numbers from 0 to 9 as your iterable and i as your iterator variable, write a list comprehension that produces a list of numbers consisting of the squared values of i.\n# Create list comprehension: squares\nsquares = [i**2 for i in range(10)]\nprint(squares)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#nested-list-comprehensions",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#nested-list-comprehensions",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Nested list comprehensions",
    "text": "Nested list comprehensions\nGreat! At this point, you have a good grasp of the basic syntax of list comprehensions. Let’s push your code-writing skills a little further. In this exercise, you will be writing a list comprehension within another list comprehension, or nested list comprehensions. It sounds a little tricky, but you can do it!\nLet’s step aside for a while from strings. One of the ways in which lists can be used are in representing multi-dimension objects such as matrices. Matrices can be represented as a list of lists in Python. For example a 5 x 5 matrix with values 0 to 4 in each row can be written as:\nmatrix = [[0, 1, 2, 3, 4],           [0, 1, 2, 3, 4],           [0, 1, 2, 3, 4],           [0, 1, 2, 3, 4],           [0, 1, 2, 3, 4]]\nYour task is to recreate this matrix by using nested listed comprehensions. Recall that you can create one of the rows of the matrix with a single list comprehension. To create the list of lists, you simply have to supply the list comprehension as the output expression of the overall list comprehension:\n[[output expression] for iterator variable in iterable]\nNote that here, the output expression is itself a list comprehension. ### Instructions - In the inner list comprehension - that is, the output expression of the nested list comprehension - create a list of values from 0 to 4 using range(). Use col as the iterator variable. - In the iterable part of your nested list comprehension, use range() to count 5 rows - that is, create a list of values from 0 to 4. Use row as the iterator variable; note that you won’t be needing this variable to create values in the list of lists.\n# Create a 5 x 5 matrix using a list of lists: matrix\nmatrix = [[col for col in range(0,5)] for row in range(0,5)]\n\n# Print the matrix\nfor row in matrix:\n    print(row)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#using-conditionals-in-comprehensions-1",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#using-conditionals-in-comprehensions-1",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Using conditionals in comprehensions (1)",
    "text": "Using conditionals in comprehensions (1)\nYou’ve been using list comprehensions to build lists of values, sometimes using operations to create these values.\nAn interesting mechanism in list comprehensions is that you can also create lists with values that meet only a certain condition. One way of doing this is by using conditionals on iterator variables. In this exercise, you will do exactly that!\nRecall from the video that you can apply a conditional statement to test the iterator variable by adding an if statement in the optional predicate expression part after the for statement in the comprehension:\n[ output expression for iterator variable in iterable if predicate expression ]. You will use this recipe to write a list comprehension for this exercise. You are given a list of strings fellowship and, using a list comprehension, you will create a list that only includes the members of fellowship that have 7 characters or more. ### Instructions - Use member as the iterator variable in the list comprehension. For the conditional, use len() to evaluate the iterator variable. Note that you only want strings with 7 characters or more.\n# Create a list of strings: fellowship\nfellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n\n# Create list comprehension: new_fellowship\nnew_fellowship = [member for member in fellowship if len(member)&gt;6]\n\n# Print the new list\nprint(new_fellowship)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#using-conditionals-in-comprehensions-2",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#using-conditionals-in-comprehensions-2",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Using conditionals in comprehensions (2)",
    "text": "Using conditionals in comprehensions (2)\nIn the previous exercise, you used an if conditional statement in the predicate expression part of a list comprehension to evaluate an iterator variable. In this exercise, you will use an if-else statement on the output expression of the list.\nYou will work on the same list, fellowship and, using a list comprehension and an if-else conditional statement in the output expression, create a list that keeps members of fellowship with 7 or more characters and replaces others with an empty string. Use member as the iterator variable in the list comprehension. ### Instructions - In the output expression, keep the string as-is if the number of characters is &gt;= 7, else replace it with an empty string - that is, ’’ or ““.\n# Create a list of strings: fellowship\nfellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n\n# Create list comprehension: new_fellowship\nnew_fellowship = [member if len(member) &gt;= 7 else '' for member in fellowship]\n\n# Print the new list\nprint(new_fellowship)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#dict-comprehensions",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#dict-comprehensions",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Dict comprehensions",
    "text": "Dict comprehensions\nComprehensions aren’t relegated merely to the world of lists. There are many other objects you can build using comprehensions, such as dictionaries, pervasive objects in Data Science. You will create a dictionary using the comprehension syntax for this exercise. In this case, the comprehension is called a dict comprehension.\nRecall that the main difference between a list comprehension and a dict comprehension is the use of curly braces {} instead of []. Additionally, members of the dictionary are created using a colon :, as in  : .\nYou are given a list of strings fellowship and, using a dict comprehension, create a dictionary with the members of the list as the keys and the length of each string as the corresponding values. ### Instructions\nCreate a dict comprehension where the key is a string in fellowship and the value is the length of the string. Remember to use the syntax  :  in the output expression part of the comprehension to create the members of the dictionary. Use member as the iterator variable.\n# Create a list of strings: fellowship\nfellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n\n# Create dict comprehension: new_fellowship\nnew_fellowship = {key:len(key) for key in fellowship}\n\n# Print the new dictionary\nprint(new_fellowship)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#list-comprehensions-vs.-generators",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#list-comprehensions-vs.-generators",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "List comprehensions vs. generators",
    "text": "List comprehensions vs. generators\nYou’ve seen from the videos that list comprehensions and generator expressions look very similar in their syntax, except for the use of parentheses () in generator expressions and brackets [] in list comprehensions.\nIn this exercise, you will recall the difference between list comprehensions and generators. To help with that task, the following code has been pre-loaded in the environment:\n# List of strings fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n# List comprehension fellow1 = [member for member in fellowship if len(member) &gt;= 7]\n# Generator expression fellow2 = (member for member in fellowship if len(member) &gt;= 7)\nTry to play around with fellow1 and fellow2 by figuring out their types and printing out their values. Based on your observations and what you can recall from the video, select from the options below the best description for the difference between list comprehensions and generators.\n# List of strings\nfellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n\n# List comprehension\nfellow1 = [member for member in fellowship if len(member) &gt;= 7]\n\n# Generator expression\nfellow2 = (member for member in fellowship if len(member) &gt;= 7)\n\nprint(fellow1)\nprint(fellow2)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#write-your-own-generator-expressions",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#write-your-own-generator-expressions",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Write your own generator expressions",
    "text": "Write your own generator expressions\nYou are familiar with what generators and generator expressions are, as well as its difference from list comprehensions. In this exercise, you will practice building generator expressions on your own.\nRecall that generator expressions basically have the same syntax as list comprehensions, except that it uses parentheses () instead of brackets []; this should make things feel familiar! Furthermore, if you have ever iterated over a dictionary with .items(), or used the range() function, for example, you have already encountered and used generators before, without knowing it! When you use these functions, Python creates generators for you behind the scenes.\nNow, you will start simple by creating a generator object that produces numeric values. ### Instructions - Create a generator object that will produce values from 0 to 30. Assign the result to result and use num as the iterator variable in the generator expression. - Print the first 5 values by using next() appropriately in print(). - Print the rest of the values by using a for loop to iterate over the generator object.\n# Create generator object: result\nresult = (num for num in range(0,31) )\n\n# Print the first 5 values\nprint(next(result))\nprint(next(result))\nprint(next(result))\nprint(next(result))\nprint(next(result))\n\n# Print the rest of the values\nfor value in result:\n    print(value)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#changing-the-output-in-generator-expressions",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#changing-the-output-in-generator-expressions",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Changing the output in generator expressions",
    "text": "Changing the output in generator expressions\nGreat! At this point, you already know how to write a basic generator expression. In this exercise, you will push this idea a little further by adding to the output expression of a generator expression. Because generator expressions and list comprehensions are so alike in syntax, this should be a familiar task for you!\nYou are given a list of strings lannister and, using a generator expression, create a generator object that you will iterate over to print its values. ### Instructions - Write a generator expression that will generate the lengths of each string in lannister. Use person as the iterator variable. Assign the result to lengths. - Supply the correct iterable in the for loop for printing the values in the generator object.\n# Create a list of strings: lannister\nlannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']\n\n# Create a generator object: lengths\nlengths = (len(person) for person in lannister  )\n\n# Iterate over and print the values in lengths\nfor value in lengths:\n    print(value)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#build-a-generator",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#build-a-generator",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Build a generator",
    "text": "Build a generator\nIn previous exercises, you’ve dealt mainly with writing generator expressions, which uses comprehension syntax. Being able to use comprehension syntax for generator expressions made your work so much easier!\nNow, recall from the video that not only are there generator expressions, there are generator functions as well. Generator functions are functions that, like generator expressions, yield a series of values, instead of returning a single value. A generator function is defined as you do a regular function, but whenever it generates a value, it uses the keyword yield instead of return.\nIn this exercise, you will create a generator function with a similar mechanism as the generator expression you defined in the previous exercise:\nlengths = (len(person) for person in lannister)\n\nInstructions\n\nComplete the function header for the function get_lengths() that has a single parameter, input_list.\nIn the for loop in the function definition, yield the length of the strings in input_list.\nComplete the iterable part of the for loop for printing the values generated by the get_lengths() generator function. Supply the call to get_lengths(), passing in the list lannister.\n\n# Create a list of strings\nlannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']\n\n# Define generator function get_lengths\ndef get_lengths(input_list):\n    \"\"\"Generator function that yields the\n    length of the strings in input_list.\"\"\"\n\n    # Yield the length of a string\n    for person in input_list:\n        yield len(person)\n\n# Print the values generated by get_lengths()\nfor value in get_lengths(lannister):\n    print(value)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#list-comprehensions-for-time-stamped-data",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#list-comprehensions-for-time-stamped-data",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "List comprehensions for time-stamped data",
    "text": "List comprehensions for time-stamped data\nYou will now make use of what you’ve learned from this chapter to solve a simple data extraction problem. You will also be introduced to a data structure, the pandas Series, in this exercise. We won’t elaborate on it much here, but what you should know is that it is a data structure that you will be working with a lot of times when analyzing data from pandas DataFrames. You can think of DataFrame columns as single-dimension arrays called Series.\nIn this exercise, you will be using a list comprehension to extract the time from time-stamped Twitter data. The pandas package has been imported as pd and the file ‘tweets.csv’ has been imported as the df DataFrame for your use. ### Instructions - Extract the column ‘created_at’ from df and assign the result to tweet_time. Fun fact: the extracted column in tweet_time here is a Series data structure! - Create a list comprehension that extracts the time from each row in tweet_time. Each row is a string that represents a timestamp, and you will access the 12th to 19th characters in the string to extract the time. Use entry as the iterator variable and assign the result to tweet_clock_time. Remember that Python uses 0-based indexing!\nimport pandas as pd\ndf = pd.read_csv('datasets/tweets.csv')\n# Extract the created_at column from df: tweet_time\ntweet_time = df['created_at']\n\n# Extract the clock time: tweet_clock_time\ntweet_clock_time = [entry[11:19] for entry in tweet_time]\n\n# Print the extracted times\nprint(tweet_clock_time)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#conditional-list-comprehensions-for-time-stamped-data",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#conditional-list-comprehensions-for-time-stamped-data",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Conditional list comprehensions for time-stamped data",
    "text": "Conditional list comprehensions for time-stamped data\nGreat, you’ve successfully extracted the data of interest, the time, from a pandas DataFrame! Let’s tweak your work further by adding a conditional that further specifies which entries to select.\nIn this exercise, you will be using a list comprehension to extract the time from time-stamped Twitter data. You will add a conditional expression to the list comprehension so that you only select the times in which entry[17:19] is equal to ‘19’. The pandas package has been imported as pd and the file ‘tweets.csv’ has been imported as the df DataFrame for your use. ### Instructions - Extract the column ‘created_at’ from df and assign the result to tweet_time. - Create a list comprehension that extracts the time from each row in tweet_time. Each row is a string that represents a timestamp, and you will access the 12th to 19th characters in the string to extract the time. Use entry as the iterator variable and assign the result to tweet_clock_time. Additionally, add a conditional expression that checks whether entry[17:19] is equal to ‘19’.\n# Extract the created_at column from df: tweet_time\ntweet_time = df['created_at']\n\n# Extract the clock time: tweet_clock_time\ntweet_clock_time = [entry[11:19] for entry in tweet_time if entry[17:19] == '19']\n\n# Print the extracted times\nprint(tweet_clock_time)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#dictionaries-for-data-science",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#dictionaries-for-data-science",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Dictionaries for data science",
    "text": "Dictionaries for data science\nFor this exercise, you’ll use what you’ve learned about the zip() function and combine two lists into a dictionary.\nThese lists are actually extracted from a bigger dataset file of world development indicators from the World Bank. For pedagogical purposes, we have pre-processed this dataset into the lists that you’ll be working with.\nThe first list feature_names contains header names of the dataset and the second list row_vals contains actual values of a row from the dataset, corresponding to each of the header names. ### Instructions - Create a zip object by calling zip() and passing to it feature_names and row_vals. Assign the result to zipped_lists. - Create a dictionary from the zipped_lists zip object by calling dict() with zipped_lists. Assign the resulting dictionary to rs_dict.\nfeature_names = ['CountryName', 'CountryCode', 'IndicatorName', 'IndicatorCode', 'Year', 'Value']\nrow_vals = ['Arab World', 'ARB', 'Adolescent fertility rate (births per 1,000 women ages 15-19)',\n 'SP.ADO.TFRT',  '1960', '133.56090740552298']\n# Zip lists: zipped_lists\nzipped_lists = zip(feature_names,row_vals)\n\n# Create a dictionary: rs_dict\nrs_dict = dict(zipped_lists)\n\n# Print the dictionary\nprint(rs_dict)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-a-function-to-help-you",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-a-function-to-help-you",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing a function to help you",
    "text": "Writing a function to help you\nSuppose you needed to repeat the same process done in the previous exercise to many, many rows of data. Rewriting your code again and again could become very tedious, repetitive, and unmaintainable.\nIn this exercise, you will create a function to house the code you wrote earlier to make things easier and much more concise. Why? This way, you only need to call the function and supply the appropriate lists to create your dictionaries! Again, the lists feature_names and row_vals are preloaded and these contain the header names of the dataset and actual values of a row from the dataset, respectively. ### Instructions - Define the function lists2dict() with two parameters: first is list1 and second is list2. - Return the resulting dictionary rs_dict in lists2dict(). - Call the lists2dict() function with the arguments feature_names and row_vals. Assign the result of the function call to rs_fxn.\n# Define lists2dict()\ndef lists2dict(list1, list2):\n    \"\"\"Return a dictionary where list1 provides\n    the keys and list2 provides the values.\"\"\"\n\n    # Zip lists: zipped_lists\n    zipped_lists = zip(list1, list2)\n\n    # Create a dictionary: rs_dict\n    rs_dict = dict(zipped_lists)\n\n    # Return the dictionary\n    return rs_dict\n\n# Call lists2dict: rs_fxn\nrs_fxn = lists2dict(feature_names,row_vals)\n\n# Print rs_fxn\nprint(rs_fxn)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#using-a-list-comprehension",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#using-a-list-comprehension",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Using a list comprehension",
    "text": "Using a list comprehension\nThis time, you’re going to use the lists2dict() function you defined in the last exercise to turn a bunch of lists into a list of dictionaries with the help of a list comprehension.\nThe lists2dict() function has already been preloaded, together with a couple of lists, feature_names and row_lists. feature_names contains the header names of the World Bank dataset and row_lists is a list of lists, where each sublist is a list of actual values of a row from the dataset.\nYour goal is to use a list comprehension to generate a list of dicts, where the keys are the header names and the values are the row entries. ### Instructions - Inspect the contents of row_lists by printing the first two lists in row_lists. - Create a list comprehension that generates a dictionary using lists2dict() for each sublist in row_lists. The keys are from the feature_names list and the values are the row entries in row_lists. Use sublist as your iterator variable and assign the resulting list of dictionaries to list_of_dicts. - Look at the first two dictionaries in list_of_dicts by printing them out.\nfeature_names = ['CountryName', 'CountryCode', 'IndicatorName', 'IndicatorCode', 'Year', 'Value']\nrow_lists = [['Arab World', \n  'ARB',\n  'Adolescent fertility rate (births per 1,000 women ages 15-19)',\n  'SP.ADO.TFRT',\n  '1960',\n  '133.56090740552298'],\n ['Arab World',\n  'ARB',\n  'Age dependency ratio (% of working-age population)',\n  'SP.POP.DPND',\n  '1960',\n  '87.7976011532547'],\n ['Arab World',\n  'ARB',\n  'Age dependency ratio, old (% of working-age population)',\n  'SP.POP.DPND.OL',\n  '1960',\n  '6.634579191565161'],\n ['Arab World',\n  'ARB',\n  'Age dependency ratio, young (% of working-age population)',\n  'SP.POP.DPND.YG',\n  '1960',\n  '81.02332950839141'],\n ['Arab World',\n  'ARB',\n  'Arms exports (SIPRI trend indicator values)',\n  'MS.MIL.XPRT.KD',\n  '1960',\n  '3000000.0'],\n ['Arab World',\n  'ARB',\n  'Arms imports (SIPRI trend indicator values)',\n  'MS.MIL.MPRT.KD',\n  '1960',\n  '538000000.0'],\n ['Arab World',\n  'ARB',\n  'Birth rate, crude (per 1,000 people)',\n  'SP.DYN.CBRT.IN',\n  '1960',\n  '47.697888095096395'],\n ['Arab World',\n  'ARB',\n  'CO2 emissions (kt)',\n  'EN.ATM.CO2E.KT',\n  '1960',\n  '59563.9892169935'],\n ['Arab World',\n  'ARB',\n  'CO2 emissions (metric tons per capita)',\n  'EN.ATM.CO2E.PC',\n  '1960',\n  '0.6439635478877049'],\n ['Arab World',\n  'ARB',\n  'CO2 emissions from gaseous fuel consumption (% of total)',\n  'EN.ATM.CO2E.GF.ZS',\n  '1960',\n  '5.041291753975099'],\n ['Arab World',\n  'ARB',\n  'CO2 emissions from liquid fuel consumption (% of total)',\n  'EN.ATM.CO2E.LF.ZS',\n  '1960',\n  '84.8514729446567'],\n ['Arab World',\n  'ARB',\n  'CO2 emissions from liquid fuel consumption (kt)',\n  'EN.ATM.CO2E.LF.KT',\n  '1960',\n  '49541.707291032304'],\n ['Arab World',\n  'ARB',\n  'CO2 emissions from solid fuel consumption (% of total)',\n  'EN.ATM.CO2E.SF.ZS',\n  '1960',\n  '4.72698138789597'],\n ['Arab World',\n  'ARB',\n  'Death rate, crude (per 1,000 people)',\n  'SP.DYN.CDRT.IN',\n  '1960',\n  '19.7544519237187'],\n ['Arab World',\n  'ARB',\n  'Fertility rate, total (births per woman)',\n  'SP.DYN.TFRT.IN',\n  '1960',\n  '6.92402738655897'],\n ['Arab World',\n  'ARB',\n  'Fixed telephone subscriptions',\n  'IT.MLT.MAIN',\n  '1960',\n  '406833.0'],\n ['Arab World',\n  'ARB',\n  'Fixed telephone subscriptions (per 100 people)',\n  'IT.MLT.MAIN.P2',\n  '1960',\n  '0.6167005703199'],\n ['Arab World',\n  'ARB',\n  'Hospital beds (per 1,000 people)',\n  'SH.MED.BEDS.ZS',\n  '1960',\n  '1.9296220724398703'],\n ['Arab World',\n  'ARB',\n  'International migrant stock (% of population)',\n  'SM.POP.TOTL.ZS',\n  '1960',\n  '2.9906371279862403'],\n ['Arab World',\n  'ARB',\n  'International migrant stock, total',\n  'SM.POP.TOTL',\n  '1960',\n  '3324685.0']]\n# Print the first two lists in row_lists\nprint(row_lists[0])\nprint(row_lists[1])\n\n# Turn list of lists into list of dicts: list_of_dicts\nlist_of_dicts = [lists2dict(feature_names, sublist) for  sublist  in  row_lists]\n\n# Print the first two dictionaries in list_of_dicts\nprint(list_of_dicts[0])\nprint(list_of_dicts[1])",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#turning-this-all-into-a-dataframe",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#turning-this-all-into-a-dataframe",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Turning this all into a DataFrame",
    "text": "Turning this all into a DataFrame\nYou’ve zipped lists together, created a function to house your code, and even used the function in a list comprehension to generate a list of dictionaries. That was a lot of work and you did a great job!\nYou will now use all of these to convert the list of dictionaries into a pandas DataFrame. You will see how convenient it is to generate a DataFrame from dictionaries with the DataFrame() function from the pandas package.\nThe lists2dict() function, feature_names list, and row_lists list have been preloaded for this exercise.\nGo for it! ### Instructions - To use the DataFrame() function you need, first import the pandas package with the alias pd. - Create a DataFrame from the list of dictionaries in list_of_dicts by calling pd.DataFrame(). Assign the resulting DataFrame to df. - Inspect the contents of df printing the head of the DataFrame. Head of the DataFrame df can be accessed by calling df.head().\n# Import the pandas package\nimport pandas as pd\n\n# Turn list of lists into list of dicts: list_of_dicts\nlist_of_dicts = [lists2dict(feature_names, sublist) for sublist in row_lists]\n\n# Turn list of dicts into a DataFrame: df\ndf = pd.DataFrame(list_of_dicts)\n\n# Print the head of the DataFrame\n\nprint(df.head())",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#processing-data-in-chunks-1",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#processing-data-in-chunks-1",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Processing data in chunks (1)",
    "text": "Processing data in chunks (1)\nSometimes, data sources can be so large in size that storing the entire dataset in memory becomes too resource-intensive. In this exercise, you will process the first 1000 rows of a file line by line, to create a dictionary of the counts of how many times each country appears in a column in the dataset.\nThe csv file ‘world_dev_ind.csv’ is in your current directory for your use. To begin, you need to open a connection to this file using what is known as a context manager. For example, the command with open(‘datacamp.csv’) as datacamp binds the csv file ‘datacamp.csv’ as datacamp in the context manager. Here, the with statement is the context manager, and its purpose is to ensure that resources are efficiently allocated when opening a connection to a file.\nIf you’d like to learn more about context managers, refer to the DataCamp course on Importing Data in Python. ### Instructions - Use open() to bind the csv file ‘world_dev_ind.csv’ as file in the context manager. - Complete the for loop so that it iterates 1000 times to perform the loop body and process only the first 1000 rows of data of the file.\nimport pandas as pd\n# Open a connection to the file\nwith open(\"datasets/world_ind_pop_data.csv\") as file:\n\n    # Skip the column names\n    file.readline()\n\n    # Initialize an empty dictionary: counts_dict\n    counts_dict = {}\n\n    # Process only the first 1000 rows\n    for j in range(0,1000):\n\n        # Split the current line into a list: line\n        line = file.readline().split(',')\n\n        # Get the value for the first column: first_col\n        first_col = line[0]\n\n        # If the column value is in the dict, increment its value\n        if first_col in counts_dict.keys():\n            counts_dict[first_col] += 1\n\n        # Else, add to the dict and set value to 1\n        else:\n            counts_dict[first_col] = 1\n\n# Print the resulting dictionary\nprint(counts_dict)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-a-generator-to-load-data-in-chunks-2",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-a-generator-to-load-data-in-chunks-2",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing a generator to load data in chunks (2)",
    "text": "Writing a generator to load data in chunks (2)\nIn the previous exercise, you processed a file line by line for a given number of lines. What if, however, you want to do this for the entire file?\nIn this case, it would be useful to use generators. Generators allow users to lazily evaluate data. This concept of lazy evaluation is useful when you have to deal with very large datasets because it lets you generate values in an efficient manner by yielding only chunks of data at a time instead of the whole thing at once.\nIn this exercise, you will define a generator function read_large_file() that produces a generator object which yields a single line from a file each time next() is called on it. The csv file ‘world_dev_ind.csv’ is in your current directory for your use.\nNote that when you open a connection to a file, the resulting file object is already a generator! So out in the wild, you won’t have to explicitly create generator objects in cases such as this. However, for pedagogical reasons, we are having you practice how to do this here with the read_large_file() function. Go for it! ### Instructions - In the function read_large_file(), read a line from file_object by using the method readline(). Assign the result to data. - In the function read_large_file(), yield the line read from the file data. - In the context manager, create a generator object gen_file by calling your generator function read_large_file() and passing file to it. - Print the first three lines produced by the generator object gen_file using next().\n# Define read_large_file()\ndef read_large_file(file_object):\n    \"\"\"A generator function to read a large file lazily.\"\"\"\n\n    # Loop indefinitely until the end of the file\n    while True:\n\n        # Read a line from the file: data\n        data = file_object.readline()\n\n        # Break if this is the end of the file\n        if not data:\n            break\n\n        # Yield the line of data\n        yield data\n        \n# Open a connection to the file\nwith open('datasets/world_ind_pop_data.csv') as file:\n\n    # Create a generator object for the file: gen_file\n    gen_file = read_large_file(file)\n\n    # Print the first three lines of the file\n    print(next(gen_file))\n    print(next(gen_file))\n    print(next(gen_file))",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-a-generator-to-load-data-in-chunks-3",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-a-generator-to-load-data-in-chunks-3",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing a generator to load data in chunks (3)",
    "text": "Writing a generator to load data in chunks (3)\nGreat! You’ve just created a generator function that you can use to help you process large files.\nNow let’s use your generator function to process the World Bank dataset like you did previously. You will process the file line by line, to create a dictionary of the counts of how many times each country appears in a column in the dataset. For this exercise, however, you won’t process just 1000 rows of data, you’ll process the entire dataset!\nThe generator function read_large_file() and the csv file ‘world_dev_ind.csv’ are preloaded and ready for your use. Go for it! ### Instructions - Bind the file ‘world_dev_ind.csv’ to file in the context manager with open(). - Complete the for loop so that it iterates over the generator from the call to read_large_file() to process all the rows of the file.\n# Initialize an empty dictionary: counts_dict\ncounts_dict = {}\n\n# Open a connection to the file\nwith open('datasets/world_ind_pop_data.csv') as file:\n\n    # Iterate over the generator from read_large_file()\n    for line in read_large_file(file):\n\n        row = line.split(',')\n        first_col = row[0]\n\n        if first_col in counts_dict.keys():\n            counts_dict[first_col] += 1\n        else:\n            counts_dict[first_col] = 1\n\n# Print            \nprint(counts_dict)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-1",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-1",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing an iterator to load data in chunks (1)",
    "text": "Writing an iterator to load data in chunks (1)\nAnother way to read data too large to store in memory in chunks is to read the file in as DataFrames of a certain length, say, 100. For example, with the pandas package (imported as pd), you can do pd.read_csv(filename, chunksize=100). This creates an iterable reader object, which means that you can use next() on it.\nIn this exercise, you will read a file in small DataFrame chunks with read_csv(). You’re going to use the World Bank Indicators data ‘ind_pop.csv’, available in your current directory, to look at the urban population indicator for numerous countries and years. ### Instructions - Use pd.read_csv() to read in ‘ind_pop.csv’ in chunks of size 10. Assign the result to df_reader. - Print the first two chunks from df_reader.\n# Import the pandas package\nimport pandas as pd\n\n# Initialize reader object: df_reader\ndf_reader = pd.read_csv('datasets/world_ind_pop_data.csv', chunksize=5)\n\n# Print two chunks\nprint(next(df_reader))\nprint(next(df_reader))",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-2",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-2",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing an iterator to load data in chunks (2)",
    "text": "Writing an iterator to load data in chunks (2)\nIn the previous exercise, you used read_csv() to read in DataFrame chunks from a large dataset. In this exercise, you will read in a file using a bigger DataFrame chunk size and then process the data from the first chunk.\nTo process the data, you will create another DataFrame composed of only the rows from a specific country. You will then zip together two of the columns from the new DataFrame, ‘Total Population’ and ‘Urban population (% of total)’. Finally, you will create a list of tuples from the zip object, where each tuple is composed of a value from each of the two columns mentioned.\nYou’re going to use the data from ‘ind_pop_data.csv’, available in your current directory. pandas has been imported as pd. ### Instructions - Use pd.read_csv() to read in the file in ‘ind_pop_data.csv’ in chunks of size 1000. Assign the result to urb_pop_reader. - Get the first DataFrame chunk from the iterable urb_pop_reader and assign this to df_urb_pop. - Select only the rows of df_urb_pop that have a ‘CountryCode’ of ‘CEB’. To do this, compare whether df_urb_pop[‘CountryCode’] is equal to ‘CEB’ within the square brackets in df_urb_pop[____]. - Using zip(), zip together the ‘Total Population’ and ‘Urban population (% of total)’ columns of df_pop_ceb. Assign the resulting zip object to pops.\n# Initialize reader object: urb_pop_reader\nurb_pop_reader = pd.read_csv('datasets/world_ind_pop_data.csv', chunksize=1000)\n\n# Get the first DataFrame chunk: df_urb_pop\ndf_urb_pop = next(urb_pop_reader)\n\n# Check out the head of the DataFrame\nprint(df_urb_pop.head())\n\n# Check out specific country: df_pop_ceb\ndf_pop_ceb = df_urb_pop[df_urb_pop['CountryCode']== 'CEB']\n\n# Zip DataFrame columns of interest: pops\npops = zip(df_pop_ceb['Total Population'], df_pop_ceb['Urban population (% of total)'])\n\n# Turn zip object into list: pops_list\npops_list = list(pops)\n\n# Print pops_list\nprint(pops_list)",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-3",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-3",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing an iterator to load data in chunks (3)",
    "text": "Writing an iterator to load data in chunks (3)\nYou’re getting used to reading and processing data in chunks by now. Let’s push your skills a little further by adding a column to a DataFrame.\nStarting from the code of the previous exercise, you will be using a list comprehension to create the values for a new column ‘Total Urban Population’ from the list of tuples that you generated earlier. Recall from the previous exercise that the first and second elements of each tuple consist of, respectively, values from the columns ‘Total Population’ and ‘Urban population (% of total)’. The values in this new column ‘Total Urban Population’, therefore, are the product of the first and second element in each tuple. Furthermore, because the 2nd element is a percentage, you need to divide the entire result by 100, or alternatively, multiply it by 0.01.\nYou will also plot the data from this new column to create a visualization of the urban population data.\nThe packages pandas and matplotlib.pyplot have been imported as pd and plt respectively for your use. ### Instructions - Write a list comprehension to generate a list of values from pops_list for the new column ‘Total Urban Population’. The output expression should be the product of the first and second element in each tuple in pops_list. Because the 2nd element is a percentage, you also need to either multiply the result by 0.01 or divide it by 100. In addition, note that the column ‘Total Urban Population’ should only be able to take on integer values. To ensure this, make sure you cast the output expression to an integer with int(). - Create a scatter plot where the x-axis are values from the ‘Year’ column and the y-axis are values from the ‘Total Urban Population’ column.\nfrom matplotlib import pyplot as plt\n# Code from previous exercise\nurb_pop_reader = pd.read_csv('datasets/world_ind_pop_data.csv', chunksize=1000)\ndf_urb_pop = next(urb_pop_reader)\ndf_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\npops = zip(df_pop_ceb['Total Population'], \n           df_pop_ceb['Urban population (% of total)'])\npops_list = list(pops)\n\n# Use list comprehension to create new DataFrame column 'Total Urban Population'\ndf_pop_ceb['Total Urban Population'] = [int(0.01*pop[0]*pop[1]) for pop in pops_list]\n\n# Plot urban population data\ndf_pop_ceb.plot(kind='scatter', x='Year', y='Total Urban Population')\nplt.show()",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-4",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-4",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing an iterator to load data in chunks (4)",
    "text": "Writing an iterator to load data in chunks (4)\nIn the previous exercises, you’ve only processed the data from the first DataFrame chunk. This time, you will aggregate the results over all the DataFrame chunks in the dataset. This basically means you will be processing the entire dataset now. This is neat because you’re going to be able to process the entire large dataset by just working on smaller pieces of it!\nYou’re going to use the data from ‘ind_pop_data.csv’, available in your current directory. The packages pandas and matplotlib.pyplot have been imported as pd and plt respectively for your use. ### Instructions - Initialize an empty DataFrame data using pd.DataFrame(). - In the for loop, iterate over urb_pop_reader to be able to process all the DataFrame chunks in the dataset. - Concatenate data and df_pop_ceb by passing a list of the DataFrames to pd.concat().\nfrom matplotlib import pyplot as plt\n# Initialize reader object: urb_pop_reader\nurb_pop_reader = pd.read_csv('datasets/world_ind_pop_data.csv', chunksize=1000)\n\n# Initialize empty DataFrame: data\ndata = pd.DataFrame()\n\n# Iterate over each DataFrame chunk\nfor df_urb_pop in urb_pop_reader:\n\n    # Check out specific country: df_pop_ceb\n    df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == 'CEB']\n\n    # Zip DataFrame columns of interest: pops\n    pops = zip(df_pop_ceb['Total Population'],\n                df_pop_ceb['Urban population (% of total)'])\n\n    # Turn zip object into list: pops_list\n    pops_list = list(pops)\n\n    # Use list comprehension to create new DataFrame column 'Total Urban Population'\n    df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\n    \n    # Concatenate DataFrame chunk to the end of data: data\n    data = pd.concat([data,df_pop_ceb])\n\n# Plot urban population data\ndata.plot(kind='scatter', x='Year', y='Total Urban Population')\nplt.show()",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-5",
    "href": "9_Python_Data_Science_Toolbox_Part2.html#writing-an-iterator-to-load-data-in-chunks-5",
    "title": "Python Data Science Toolbox (Part 2)",
    "section": "Writing an iterator to load data in chunks (5)",
    "text": "Writing an iterator to load data in chunks (5)\nThis is the last leg. You’ve learned a lot about processing a large dataset in chunks. In this last exercise, you will put all the code for processing the data into a single function so that you can reuse the code without having to rewrite the same things all over again.\nYou’re going to define the function plot_pop() which takes two arguments: the filename of the file to be processed, and the country code of the rows you want to process in the dataset.\nBecause all of the previous code you’ve written in the previous exercises will be housed in plot_pop(), calling the function already does the following:\nLoading of the file chunk by chunk,\nCreating the new column of urban population values, and\nPlotting the urban population data.\nThat’s a lot of work, but the function now makes it convenient to repeat the same process for whatever file and country code you want to process and visualize!\nYou’re going to use the data from ‘ind_pop_data.csv’, available in your current directory. The packages pandas and matplotlib.pyplot has been imported as pd and plt respectively for your use.\nAfter you are done, take a moment to look at the plots and reflect on the new skills you have acquired. The journey doesn’t end here! If you have enjoyed working with this data, you can continue exploring it using the pre-processed version available on Kaggle. ### Instructions - Define the function plot_pop() that has two arguments: first is filename for the file to process and second is country_code for the country to be processed in the dataset. - Call plot_pop() to process the data for country code ‘CEB’ in the file ‘ind_pop_data.csv’. - Call plot_pop() to process the data for country code ‘ARB’ in the file ‘ind_pop_data.csv’.\n# Define plot_pop()\ndef plot_pop(filename,country_code):\n\n    # Initialize reader object: urb_pop_reader\n    urb_pop_reader = pd.read_csv(filename, chunksize=1000)\n\n    # Initialize empty DataFrame: data\n    data = pd.DataFrame()\n    \n    # Iterate over each DataFrame chunk\n    for df_urb_pop in urb_pop_reader:\n        # Check out specific country: df_pop_ceb\n        df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == country_code]\n\n        # Zip DataFrame columns of interest: pops\n        pops = zip(df_pop_ceb['Total Population'],\n                    df_pop_ceb['Urban population (% of total)'])\n\n        # Turn zip object into list: pops_list\n        pops_list = list(pops)\n\n        # Use list comprehension to create new DataFrame column 'Total Urban Population'\n        df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\n        \n        # Concatenate DataFrame chunk to the end of data: data\n        data = pd.concat([data, df_pop_ceb])\n\n    # Plot urban population data\n    data.plot(kind='scatter', x='Year', y='Total Urban Population')\n    plt.show()\n\n# Set the filename: fn\nfn = 'ind_pop_data.csv'\n\n# Call plot_pop for country code 'CEB'\nplot_pop('datasets/world_ind_pop_data.csv','CEB')\n\n# Call plot_pop for country code 'ARB'\nplot_pop(\"datasets/world_ind_pop_data.csv\",'ARB')",
    "crumbs": [
      "Python Data Science Toolbox (Part 2)"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "",
    "text": "Explore Datasets\nUse the DataFrames imported in the first cell to explore the data and practice your skills! - Use lmplot() to look at the relationship between temp and total_rentals from bike_share. Plot two regression lines for working and non-working days (workingday). - Create a heat map from daily_show to see how the types of guests (Group) have changed yearly. - Explore the variables from insurance and their relationship by creating pairwise plots and experimenting with different variables and types of plots. Additionally, you can use color to segment visually for region. - Make sure to add titles and labels to your plots and adjust their format for readability!",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#reading-a-csv-file",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#reading-a-csv-file",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Reading a csv file",
    "text": "Reading a csv file\nBefore you analyze data, you will need to read the data into a pandas DataFrame. In this exercise, you will be looking at data from US School Improvement Grants in 2010. This program gave nearly $4B to schools to help them renovate or improve their programs.\nThis first step in most data analysis is to import pandas and seaborn and read a data file in order to analyze it further.\nThis course introduces a lot of new concepts, so if you ever need a quick refresher, download the Seaborn Cheat Sheet and keep it handy!\n\n\n\n\n\n\nInstructions\n\n\n\n\nImport pandas and seaborn using the standard naming conventions. - The path to the csv file is stored in the grant_file variable. - Use pandas to read the file. - Store the resulting DataFrame in the variable df.\n\n\n\n\ngrant_file=\"datasets/schoolimprovement2010grants.csv\"\n# import all modules\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Read in the DataFrame\ndf = pd.read_csv(grant_file)\ndf.head(3)\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nSchool Name\nCity\nState\nDistrict Name\nModel Selected\nAward_Amount\nRegion\n\n\n\n\n0\n0\nHOGARTH KINGEEKUK MEMORIAL SCHOOL\nSAVOONGA\nAK\nBERING STRAIT SCHOOL DISTRICT\nTransformation\n471014\nWest\n\n\n1\n1\nAKIACHAK SCHOOL\nAKIACHAK\nAK\nYUPIIT SCHOOL DISTRICT\nTransformation\n520579\nWest\n\n\n2\n2\nGAMBELL SCHOOL\nGAMBELL\nAK\nBERING STRAIT SCHOOL DISTRICT\nTransformation\n449592\nWest",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#comparing-a-histogram-and-displot",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#comparing-a-histogram-and-displot",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Comparing a histogram and displot",
    "text": "Comparing a histogram and displot\nThe pandas library supports simple plotting of data, which is very convenient when data is already likely to be in a pandas DataFrame.\nSeaborn generally does more statistical analysis on data and can provide more sophisticated insight into the data. In this exercise, we will compare a pandas histogram vs the seaborn displot. ## Instructions - Use the pandas’ plot.hist() function to plot a histogram of the Award_Amount column. - Use Seaborn’s displot() function to plot a distribution plot of the same column.\n\n# Display pandas histogram\ndf['Award_Amount'].plot.hist()\nplt.show()\n\n# Clear out the pandas histogram\nplt.clf()\n\n# Display a Seaborn displot\nsns.displot(df['Award_Amount'])\nplt.show()\n\n# Clear the displot\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#plot-a-histogram",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#plot-a-histogram",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Plot a histogram",
    "text": "Plot a histogram\nThe displot() function will return a histogram by default. The displot() can also create a KDE or rug plot which are useful ways to look at the data. Seaborn can also combine these plots so you can perform more meaningful analysis.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a displot for the data. - Explicitly pass in the number 20 for the number of bins in the histogram. - Display the plot using plt.show().\n\n\n\n\n# Create a displot\nsns.displot(df['Award_Amount'],\n             bins=20)\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#rug-plot-and-kde-shading",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#rug-plot-and-kde-shading",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Rug plot and kde shading",
    "text": "Rug plot and kde shading\nNow that you understand some function arguments for displot(), we can continue further refining the output. This process of creating a visualization and updating it in an incremental fashion is a useful and common approach to look at data from multiple perspectives.\nSeaborn excels at making this process simple.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a displot of the Award_Amount column in the df. - Configure it to show a shaded kde plot (using the kind and fill parameters). - Add a rug plot above the x axis (using the rug parameter). - Display the plot.\n\n\n\n\n# Create a displot of the Award Amount\nsns.displot(df['Award_Amount'],\n             kind='kde',\n             rug=True,\n             fill=True)\n\n# Plot the results\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#create-a-regression-plot",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#create-a-regression-plot",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Create a regression plot",
    "text": "Create a regression plot\nFor this set of exercises, we will be looking at FiveThirtyEight’s data on which US State has the worst drivers. The data set includes summary level information about fatal accidents as well as insurance premiums for each state as of 2010.\nIn this exercise, we will look at the difference between the regression plotting functions.\n\n\n\n\n\n\nInstructions\n\n\n\n\nThe data is available in the dataframe called df. - Create a regression plot using regplot() with “insurance_losses” on the x axis and “premiums” on the y axis. - Create a regression plot of “premiums” versus “insurance_losses” using lmplot(). Display the plot.\n\n\n\n\ndf=pd.read_csv(\"datasets/insurance_premiums.csv\")\n# Create a regression plot of premiums vs. insurance_losses\nsns.regplot(data=df, x='insurance_losses', y='premiums')\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create an lmplot of premiums vs. insurance_losses\nsns.lmplot(data=df,x='insurance_losses',y='premiums')\n# Display the second plot\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#plotting-multiple-variables",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#plotting-multiple-variables",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Plotting multiple variables",
    "text": "Plotting multiple variables\nSince we are using lmplot() now, we can look at the more complex interactions of data. This data set includes geographic information by state and area. It might be interesting to see if there is a difference in relationships based on the Region of the country.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse lmplot() to look at the relationship between insurance_losses and premiums. - Plot a regression line for each Region of the country.\n\n\n\n\n# Create a regression plot using hue\nsns.lmplot(data=df,\n           x=\"insurance_losses\",\n           y=\"premiums\",\n           hue=\"Region\")\n\n# Show the results\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#facetting-multiple-regressions",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#facetting-multiple-regressions",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Facetting multiple regressions",
    "text": "Facetting multiple regressions\nlmplot() allows us to facet the data across multiple rows and columns. In the previous plot, the multiple lines were difficult to read in one plot. We can try creating multiple plots by Region to see if that is a more useful visualization.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse lmplot() to look at the relationship between insurance_losses and premiums. - Create a plot for each Region of the country. - Display the plots across multiple rows.\n\n\n\n\n# Create a regression plot with multiple rows\nsns.lmplot(data=df,\n           x=\"insurance_losses\",\n           y=\"premiums\",\n           row=\"Region\")\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#theme-examples-with-sns.set_style",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#theme-examples-with-sns.set_style",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Theme examples with sns.set_style()",
    "text": "Theme examples with sns.set_style()\n\nfor style in ['white','dark','whitegrid','darkgrid','ticks']:    \n    sns.set_style(style)    \n    sns.displot(df['premiums'])    \n    plt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#setting-the-default-style",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#setting-the-default-style",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Setting the default style",
    "text": "Setting the default style\nFor these exercises, we will be looking at fair market rent values calculated by the US Housing and Urban Development Department. This data is used to calculate guidelines for several federal programs. The actual values for rents vary greatly across the US. We can use this data to get some experience with configuring Seaborn plots.\nAll of the necessary imports for seaborn, pandas and matplotlib have been completed. The data is stored in the pandas DataFrame df.\nBy the way, if you haven’t downloaded it already, check out the Seaborn Cheat Sheet. It includes an overview of the most important concepts, functions and methods and might come in handy if you ever need a quick refresher!\n\n\n\n\n\n\nInstructions\n\n\n\n\nPlot a pandas histogram without adjusting the style. - Set Seaborn’s default style. - Create another pandas histogram of the fmr_2 column which represents fair market rent for a 2-bedroom apartment.\n\n\n\n\n# Plot the pandas histogram\ndf['premiums'].plot.hist()\nplt.show()\nplt.clf()\n\n# Set the default seaborn style\nsns.set()\n\n# Plot the pandas histogram again\ndf['premiums'].plot.hist()\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#comparing-styles",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#comparing-styles",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Comparing styles",
    "text": "Comparing styles\nSeaborn supports setting different styles that can control the aesthetics of the final plot. In this exercise, you will plot the same data in two different styles in order to see how the styles change the output.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a displot() of the fmr_2 column in df using a dark style. Use plt.clf() to clear the figure. - Create the same displot() of fmr_2 using a whitegrid style. Clear the plot after showing it.\n\n\n\n\nsns.set_style('dark')\nsns.displot(df['premiums'])\nplt.show()\nplt.clf()\n\nsns.set_style('whitegrid')\nsns.displot(df['premiums'])\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#removing-spines",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#removing-spines",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Removing spines",
    "text": "Removing spines\nIn general, visualizations should minimize extraneous markings so that the data speaks for itself. Seaborn allows you to remove the lines on the top, bottom, left and right axis, which are often called spines.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse a white style for the plot. - Create a lmplot() comparing the pop2010 and the fmr_2 columns. - Remove the top and right spines using despine().\n\n\n\n\ndf.head(2)\n# Set the style to white\nsns.set_style('white')\n\n# Create a regression plot\nsns.lmplot(data=df,\n           x='premiums',\n           y='insurance_losses')\n\n# Remove the spines\nsns.despine()\n\n# Show the plot and clear the figure\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#defining-a-color-for-a-plot",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#defining-a-color-for-a-plot",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Defining a color for a plot",
    "text": "Defining a color for a plot\n\ndf=college_data\nsns.set(color_codes=True)\nsns.displot(df['Tuition'], color='g')",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#palettes",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#palettes",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Palettes",
    "text": "Palettes\n\npalettes = ['deep', 'muted', 'pastel', 'bright', 'dark','colorblind']\nfor p in palettes:\n    sns.set_palette(p)\n    sns.displot(df['Tuition'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisplaying Palettes\n\npalettes = ['deep', 'muted', 'pastel', 'bright','dark','colorblind']\nfor p in palettes:    \n    sns.set_palette(p)    \n    sns.palplot(sns.color_palette())    \n    plt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#matplotlib-color-codes",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#matplotlib-color-codes",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Matplotlib color codes",
    "text": "Matplotlib color codes\nSeaborn offers several options for modifying the colors of your visualizations. The simplest approach is to explicitly state the color of the plot. A quick way to change colors is to use the standard matplotlib color codes.\n\n\n\n\n\n\nInstructions\n\n\n\n\nSet the default Seaborn style and enable the matplotlib color codes. - Create a displot for the fmr_3 column using matplotlib’s magenta (m) color code.\n\n\n\n\n# Set style, enable color code, and create a magenta displot\nsns.set(color_codes=True)\nsns.displot(df['Tuition'], color='m')\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-default-palettes",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-default-palettes",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Using default palettes",
    "text": "Using default palettes\nSeaborn includes several default palettes that can be easily applied to your plots. In this example, we will look at the impact of two different palettes on the same displot.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a for loop to show the difference between the bright and colorblind palette. - Set the palette using the set_palette() function. - Use a displot of the fmr_3 column.\n\n\n\n\n# Loop through differences between bright and colorblind palettes\nfor p in ['bright', 'colorblind']:\n    sns.set_palette(p)\n    sns.displot(df['Tuition'])\n    plt.show()\n    \n    # Clear the plots    \n    plt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#creating-custom-palettes",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#creating-custom-palettes",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Creating Custom Palettes",
    "text": "Creating Custom Palettes\nChoosing a cohesive palette that works for your data can be time consuming. Fortunately, Seaborn provides the color_palette() function to create your own custom sequential, categorical, or diverging palettes. Seaborn also makes it easy to view your palettes by using the palplot() function.\nIn this exercise, you can experiment with creating different palettes.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate and display a Purples sequential palette containing 8 colors. - Create and display a palette with 10 colors using the husl system. - Create and display a diverging palette with 6 colors coolwarm.\n\n\n\n\nsns.palplot(sns.color_palette(\"Purples\",8))\nplt.show()\nsns.palplot(sns.color_palette(\"husl\",10))\nplt.show()\nsns.palplot(sns.color_palette(\"coolwarm\",6))\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#matplotlib-axes",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#matplotlib-axes",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Matplotlib Axes",
    "text": "Matplotlib Axes\n\nMost customization available through matplotlib\nAxes objectsAxes can be passed to seaborn functions\n\n\nfig, ax = plt.subplots()\nsns.histplot(df['Tuition'], ax=ax)\nax.set(xlabel='Tuition 2013-14')",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#further-customizations",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#further-customizations",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Further Customizations",
    "text": "Further Customizations\n\nThe axes object supports many common customizations\n\n\nfig, ax = plt.subplots()\nsns.histplot(df['Tuition'], ax=ax)\nax.set(xlabel=\"Tuition 2013-14\",       ylabel=\"Distribution\", xlim=(0, 50000), title=\"2013-14 Tuition and Fees Distribution\")\n\n[Text(0.5, 0, 'Tuition 2013-14'),\n Text(0, 0.5, 'Distribution'),\n (0.0, 50000.0),\n Text(0.5, 1.0, '2013-14 Tuition and Fees Distribution')]",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#combining-plots",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#combining-plots",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Combining Plots",
    "text": "Combining Plots\n\nIt is possible to combine and configure multiple plots\n\n\nfig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2,sharey=True, figsize=(7,4))\nsns.histplot(df['Tuition'], stat='density', ax=ax0)\nsns.histplot(df.query('Regions == \"South East\"')['Tuition'], stat='density', ax=ax1)\nax1.set(xlabel='Tuition (South East)', xlim=(0, 70000))\nax1.axvline(x=20000, label='My Budget', linestyle='--')\nax1.legend()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-matplotlib-axes",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-matplotlib-axes",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Using matplotlib axes",
    "text": "Using matplotlib axes\nSeaborn uses matplotlib as the underlying library for creating plots. Most of the time, you can use the Seaborn API to modify your visualizations but sometimes it is helpful to use matplotlib’s functions to customize your plots. The most important object in this case is matplotlib’s axes.\nOnce you have an axes object, you can perform a lot of customization of your plot.\nIn these examples, the US HUD data is loaded in the dataframe df and all libraries are imported.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse plt.subplots() to create a axes and figure objects. - Plot a histplot of column fmr_3 on the axes. - Set a more useful label on the x axis of “3 Bedroom Fair Market Rent”.\n\n\n\n\n# Create a figure and axes\nfig, ax = plt.subplots()\n\n# Plot the distribution of data\nsns.histplot(df['Tuition'], ax=ax)\n\n# Create a more descriptive x axis label\nax.set(xlabel=\"Tuition fees\")\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#additional-plot-customizations",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#additional-plot-customizations",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Additional plot customizations",
    "text": "Additional plot customizations\nThe matplotlib API supports many common customizations such as labeling axes, adding titles, and setting limits. Let’s complete another customization exercise.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a histplot of the fmr_1 column. - Modify the x axis label to say “1 Bedroom Fair Market Rent”. - Change the x axis limits to be between 100 and 1500. - Add a descriptive title of “US Rent” to the plot.\n\n\n\n\n# Create a figure and axes\nfig, ax = plt.subplots()\n\n# Plot the distribution of 1 bedroom rents\nsns.histplot(df['Tuition'], ax=ax)\n\n# Modify the properties of the plot\nax.set(xlabel=\"Tuition\",\n       xlim=(500,50000),\n       title=\"University Tuition fees\")\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#adding-annotations",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#adding-annotations",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Adding annotations",
    "text": "Adding annotations\nEach of the enhancements we have covered can be combined together. In the next exercise, we can annotate our distribution plot to include lines that show the mean and median rent prices.\nFor this example, the palette has been changed to bright using sns.set_palette()\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a figure and axes. - Plot the fmr_1 column distribution. - Add a vertical line using axvline for the median and mean of the values which are already defined.\n\n\n\n\nmean=df['Tuition'].mean()\nmedian=df['Tuition'].median()\n# Create a figure and axes. Then plot the data\nfig, ax = plt.subplots()\nsns.histplot(df['Tuition'], ax=ax)\n\n# Customize the labels and limits\nax.set(xlabel=\"Tuition\", xlim=(100,50000), title=\"Tuition Fees\")\n\n# Add vertical lines for the median and mean\nax.axvline(x=median, color='m', label='Median', linestyle='--', linewidth=2)\nax.axvline(x=mean, color='b', label='Mean', linestyle='-', linewidth=2)\n\n# Show the legend and plot the data\nax.legend()\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#multiple-plots",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#multiple-plots",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Multiple plots",
    "text": "Multiple plots\nFor the final exercise we will plot a comparison of the fair market rents for 1-bedroom and 2-bedroom apartments.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate two axes objects, ax0 and ax1. - Plot fmr_1 on ax0 and fmr_2 on ax1. - Display the plots side by side.\n\n\n\n\n# Create a plot with 1 row and 2 columns that share the y axis label\nfig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharey=True)\n\n# Plot the distribution of 1 bedroom apartments on ax0\nsns.histplot(df['PCTPELL'], ax=ax0)\nax0.set(xlabel=\"PCTPELL\", xlim=(0.1,0.9))\n\n# Plot the distribution of 2 bedroom apartments on ax1\nsns.histplot(df['PCTFLOAN'], ax=ax1)\nax1.set(xlabel=\"PCTFLOAN\", xlim=(0.1,0.9))\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#plots-of-each-observation---stripplot-1",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#plots-of-each-observation---stripplot-1",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Plots of each observation - stripplot",
    "text": "Plots of each observation - stripplot\n\ndf=insurance\ndf.head()\n\n\n\n\n\n\n\n\n\nState\nfatal_collisions\nfatal_collisions_speeding\nfatal_collisions_alc\nfatal_collisions_not_distracted\nfatal_collisions_no_hist\npremiums\ninsurance_losses\nRegion\n\n\n\n\n0\nAlabama\n18.8\n39\n30\n96\n80\n784.55\n145.08\nSouth\n\n\n1\nAlaska\n18.1\n41\n25\n90\n94\n1053.48\n133.93\nWest\n\n\n2\nArizona\n18.6\n35\n28\n84\n96\n899.47\n110.35\nWest\n\n\n3\nArkansas\n22.4\n18\n26\n94\n95\n827.34\n142.39\nSouth\n\n\n4\nCalifornia\n12.0\n35\n28\n91\n89\n878.41\n165.63\nWest\n\n\n\n\n\n\n\n\n\nsns.stripplot(data=df, y=\"Region\", x=\"insurance_losses\", jitter=True)",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#plots-of-each-observation---swarmplot-1",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#plots-of-each-observation---swarmplot-1",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Plots of each observation - swarmplot",
    "text": "Plots of each observation - swarmplot\n\nsns.swarmplot(data=df, y=\"Region\", x=\"insurance_losses\")",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#abstract-representations---boxplot-1",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#abstract-representations---boxplot-1",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Abstract representations - boxplot",
    "text": "Abstract representations - boxplot\n\nsns.boxplot(data=df, y=\"Region\", x=\"insurance_losses\")",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#abstract-representation---violinplot-1",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#abstract-representation---violinplot-1",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Abstract representation - violinplot",
    "text": "Abstract representation - violinplot\n\nsns.violinplot(data=df, y=\"Region\", x=\"insurance_losses\")",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#abstract-representation---boxenplot-1",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#abstract-representation---boxenplot-1",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Abstract representation - boxenplot",
    "text": "Abstract representation - boxenplot\n\nsns.boxenplot(data=df, y=\"Region\", x=\"insurance_losses\")",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#statistical-estimates---barplot-1",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#statistical-estimates---barplot-1",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Statistical estimates - barplot",
    "text": "Statistical estimates - barplot\n\nsns.barplot(data=df, y=\"Region\", x=\"insurance_losses\")#,hue='State')",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#statistical-estimates---pointplot-1",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#statistical-estimates---pointplot-1",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Statistical estimates - pointplot",
    "text": "Statistical estimates - pointplot\n\nsns.pointplot(data=df, y=\"Region\", x=\"insurance_losses\")#,hue='State')",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#stripplot-and-swarmplot",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#stripplot-and-swarmplot",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "stripplot() and swarmplot()",
    "text": "stripplot() and swarmplot()\nMany datasets have categorical data and Seaborn supports several useful plot types for this data. In this example, we will continue to look at the 2010 School Improvement data and segment the data by the types of school improvement models used.\nAs a refresher, here is the KDE distribution of the Award Amounts:\nWhile this plot is useful, there is a lot more we can learn by looking at the individual Award_Amount and how the amounts are distributed among the four categories.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a stripplot of the Award_Amount with the Model Selected on the y axis with jitter enabled. - Create a swarmplot() of the same data, but also include the hue by Region.\n\n\n\n\ndf=college_data\nprint(df.columns)\n\nIndex(['INSTNM', 'OPEID', 'REGION', 'SAT_AVG_ALL', 'PCTPELL', 'PCTFLOAN',\n       'ADM_RATE_ALL', 'UG', 'AVGFACSAL', 'COMPL_RPY_5YR_RT', 'DEBT_MDN',\n       'MEDIAN_HH_INC', 'ICLEVEL', 'HIGHDEG', 'CONTROL', 'WOMENONLY',\n       'MENONLY', 'LOCALE', 'Tuition', 'Degree_Type', 'Ownership', 'Regions',\n       'Locales', 'Locale_Short'],\n      dtype='object')\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nINSTNM\nOPEID\nREGION\nSAT_AVG_ALL\nPCTPELL\nPCTFLOAN\nADM_RATE_ALL\nUG\nAVGFACSAL\nCOMPL_RPY_5YR_RT\n...\nCONTROL\nWOMENONLY\nMENONLY\nLOCALE\nTuition\nDegree_Type\nOwnership\nRegions\nLocales\nLocale_Short\n\n\n\n\n0\nAlabama A & M University\n100200\n5\n850.0\n0.7249\n0.8159\n0.653841\n4380.0\n7017.0\n0.477631579\n...\n1\n0.0\n0.0\n12.0\n13435.0\nGraduate\nPublic\nSouth East\nCity: Midsize\nCity\n\n\n1\nUniversity of Alabama at Birmingham\n105200\n5\n1147.0\n0.3505\n0.5218\n0.604275\n10331.0\n10221.0\n0.673230442\n...\n1\n0.0\n0.0\n12.0\n16023.0\nGraduate\nPublic\nSouth East\nCity: Midsize\nCity\n\n\n2\nAmridge University\n2503400\n5\nNaN\n0.7455\n0.8781\nNaN\n98.0\n3217.0\n0.636363636\n...\n2\n0.0\n0.0\n12.0\n8862.0\nGraduate\nPrivate non-profit\nSouth East\nCity: Midsize\nCity\n\n\n3\nUniversity of Alabama in Huntsville\n105500\n5\n1221.0\n0.3179\n0.4589\n0.811971\n5220.0\n9514.0\n0.762222222\n...\n1\n0.0\n0.0\n12.0\n18661.0\nGraduate\nPublic\nSouth East\nCity: Midsize\nCity\n\n\n4\nAlabama State University\n100500\n5\n844.0\n0.7567\n0.7692\n0.463858\n4348.0\n7940.0\n0.43006993\n...\n1\n0.0\n0.0\n12.0\n7400.0\nGraduate\nPublic\nSouth East\nCity: Midsize\nCity\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n\n# Create the stripplot\nsns.stripplot(data=df,\n         x='Tuition',\n         y='Regions',\n         jitter=True\n         )\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Create the stripplot\nsns.swarmplot(data=df,\n         x='Tuition',\n         y='Regions',\n         hue='Ownership'\n         )\n\nplt.show()\n\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 90.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 82.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 82.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 81.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 60.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 65.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 86.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 85.9% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 70.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 91.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 82.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 83.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 81.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 61.2% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 66.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/categorical.py:3399: UserWarning: 87.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#boxplots-violinplots-and-boxenplots",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#boxplots-violinplots-and-boxenplots",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "boxplots, violinplots and boxenplots",
    "text": "boxplots, violinplots and boxenplots\nSeaborn’s categorical plots also support several abstract representations of data. The API for each of these is the same so it is very convenient to try each plot and see if the data lends itself to one over the other.\nIn this exercise, we will use the color palette options presented in Chapter 2 to show how colors can easily be included in the plots.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate and display a boxplot of the data with Award_Amount on the x axis and Model Selected on the y axis.\nUse Award_Amount on the x axis and Model Selected on the y axis. - Create and display an boxenplot using the Paired palette and the Region column as the hue.\n\n\n\n\n# Read in the DataFrame\ndf_grant = pd.read_csv(\"datasets/schoolimprovement2010grants.csv\")\n# Create a boxplot\nsns.boxplot(data=df_grant,\n         x='Award_Amount',\n         y='Model Selected')\n\nplt.show()\nplt.clf()\n\n# Create a violinplot with the husl palette\nsns.violinplot(data=df_grant,\n         x='Award_Amount',\n         y='Model Selected',\n         palette='husl')\n\nplt.show()\nplt.clf()\n\n# Create a boxenplot with the Paired palette and the Region column as the hue\nsns.boxenplot(data=df_grant,\n         x='Award_Amount',\n         y='Model Selected',\n         palette='Paired',\n         hue='Region')\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n/var/folders/53/yp3kynfd7rn5y13c2wwfm33rmgtrfb/T/ipykernel_38419/1096742578.py:12: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.violinplot(data=df_grant,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#barplots-pointplots-and-countplots",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#barplots-pointplots-and-countplots",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "barplots, pointplots and countplots",
    "text": "barplots, pointplots and countplots\nThe final group of categorical plots are barplots, pointplots and countplot which create statistical summaries of the data. The plots follow a similar API as the other plots and allow further customization for the specific problem at hand.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a countplot with the df dataframe and Model Selected on the y axis and the color varying by Region. - Create a pointplot with the df dataframe and Model Selected on the x-axis and Award_Amount on the y-axis. Use a capsize in the pointplot in order to add caps to the error bars. - Create a barplot with the same data on the x and y axis and change the color of each bar based on the Region column.\n\n\n\n\n'''\n# Show a countplot with the number of models used with each region a different color\nsns.countplot(data=df,\n         y=\"Model Selected\",\n         hue=\"Region\")\n\nplt.show()\nplt.clf()\n\n# Create a pointplot and include the capsize in order to show caps on the error bars\nsns.pointplot(data=df,\n         y='Award_Amount',\n         x='Model Selected',\n         capsize=.1)\n\nplt.show()\nplt.clf()\n\n# Create a barplot with each Region shown as a different color\nsns.barplot(data=df,\n         y='Award_Amount',\n         x='Model Selected',\n         hue='Region')\n\nplt.show()\nplt.clf()\n'''\n\n'\\n# Show a countplot with the number of models used with each region a different color\\nsns.countplot(data=df,\\n         y=\"Model Selected\",\\n         hue=\"Region\")\\n\\nplt.show()\\nplt.clf()\\n\\n# Create a pointplot and include the capsize in order to show caps on the error bars\\nsns.pointplot(data=df,\\n         y=\\'Award_Amount\\',\\n         x=\\'Model Selected\\',\\n         capsize=.1)\\n\\nplt.show()\\nplt.clf()\\n\\n# Create a barplot with each Region shown as a different color\\nsns.barplot(data=df,\\n         y=\\'Award_Amount\\',\\n         x=\\'Model Selected\\',\\n         hue=\\'Region\\')\\n\\nplt.show()\\nplt.clf()\\n'",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#regression-and-residual-plots",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#regression-and-residual-plots",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Regression and residual plots",
    "text": "Regression and residual plots\nLinear regression is a useful tool for understanding the relationship between numerical variables. Seaborn has simple but powerful tools for examining these relationships.\nFor these exercises, we will look at some details from the US Department of Education on 4 year college tuition information and see if there are any interesting insights into which variables might help predict tuition costs.\nFor these exercises, all data is loaded in the df variable.\n\n\n\n\n\n\nInstructions\n\n\n\n\nPlot a regression plot comparing Tuition and average SAT scores(SAT_AVG_ALL). Make sure the values are shown as green triangles. - Use a residual plot to determine if the relationship looks linear.\n\n\n\n\ndf = college_data\ndf.head(3)\n\n\n\n\n\n\n\n\n\nINSTNM\nOPEID\nREGION\nSAT_AVG_ALL\nPCTPELL\nPCTFLOAN\nADM_RATE_ALL\nUG\nAVGFACSAL\nCOMPL_RPY_5YR_RT\n...\nCONTROL\nWOMENONLY\nMENONLY\nLOCALE\nTuition\nDegree_Type\nOwnership\nRegions\nLocales\nLocale_Short\n\n\n\n\n0\nAlabama A & M University\n100200\n5\n850.0\n0.7249\n0.8159\n0.653841\n4380.0\n7017.0\n0.477631579\n...\n1\n0.0\n0.0\n12.0\n13435.0\nGraduate\nPublic\nSouth East\nCity: Midsize\nCity\n\n\n1\nUniversity of Alabama at Birmingham\n105200\n5\n1147.0\n0.3505\n0.5218\n0.604275\n10331.0\n10221.0\n0.673230442\n...\n1\n0.0\n0.0\n12.0\n16023.0\nGraduate\nPublic\nSouth East\nCity: Midsize\nCity\n\n\n2\nAmridge University\n2503400\n5\nNaN\n0.7455\n0.8781\nNaN\n98.0\n3217.0\n0.636363636\n...\n2\n0.0\n0.0\n12.0\n8862.0\nGraduate\nPrivate non-profit\nSouth East\nCity: Midsize\nCity\n\n\n\n\n3 rows × 24 columns\n\n\n\n\n\n# Display a regression plot for Tuition\nsns.regplot(data=df,\n         y='Tuition',\n         x='SAT_AVG_ALL',\n         marker='^',\n         color='g')\n\nplt.show()\nplt.clf()\n# Display the residual plot\nsns.residplot(data=df,\n          y='Tuition',\n          x='SAT_AVG_ALL',\n          color='g')\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#regression-plot-parameters",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#regression-plot-parameters",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Regression plot parameters",
    "text": "Regression plot parameters\nSeaborn’s regression plot supports several parameters that can be used to configure the plots and drive more insight into the data.\nFor the next exercise, we can look at the relationship between tuition and the percent of students that receive Pell grants. A Pell grant is based on student financial need and subsidized by the US Government. In this data set, each University has some percentage of students that receive these grants. Since this data is continuous, using x_bins can be useful to break the percentages into categories in order to summarize and understand the data.\n\n\n\n\n\n\nInstructions\n\n\n\n\nPlot a regression plot of Tuition and PCTPELL. - Create another plot that breaks the PCTPELL column into 5 different bins. -\n\n\n\n\n# Plot a regression plot of Tuition and the Percentage of Pell Grants\nsns.regplot(data=df,\n            y='Tuition',\n            x='PCTPELL')\n\nplt.show()\nplt.clf()\n\n# Create another plot that estimates the tuition by PCTPELL\nsns.regplot(data=df,\n            y='Tuition',\n            x='PCTPELL',\n            x_bins=5)\n\nplt.show()\nplt.clf()\n\n# The final plot should include a line using a 2nd order polynomial\nsns.regplot(data=df,\n            y='Tuition',\n            x='PCTPELL',\n            x_bins=5,\n            order=2)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#binning-data",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#binning-data",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Binning data",
    "text": "Binning data\nWhen the data on the x axis is a continuous value, it can be useful to break it into different bins in order to get a better visualization of the changes in the data.\nFor this exercise, we will look at the relationship between tuition and the Undergraduate population abbreviated as UG in this data. We will start by looking at a scatter plot of the data and examining the impact of different bin sizes on the visualization.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a regplot of Tuition and UG and set the fit_reg parameter to False to disable the regression line. - Create another plot with the UG data divided into 5 bins. - Create a regplot() with the data divided into 8 bins.\n\n\n\n\n# Create a scatter plot by disabling the regression line\nsns.regplot(data=df,\n            y='Tuition',\n            x='UG',\n            fit_reg=False)\n\nplt.show()\nplt.clf()\n# Create a scatter plot and bin the data into 5 bins\nsns.regplot(data=df,\n            y='Tuition',\n            x='UG',\n            x_bins=5)\n\nplt.show()\nplt.clf()\n\n# Create a regplot and bin the data into 8 bins\nsns.regplot(data=df,\n         y='Tuition',\n         x='UG',\n         x_bins=8)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#creating-heatmaps",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#creating-heatmaps",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Creating heatmaps",
    "text": "Creating heatmaps\nA heatmap is a common matrix plot that can be used to graphically summarize the relationship between two variables. For this exercise, we will start by looking at guests of the Daily Show from 1999 - 2015 and see how the occupations of the guests have changed over time.\nThe data includes the date of each guest appearance as well as their occupation. For the first exercise, we need to get the data into the right format for Seaborn’s heatmap function to correctly plot the data. All of the data has already been read into the df variable.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse pandas’ crosstab() function to build a table of visits by Group and Year. - Print the pd_crosstab DataFrame. - Plot the data using Seaborn’s heatmap().\n\n\n\n\ndf=daily_show\n# Create a crosstab table of the data\npd_crosstab = pd.crosstab(df[\"Group\"], df[\"YEAR\"])\nprint(pd_crosstab)\n\n# Plot a heatmap of the table\nsns.heatmap(pd_crosstab)\n\n# Rotate tick marks for visibility\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\n\nplt.show()\n\nYEAR            1999  2000  2001  2002  2003  2004  2005  2006  2007  2008  \\\nGroup                                                                        \nAcademic           0     0     2     0     4     1    12     9    13     5   \nActing           108   100    92    84    74    51    44    44    25    26   \nAdvocacy           0     1     0     1     0     4     0     0     2     3   \nAthletics          0     3     1     2     0     2     2     5     4     1   \nBusiness           0     1     0     0     0     2     1     1     2     1   \nClergy             0     0     0     1     1     1     0     0     1     0   \nComedy            25    12    11     5    12     7     5     8     9     7   \nConsultant         0     0     0     0     1     4     1     4     2     3   \nGovernment         0     0     2     1     2     3     1     3     1     0   \nMedia             11    21    31    42    41    45    54    47    47    77   \nMilitary           0     0     0     0     0     0     1     1     3     1   \nMisc               0     0     2     1     1     0     4     3     2     2   \nMusician          17    13    11    10     7     5    11     6     2     1   \nPolitical Aide     0     1     1     2     1     2     3     3     2     6   \nPolitician         2    13     3     8    14    32    22    25    21    27   \nScience            0     0     0     0     1     2     1     1     4     1   \n\nYEAR            2009  2010  2011  2012  2013  2014  2015  \nGroup                                                     \nAcademic          11     8    10     8     8    10     2  \nActing            22    45    42    33    60    47    33  \nAdvocacy           1     1     1     2     2     3     3  \nAthletics          7     5     2     7     4     4     3  \nBusiness           4     2     3     3     3     1     1  \nClergy             1     0     1     2     0     0     0  \nComedy             7     7     7     6     6     9     7  \nConsultant         2     1     0     0     0     0     0  \nGovernment         5     3     3     3     7     6     0  \nMedia             59    50    51    52    51    53    24  \nMilitary           1     2     3     1     1     1     1  \nMisc               5     4     5     6     2     5     3  \nMusician           5     6     6     5     5     8     5  \nPolitical Aide     3     2     1     1     3     2     3  \nPolitician        26    25    23    29    11    13    14  \nScience            4     3     5     2     2     1     1",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#customizing-heatmaps",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#customizing-heatmaps",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Customizing heatmaps",
    "text": "Customizing heatmaps\nSeaborn supports several types of additional customizations to improve the output of a heatmap. For this exercise, we will continue to use the Daily Show data that is stored in the df variable but we will customize the output.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a crosstab table of Group and YEAR - Create a heatmap of the data using the BuGn palette - Disable the cbar and increase the linewidth to 0.3\n\n\n\n\n# Create the crosstab DataFrame\npd_crosstab = pd.crosstab(df[\"Group\"], df[\"YEAR\"])\n\n# Plot a heatmap of the table with no color bar and using the BuGn palette\nsns.heatmap(pd_crosstab, cbar=False, cmap=\"BuGn\", linewidths=0.3)\n\n# Rotate tick marks for visibility\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\n\n#Show the plot\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#building-a-facetgrid",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#building-a-facetgrid",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Building a FacetGrid",
    "text": "Building a FacetGrid\nSeaborn’s FacetGrid is the foundation for building data-aware grids. A data-aware grid allows you to create a series of small plots that can be useful for understanding complex data relationships.\nFor these exercises, we will continue to look at the College Scorecard Data from the US Department of Education. This rich dataset has many interesting data elements that we can plot with Seaborn.\nWhen building a FacetGrid, there are two steps:\nCreate a FacetGrid object with columns, rows, or hue.\nMap individual plots to the grid.\n\nInstructions\n\nCreate a FacetGrid that shows a point plot of the Average SAT scores SAT_AVG_ALL.\nUse row_order to control the display order of the degree types.\n\n\ndf=college_data\n# Create FacetGrid with Degree_Type and specify the order of the rows using row_order\ng2 = sns.FacetGrid(df, \n             row=\"Degree_Type\",\n             row_order=['Graduate', 'Bachelors', 'Associates', 'Certificate'])\n\n# Map a pointplot of SAT_AVG_ALL onto the grid\ng2.map(sns.pointplot, 'SAT_AVG_ALL')\n\n# Show the plot\nplt.show()\nplt.clf()\n\n/Users/00110139/miniforge3/envs/TF2/lib/python3.9/site-packages/seaborn/axisgrid.py:718: UserWarning: Using the pointplot function without specifying `order` is likely to produce an incorrect plot.\n  warnings.warn(warning)\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-a-catplot",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-a-catplot",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Using a catplot",
    "text": "Using a catplot\nIn many cases, Seaborn’s catplot() can be a simpler way to create a FacetGrid. Instead of creating a grid and mapping the plot, we can use the catplot() to create a plot with one line of code.\nFor this exercise, we will recreate one of the plots from the previous exercise using catplot() and show how to create a boxplot on a data-aware grid. Instructions - Create a catplot() that contains a boxplot (box) of Tuition values varying by Degree_Type across rows. - Create a catplot() of SAT Averages (SAT_AVG_ALL) facetted across Degree_Type that shows a pointplot (point). Use row_order to order the degrees from highest to lowest level.\n\n# Create a factor plot that contains boxplots of Tuition values\nsns.catplot(data=df,\n         x='Tuition',\n         kind='box',\n         row='Degree_Type')\n\nplt.show()\nplt.clf()\n\n\n# Create a facetted pointplot of Average SAT_AVG_ALL scores facetted by Degree Type \nsns.catplot(data=df,\n        x='SAT_AVG_ALL',\n        kind='point',\n        row='Degree_Type',\n        row_order=['Graduate', 'Bachelors', 'Associates', 'Certificate'])\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-a-lmplot",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-a-lmplot",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Using a lmplot",
    "text": "Using a lmplot\nThe lmplot is used to plot scatter plots with regression lines on FacetGrid objects. The API is similar to catplot with the difference that the default behavior of lmplot is to plot regression lines.\nFor the first set of exercises, we will look at the Undergraduate population (UG) and compare it to the percentage of students receiving Pell Grants (PCTPELL).\nFor the second lmplot exercise, we can look at the relationships between Average SAT scores and Tuition across the different degree types and public vs. non-profit schools.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a FacetGrid() with Degree_Type columns and scatter plot of UG and PCTPELL. - Create a lmplot() using the same values from the FacetGrid(). - Create a facetted lmplot() comparing SAT_AVG_ALL to Tuition with columns varying by Ownership and rows by Degree_Type. In the lmplot() add a hue for Women Only Universities.\n\n\n\n\ndf.columns\n\nIndex(['INSTNM', 'OPEID', 'REGION', 'SAT_AVG_ALL', 'PCTPELL', 'PCTFLOAN',\n       'ADM_RATE_ALL', 'UG', 'AVGFACSAL', 'COMPL_RPY_5YR_RT', 'DEBT_MDN',\n       'MEDIAN_HH_INC', 'ICLEVEL', 'HIGHDEG', 'CONTROL', 'WOMENONLY',\n       'MENONLY', 'LOCALE', 'Tuition', 'Degree_Type', 'Ownership', 'Regions',\n       'Locales', 'Locale_Short'],\n      dtype='object')\n\n\n\ndf = college_data\ndegree_ord =['Graduate', 'Bachelors', 'Associates']\n# Create a FacetGrid varying by column and columns ordered with the degree_order variable\ng = sns.FacetGrid(df, col=\"Degree_Type\", col_order=degree_ord)\n\n# Map a scatter plot of Undergrad Population compared to PCTPELL\ng.map(plt.scatter, 'UG', 'PCTPELL')\n\nplt.show()\nplt.clf()\n# Re-create the previous plot as an lmplot\nsns.lmplot(data=df,\n        x='UG',\n        y='PCTPELL',\n        col=\"Degree_Type\",\n        col_order=degree_ord)\n\nplt.show()\nplt.clf()\ninst_ord = ['Public', 'Private non-profit']\n# Create an lmplot that has a column for Ownership, a row for Degree_Type and hue based on the WOMENONLY column\nsns.lmplot(data=df,\n        x='SAT_AVG_ALL',\n        y='Tuition',\n        col=\"Ownership\",\n        row='Degree_Type',\n        row_order=['Graduate', 'Bachelors'],\n        hue='WOMENONLY',\n        col_order=inst_ord)\n\nplt.show()\nplt.clf()\n\n# Create an lmplot that has a column for Ownership, a row for Degree_Type and hue based on the WOMENONLY column\nsns.lmplot(data=df,\n        x='SAT_AVG_ALL',\n        y='Tuition',\n        col=\"Ownership\",\n        row='Degree_Type',\n        row_order=['Graduate', 'Bachelors'],\n        hue='WOMENONLY',\n        col_order=inst_ord)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#building-a-pairgrid",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#building-a-pairgrid",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Building a PairGrid",
    "text": "Building a PairGrid\nWhen exploring a dataset, one of the earliest tasks is exploring the relationship between pairs of variables. This step is normally a precursor to additional investigation.\nSeaborn supports this pair-wise analysis using the PairGrid. In this exercise, we will look at the Car Insurance Premium data we analyzed in Chapter 1. All data is available in the df variable.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCompare “fatal_collisions” to “premiums” by using a scatter plot mapped to a PairGrid(). - Create another PairGrid but plot a histogram on the diagonal and scatter plot on the off diagonal.\n\n\n\n\ndf=insurance\n# Create a PairGrid with a scatter plot for fatal_collisions and premiums\ng = sns.PairGrid(df, vars=[\"fatal_collisions\", \"premiums\"])\ng2 = g.map(sns.scatterplot)\n\nplt.show()\nplt.clf()\n\n# Create the same PairGrid but map a histogram on the diag\ng = sns.PairGrid(df, vars=[\"fatal_collisions\", \"premiums\"])\ng2 = g.map_diag(sns.histplot)\ng3 = g2.map_offdiag(sns.scatterplot)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-a-pairplot",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#using-a-pairplot",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Using a pairplot",
    "text": "Using a pairplot\nThe pairplot() function is generally a more convenient way to look at pairwise relationships. In this exercise, we will create the same results as the PairGrid using less code. Then, we will explore some additional functionality of the pairplot(). We will also use a different palette and adjust the transparency of the diagonal plots using the alpha parameter.\n\n\n\n\n\n\nInstructions\n\n\n\n\nRecreate the pairwise plot from the previous exercise using pairplot(). - Create another pairplot using the “Region” to color code the results.Use the RdBu palette to change the colors of the plot.\n\n\n\n\n #Create a pairwise plot of the variables using a scatter plot\nsns.pairplot(data=df,\n        vars=[\"fatal_collisions\", \"premiums\"],\n        kind='scatter')\n\nplt.show()\nplt.clf()\n\n# Plot the same data but use a different color palette and color code by Region\nsns.pairplot(data=df,\n        vars=[\"fatal_collisions\", \"premiums\"],\n        kind='scatter',\n        hue='Region',\n        palette='RdBu',\n        diag_kws={'alpha':.5})\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#additional-pairplots",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#additional-pairplots",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Additional pairplots",
    "text": "Additional pairplots\nThis exercise will go through a couple of more examples of how the pairplot() can be customized for quickly analyzing data and determining areas of interest that might be worthy of additional analysis.\nOne area of customization that is useful is to explicitly define the x_vars and y_vars that you wish to examine. Instead of examining all pairwise relationships, this capability allows you to look only at the specific interactions that may be of interest.\nWe have already looked at using kind to control the types of plots. We can also use diag_kind to control the types of plots shown on the diagonals. In the final example, we will include a regression and kde plot in the pairplot.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a pair plot that examines fatal_collisions_speeding and fatal_collisions_alc on the x axis and premiums and insurance_losses on the y axis. - Use the husl palette and color code the scatter plot by Region. - Build a pairplot() with kde plots along the diagonals. Include the insurance_losses and premiums as the variables. - Use a reg plot for the the non-diagonal plots. - Use the BrBG palette for the final plot.\n\n\n\n\n# Build a pairplot with different x and y variables\nsns.pairplot(data=df,\n        x_vars=[\"fatal_collisions_speeding\", \"fatal_collisions_alc\"],\n        y_vars=['premiums', 'insurance_losses'],\n        kind='scatter',\n        hue='Region',\n        palette='husl')\n\nplt.show()\nplt.clf()\n\n# plot relationships between insurance_losses and premiums\nsns.pairplot(data=df,\n             vars=[\"insurance_losses\", \"premiums\"],\n             kind='reg',\n             palette='BrBG',\n             diag_kind = 'kde',\n             hue='Region')\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#building-a-jointgrid-and-jointplot",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#building-a-jointgrid-and-jointplot",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Building a JointGrid and jointplot",
    "text": "Building a JointGrid and jointplot\nSeaborn’s JointGrid combines univariate plots such as histograms, rug plots and kde plots with bivariate plots such as scatter and regression plots. The process for creating these plots should be familiar to you now. These plots also demonstrate how Seaborn provides convenient functions to combine multiple plots together.\nFor these exercises, we will use the bike share data that we reviewed earlier. In this exercise, we will look at the relationship between humidity levels and total rentals to see if there is an interesting relationship we might want to explore later.\n\n\n\n\n\n\nInstructions\n\n\n\n\nUse Seaborn’s “whitegrid” style for these plots. - Create a JointGrid() with “hum” on the x-axis and “total_rentals” on the y. - Plot a regplot() and histplot() on the margins. - Re-create the plot using a jointplot().\n\n\n\n\ndf=bike_share\n# Build a JointGrid comparing humidity and total_rentals\nsns.set_style(\"whitegrid\")\ng = sns.JointGrid(x=\"hum\",\n            y=\"total_rentals\",\n            data=df,\n            xlim=(0.1, 1.0)) \n\ng.plot(sns.regplot, sns.histplot)\n\nplt.show()\nplt.clf()\n# Create a jointplot similar to the JointGrid \nsns.jointplot(x=\"hum\",\n        y=\"total_rentals\",\n        kind='reg',\n        data=df)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#jointplots-and-regression",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#jointplots-and-regression",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Jointplots and regression",
    "text": "Jointplots and regression\nSince the previous plot does not show a relationship between humidity and rental amounts, we can look at another variable that we reviewed earlier. Specifically, the relationship between temp and total_rentals.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a jointplot with a 2nd order polynomial regression plot comparing temp and total_rentals.\n\n\n\n\n# Plot temp vs. total_rentals as a regression plot\nsns.jointplot(x=\"temp\",\n         y=\"total_rentals\",\n         kind='reg',\n         data=df,\n         order=2,\n         xlim=(0, 1))\n\nplt.show()\nplt.clf()\n\n# Plot a jointplot showing the residuals\nsns.jointplot(x=\"temp\",\n        y=\"total_rentals\",\n        kind='resid',\n        data=df,\n        order=2)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#complex-jointplots",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#complex-jointplots",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Complex jointplots",
    "text": "Complex jointplots\nThe jointplot is a convenience wrapper around many of the JointGrid functions. However, it is possible to overlay some of the JointGrid plots on top of the standard jointplot. In this example, we can look at the different distributions for riders that are considered casual versus those that are registered.\n\n\n\n\n\n\nInstructions\n\n\n\n\nCreate a jointplot with a scatter plot comparing temp and casual riders. - Overlay a kdeplot on top of the scatter plot. - Build a similar plot for registered users.\n\n\n\n\n# Create a jointplot of temp vs. casual riders\n# Include a kdeplot over the scatter plot\ng = sns.jointplot(x=\"temp\",\n             y=\"casual\",\n             kind='scatter',\n             data=df,\n             marginal_kws=dict(bins=10))\ng.plot_joint(sns.kdeplot)\n    \nplt.show()\nplt.clf()\n\n# Replicate the above plot but only for registered riders\ng = sns.jointplot(x=\"temp\",\n             y=\"registered\",\n             kind='scatter',\n             data=df,\n             marginal_kws=dict(bins=10))\ng.plot_joint(sns.kdeplot)\n\nplt.show()\nplt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "10_Intermediate_Data_Visualization_with_Seaborn.html#bee-swarm-plot-example",
    "href": "10_Intermediate_Data_Visualization_with_Seaborn.html#bee-swarm-plot-example",
    "title": "Intermediate Data Visualization with Seaborn",
    "section": "Bee swarm plot example",
    "text": "Bee swarm plot example\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sample data (replace this with your own dataset)\ndata = sns.load_dataset(\"iris\")\n\n# Create a bee swarm plot using Seaborn\nsns.set(style=\"whitegrid\")  # Set the style\n\n# Replace 'species' with the column name you want to plot\nsns.swarmplot(x=\"species\", y=\"sepal_length\", data=data)\n\n# Set plot labels and title\nplt.xlabel(\"Species\")\nplt.ylabel(\"Sepal Length\")\nplt.title(\"Bee Swarm Plot of Sepal Length by Species\")\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Intermediate Data Visualization with Seaborn"
    ]
  },
  {
    "objectID": "Project-A_Visual_History_of_Nobel_Prize_Winners.html",
    "href": "Project-A_Visual_History_of_Nobel_Prize_Winners.html",
    "title": "Project: A Visual History of Nobel Prize Winners",
    "section": "",
    "text": "Task 1\nWhat is the most commonly awarded gender and birth country? Storing the string answers as top_gender and top_country.\ncount_by_gender = nobel.value_counts(\"sex\")\ntop_gender=count_by_gender.index[0]\ntop_gender\n\n'Male'\ncount_by_country=nobel.value_counts(\"birth_country\")\ntop_country = count_by_country.index[0]\ntop_country\n\n'United States of America'",
    "crumbs": [
      "Project: A Visual History of Nobel Prize Winners"
    ]
  },
  {
    "objectID": "Project-A_Visual_History_of_Nobel_Prize_Winners.html#task-2",
    "href": "Project-A_Visual_History_of_Nobel_Prize_Winners.html#task-2",
    "title": "Project: A Visual History of Nobel Prize Winners",
    "section": "Task 2",
    "text": "Task 2\nWhat decade had the highest proportion of US-born winners? Store this as an integer called max_decade_usa.\n\nnobel[\"USA_born_winers\"] = nobel['birth_country']=='United States of America'\nnobel[\"decade\"] = 10*(nobel[\"year\"]/10).astype(\"int\")\n\n\n10*(nobel[\"year\"]/10).astype(\"int\").unique()\n\narray([1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000,\n       2010, 2020])\n\n\n\nnobel.head(5)\n\n\n\n\n\n\n\n\n\nyear\ncategory\nprize\nmotivation\nprize_share\nlaureate_id\nlaureate_type\nfull_name\nbirth_date\nbirth_city\nbirth_country\nsex\norganization_name\norganization_city\norganization_country\ndeath_date\ndeath_city\ndeath_country\nUSA_born_winers\ndecade\n\n\n\n\n0\n1901\nChemistry\nThe Nobel Prize in Chemistry 1901\n\"in recognition of the extraordinary services ...\n1/1\n160\nIndividual\nJacobus Henricus van 't Hoff\n1852-08-30\nRotterdam\nNetherlands\nMale\nBerlin University\nBerlin\nGermany\n1911-03-01\nBerlin\nGermany\nFalse\n1900\n\n\n1\n1901\nLiterature\nThe Nobel Prize in Literature 1901\n\"in special recognition of his poetic composit...\n1/1\n569\nIndividual\nSully Prudhomme\n1839-03-16\nParis\nFrance\nMale\nNaN\nNaN\nNaN\n1907-09-07\nChâtenay\nFrance\nFalse\n1900\n\n\n2\n1901\nMedicine\nThe Nobel Prize in Physiology or Medicine 1901\n\"for his work on serum therapy, especially its...\n1/1\n293\nIndividual\nEmil Adolf von Behring\n1854-03-15\nHansdorf (Lawice)\nPrussia (Poland)\nMale\nMarburg University\nMarburg\nGermany\n1917-03-31\nMarburg\nGermany\nFalse\n1900\n\n\n3\n1901\nPeace\nThe Nobel Peace Prize 1901\nNaN\n1/2\n462\nIndividual\nJean Henry Dunant\n1828-05-08\nGeneva\nSwitzerland\nMale\nNaN\nNaN\nNaN\n1910-10-30\nHeiden\nSwitzerland\nFalse\n1900\n\n\n4\n1901\nPeace\nThe Nobel Peace Prize 1901\nNaN\n1/2\n463\nIndividual\nFrédéric Passy\n1822-05-20\nParis\nFrance\nMale\nNaN\nNaN\nNaN\n1912-06-12\nParis\nFrance\nFalse\n1900\n\n\n\n\n\n\n\n\n\ndecade_sum = nobel.groupby(\"decade\")[\"USA_born_winers\"].agg(\"count\")\ndecade_sum\n\ndecade\n1900     57\n1910     40\n1920     54\n1930     56\n1940     43\n1950     72\n1960     79\n1970    104\n1980     97\n1990    104\n2000    123\n2010    121\n2020     50\nName: USA_born_winers, dtype: int64\n\n\n\nUSA_only = nobel[nobel[\"USA_born_winers\"]]\nUSA_only.head(10)\n\n\n\n\n\n\n\n\n\nyear\ncategory\nprize\nmotivation\nprize_share\nlaureate_id\nlaureate_type\nfull_name\nbirth_date\nbirth_city\nbirth_country\nsex\norganization_name\norganization_city\norganization_country\ndeath_date\ndeath_city\ndeath_country\nUSA_born_winers\ndecade\n\n\n\n\n35\n1906\nPeace\nThe Nobel Peace Prize 1906\nNaN\n1/1\n470\nIndividual\nTheodore Roosevelt\n1858-10-27\nNew York, NY\nUnited States of America\nMale\nNaN\nNaN\nNaN\n1919-01-06\nOyster Bay, NY\nUnited States of America\nTrue\n1900\n\n\n72\n1912\nPeace\nThe Nobel Peace Prize 1912\nNaN\n1/1\n480\nIndividual\nElihu Root\n1845-02-15\nClinton, NY\nUnited States of America\nMale\nNaN\nNaN\nNaN\n1937-02-07\nNew York, NY\nUnited States of America\nTrue\n1910\n\n\n79\n1914\nChemistry\nThe Nobel Prize in Chemistry 1914\n\"in recognition of his accurate determinations...\n1/1\n175\nIndividual\nTheodore William Richards\n1868-01-31\nGermantown, PA\nUnited States of America\nMale\nHarvard University\nCambridge, MA\nUnited States of America\n1928-04-02\nCambridge, MA\nUnited States of America\nTrue\n1910\n\n\n95\n1919\nPeace\nThe Nobel Peace Prize 1919\nNaN\n1/1\n483\nIndividual\nThomas Woodrow Wilson\n1856-12-28\nStaunton, VA\nUnited States of America\nMale\nNaN\nNaN\nNaN\n1924-02-03\nWashington, DC\nUnited States of America\nTrue\n1910\n\n\n117\n1923\nPhysics\nThe Nobel Prize in Physics 1923\n\"for his work on the elementary charge of elec...\n1/1\n28\nIndividual\nRobert Andrews Millikan\n1868-03-22\nMorrison, IL\nUnited States of America\nMale\nCalifornia Institute of Technology (Caltech)\nPasadena, CA\nUnited States of America\n1953-12-19\nSan Marino, CA\nUnited States of America\nTrue\n1920\n\n\n124\n1925\nPeace\nThe Nobel Peace Prize 1925\nNaN\n1/2\n489\nIndividual\nCharles Gates Dawes\n1865-08-27\nMarietta, OH\nUnited States of America\nMale\nNaN\nNaN\nNaN\n1951-04-23\nEvanston, IL\nUnited States of America\nTrue\n1920\n\n\n138\n1927\nPhysics\nThe Nobel Prize in Physics 1927\n\"for his discovery of the effect named after him\"\n1/2\n33\nIndividual\nArthur Holly Compton\n1892-09-10\nWooster, OH\nUnited States of America\nMale\nUniversity of Chicago\nChicago, IL\nUnited States of America\n1962-03-15\nBerkeley, CA\nUnited States of America\nTrue\n1920\n\n\n149\n1929\nPeace\nThe Nobel Peace Prize 1929\nNaN\n1/1\n494\nIndividual\nFrank Billings Kellogg\n1856-12-22\nPotsdam, NY\nUnited States of America\nMale\nNaN\nNaN\nNaN\n1937-12-21\nSt. Paul, MN\nUnited States of America\nTrue\n1920\n\n\n152\n1930\nLiterature\nThe Nobel Prize in Literature 1930\n\"for his vigorous and graphic art of descripti...\n1/1\n603\nIndividual\nSinclair Lewis\n1885-02-07\nSauk Centre, MN\nUnited States of America\nMale\nNaN\nNaN\nNaN\n1951-01-10\nRome\nItaly\nTrue\n1930\n\n\n160\n1931\nPeace\nThe Nobel Peace Prize 1931\nNaN\n1/2\n496\nIndividual\nJane Addams\n1860-09-06\nCedarville, IL\nUnited States of America\nFemale\nNaN\nNaN\nNaN\n1935-05-21\nChicago, IL\nUnited States of America\nTrue\n1930\n\n\n\n\n\n\n\n\n\ndecade_sum_usa = USA_only.groupby(\"decade\")[\"USA_born_winers\"].agg(\"count\")\ndecade_sum_usa\n\ndecade\n1900     1\n1910     3\n1920     4\n1930    14\n1940    13\n1950    21\n1960    21\n1970    33\n1980    31\n1990    42\n2000    52\n2010    38\n2020    18\nName: USA_born_winers, dtype: int64\n\n\n\nUSA_decade_prop = pd.DataFrame(decade_sum_usa/decade_sum)\n\n\nmax_decade_usa=USA_decade_prop[USA_decade_prop[\"USA_born_winers\"]==USA_decade_prop[\"USA_born_winers\"].max()].index[0]\n\n\nmax_decade_usa\n\n2000",
    "crumbs": [
      "Project: A Visual History of Nobel Prize Winners"
    ]
  },
  {
    "objectID": "Project-A_Visual_History_of_Nobel_Prize_Winners.html#task-3",
    "href": "Project-A_Visual_History_of_Nobel_Prize_Winners.html#task-3",
    "title": "Project: A Visual History of Nobel Prize Winners",
    "section": "Task 3",
    "text": "Task 3\nWhat decade and category pair had the highest proportion of female laureates? Store this as a dictionary called max_female_dict where the decade is the key and the category is the value.\n\ndecade_N_cat = nobel.groupby([\"decade\",\"category\"])[\"prize\"].agg(\"count\")\ndecade_N_cat\n\ndecade  category  \n1900    Chemistry      9\n        Literature    10\n        Medicine      11\n        Peace         14\n        Physics       13\n                      ..\n2020    Economics      9\n        Literature     4\n        Medicine       8\n        Peace          7\n        Physics       12\nName: prize, Length: 72, dtype: int64\n\n\n\nnobel[\"male\"] = nobel[\"sex\"]==\"Male\"\nnobel_male_only=nobel[nobel[\"male\"]]\nnobel_male_only.shape\n\ndecade_N_cat_male = nobel_male_only.groupby([\"decade\",\"category\"])[\"prize\"].agg(\"count\")\ndecade_N_cat_male\n\ndecade  category  \n1900    Chemistry      9\n        Literature     9\n        Medicine      11\n        Peace         12\n        Physics       12\n                      ..\n2020    Economics      8\n        Literature     2\n        Medicine       7\n        Peace          2\n        Physics       10\nName: prize, Length: 72, dtype: int64\n\n\n\nmale_prop = decade_N_cat_male/decade_N_cat\nmale_prop\n\ndecade  category  \n1900    Chemistry     1.000000\n        Literature    0.900000\n        Medicine      1.000000\n        Peace         0.857143\n        Physics       0.923077\n                        ...   \n2020    Economics     0.888889\n        Literature    0.500000\n        Medicine      0.875000\n        Peace         0.285714\n        Physics       0.833333\nName: prize, Length: 72, dtype: float64\n\n\n\nfemale_prop = 1-male_prop\nfemale_prop\nnp.max(female_prop)\nmax_female_prop = female_prop.idxmax()\n\n\nmax_female_dict = dict([max_female_prop])\nmax_female_dict\n\n{2020: 'Peace'}\n\n\n\n# Calculating the proportion of female laureates per decade\nnobel['female_winner'] = nobel.sex=='Female'\nprop_female_winners = nobel.groupby(['decade','category'],as_index=False)['female_winner'].mean()\nprop_female_winners.sort_values(\"female_winner\",ascending=False,inplace=True)\nprop_female_winners\n\n\n\n\n\n\n\n\n\ndecade\ncategory\nfemale_winner\n\n\n\n\n68\n2020\nLiterature\n0.500000\n\n\n64\n2010\nPeace\n0.357143\n\n\n50\n1990\nLiterature\n0.300000\n\n\n56\n2000\nLiterature\n0.300000\n\n\n66\n2020\nChemistry\n0.300000\n\n\n...\n...\n...\n...\n\n\n34\n1960\nPeace\n0.000000\n\n\n37\n1970\nEconomics\n0.000000\n\n\n38\n1970\nLiterature\n0.000000\n\n\n41\n1970\nPhysics\n0.000000\n\n\n36\n1970\nChemistry\n0.000000\n\n\n\n\n72 rows × 3 columns\n\n\n\n\n\n#max_female_dict = dict([prop_female_winners[\"decade\"].iloc[0],])\n\n\ndecade=prop_female_winners[\"decade\"].iloc[0]\ncategory=prop_female_winners[\"category\"].iloc[0]\nprint(category)\nmax_female_dict = {decade:category}\nmax_female_dict\n\nLiterature\n\n\n{2020: 'Literature'}",
    "crumbs": [
      "Project: A Visual History of Nobel Prize Winners"
    ]
  },
  {
    "objectID": "Project-A_Visual_History_of_Nobel_Prize_Winners.html#task-4",
    "href": "Project-A_Visual_History_of_Nobel_Prize_Winners.html#task-4",
    "title": "Project: A Visual History of Nobel Prize Winners",
    "section": "Task 4",
    "text": "Task 4\nWho was the first woman to receive a Nobel Prize, and in what category? Save your string answers as first_woman_name and first_woman_category.\n\nnobel_female_only = nobel[nobel.female_winner==True]\nnobel_female_only.sort_values(\"year\",ascending=True,inplace=True)\nnobel_female_only\n\n/var/folders/53/yp3kynfd7rn5y13c2wwfm33rmgtrfb/T/ipykernel_38970/3855593822.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  nobel_female_only.sort_values(\"year\",ascending=True,inplace=True)\n\n\n\n\n\n\n\n\n\n\nyear\ncategory\nprize\nmotivation\nprize_share\nlaureate_id\nlaureate_type\nfull_name\nbirth_date\nbirth_city\n...\norganization_name\norganization_city\norganization_country\ndeath_date\ndeath_city\ndeath_country\nUSA_born_winers\ndecade\nmale\nfemale_winner\n\n\n\n\n19\n1903\nPhysics\nThe Nobel Prize in Physics 1903\n\"in recognition of the extraordinary services ...\n1/4\n6\nIndividual\nMarie Curie, née Sklodowska\n1867-11-07\nWarsaw\n...\nNaN\nNaN\nNaN\n1934-07-04\nSallanches\nFrance\nFalse\n1900\nFalse\nTrue\n\n\n29\n1905\nPeace\nThe Nobel Peace Prize 1905\nNaN\n1/1\n468\nIndividual\nBaroness Bertha Sophie Felicita von Suttner, n...\n1843-06-09\nPrague\n...\nNaN\nNaN\nNaN\n1914-06-21\nVienna\nAustria\nFalse\n1900\nFalse\nTrue\n\n\n51\n1909\nLiterature\nThe Nobel Prize in Literature 1909\n\"in appreciation of the lofty idealism, vivid ...\n1/1\n579\nIndividual\nSelma Ottilia Lovisa Lagerlöf\n1858-11-20\nMårbacka\n...\nNaN\nNaN\nNaN\n1940-03-16\nMårbacka\nSweden\nFalse\n1900\nFalse\nTrue\n\n\n62\n1911\nChemistry\nThe Nobel Prize in Chemistry 1911\n\"in recognition of her services to the advance...\n1/1\n6\nIndividual\nMarie Curie, née Sklodowska\n1867-11-07\nWarsaw\n...\nSorbonne University\nParis\nFrance\n1934-07-04\nSallanches\nFrance\nFalse\n1910\nFalse\nTrue\n\n\n128\n1926\nLiterature\nThe Nobel Prize in Literature 1926\n\"for her idealistically inspired writings whic...\n1/1\n597\nIndividual\nGrazia Deledda\n1871-09-27\nNuoro, Sardinia\n...\nNaN\nNaN\nNaN\n1936-08-15\nRome\nItaly\nFalse\n1920\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n982\n2022\nLiterature\nThe Nobel Prize in Literature 2022\n\"for the courage and clinical acuity with whic...\n1/1\n1017\nIndividual\nAnnie Ernaux\n1940-09-01\nLillebonne\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nFalse\n2020\nFalse\nTrue\n\n\n993\n2023\nPhysics\nThe Nobel Prize in Physics 2023\n\"for experimental methods that generate attose...\n1/3\n1028\nIndividual\nAnne L’Huillier\n1958-08-16\nParis\n...\nLund University\nLund\nSweden\nNaN\nNaN\nNaN\nFalse\n2020\nFalse\nTrue\n\n\n998\n2023\nPeace\nThe Nobel Peace Prize 2023\n\"for her fight against the oppression of women...\n1/1\n1033\nIndividual\nNarges Mohammadi\n1972-04-21\nZanjan\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nFalse\n2020\nFalse\nTrue\n\n\n989\n2023\nMedicine\nThe Nobel Prize in Physiology or Medicine 2023\n\"for their discoveries concerning nucleoside b...\n1/2\n1024\nIndividual\nKatalin Karikó\n1955-01-17\nSzolnok\n...\nSzeged University\nSzeged\nHungary\nNaN\nNaN\nNaN\nFalse\n2020\nFalse\nTrue\n\n\n999\n2023\nEconomics\nThe Sveriges Riksbank Prize in Economic Scienc...\n\"for having advanced our understanding of wome...\n1/1\n1034\nIndividual\nClaudia Goldin\n1946-00-00\nNew York, NY\n...\nHarvard University\nCambridge, MA\nUnited States of America\nNaN\nNaN\nNaN\nTrue\n2020\nFalse\nTrue\n\n\n\n\n65 rows × 22 columns\n\n\n\n\n\nfirst_woman_name=nobel_female_only[\"full_name\"].iloc[0]\nfirst_woman_name\n\n'Marie Curie, née Sklodowska'\n\n\n\nfirst_woman_category = nobel_female_only[\"category\"].iloc[0]\nfirst_woman_category\n\n'Physics'",
    "crumbs": [
      "Project: A Visual History of Nobel Prize Winners"
    ]
  },
  {
    "objectID": "Project-A_Visual_History_of_Nobel_Prize_Winners.html#task-5",
    "href": "Project-A_Visual_History_of_Nobel_Prize_Winners.html#task-5",
    "title": "Project: A Visual History of Nobel Prize Winners",
    "section": "Task 5",
    "text": "Task 5\nWhich individuals or organizations have won multiple Nobel Prizes throughout the years? Store the full names in a list named repeat_list.\n\nnobel_by_name = pd.DataFrame(nobel.groupby(\"full_name\")[\"year\"].agg(\"count\"))\nnobel_by_name.sort_values(\"year\",ascending=False,inplace=True)\n\n\nnobel_by_name=nobel_by_name[nobel_by_name[\"year\"]&gt;=2]\nnobel_by_name\n\n\n\n\n\n\n\n\n\nyear\n\n\nfull_name\n\n\n\n\n\nComité international de la Croix Rouge (International Committee of the Red Cross)\n3\n\n\nOffice of the United Nations High Commissioner for Refugees (UNHCR)\n2\n\n\nFrederick Sanger\n2\n\n\nLinus Carl Pauling\n2\n\n\nJohn Bardeen\n2\n\n\nMarie Curie, née Sklodowska\n2\n\n\n\n\n\n\n\n\n\nrepeat_list = list(nobel_by_name.index)\n\n\nrepeat_list\n\n['Comité international de la Croix Rouge (International Committee of the Red Cross)',\n 'Office of the United Nations High Commissioner for Refugees (UNHCR)',\n 'Frederick Sanger',\n 'Linus Carl Pauling',\n 'John Bardeen',\n 'Marie Curie, née Sklodowska']",
    "crumbs": [
      "Project: A Visual History of Nobel Prize Winners"
    ]
  },
  {
    "objectID": "Project-Netflix_movies.html",
    "href": "Project-Netflix_movies.html",
    "title": "Project: Investigating Netflix Movies",
    "section": "",
    "text": "The data",
    "crumbs": [
      "Project: Investigating Netflix Movies"
    ]
  },
  {
    "objectID": "Project-Netflix_movies.html#the-data",
    "href": "Project-Netflix_movies.html#the-data",
    "title": "Project: Investigating Netflix Movies",
    "section": "",
    "text": "netflix_data.csv\n\n\n\nColumn\nDescription\n\n\n\n\nshow_id\nThe ID of the show\n\n\ntype\nType of show\n\n\ntitle\nTitle of the show\n\n\ndirector\nDirector of the show\n\n\ncast\nCast of the show\n\n\ncountry\nCountry of origin\n\n\ndate_added\nDate added to Netflix\n\n\nrelease_year\nYear of Netflix release\n\n\nduration\nDuration of the show in minutes\n\n\ndescription\nDescription of the show\n\n\ngenre\nShow genre\n\n\n\n\n\nImporting pandas and matplotlib\n\n# Importing pandas and matplotlib\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Start coding!\n\n\n\nLoad the CSV file and store as netflix_df.\n\n#Load the CSV file and store as netflix_df.\nnetflix_df = pd.read_csv(\"datasets/netflix_data.csv\")\nnetflix_df.head(2)\n\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nduration\ndescription\ngenre\n\n\n\n\n0\ns1\nTV Show\n3%\nNaN\nJoão Miguel, Bianca Comparato, Michel Gomes, R...\nBrazil\nAugust 14, 2020\n2020\n4\nIn a future where the elite inhabit an island ...\nInternational TV\n\n\n1\ns2\nMovie\n7:19\nJorge Michel Grau\nDemián Bichir, Héctor Bonilla, Oscar Serrano, ...\nMexico\nDecember 23, 2016\n2016\n93\nAfter a devastating earthquake hits Mexico Cit...\nDramas\n\n\n\n\n\n\n\n\n\n\nFilter the data to remove TV shows and store as netflix_subset\n\n#Filter the data to remove TV shows and store as netflix_subset\nnetflix_subset = netflix_df[netflix_df.type!='TV Show']\nnetflix_subset.head()\n\n\n\n\n\n\n\n\n\nshow_id\ntype\ntitle\ndirector\ncast\ncountry\ndate_added\nrelease_year\nduration\ndescription\ngenre\n\n\n\n\n1\ns2\nMovie\n7:19\nJorge Michel Grau\nDemián Bichir, Héctor Bonilla, Oscar Serrano, ...\nMexico\nDecember 23, 2016\n2016\n93\nAfter a devastating earthquake hits Mexico Cit...\nDramas\n\n\n2\ns3\nMovie\n23:59\nGilbert Chan\nTedd Chan, Stella Chung, Henley Hii, Lawrence ...\nSingapore\nDecember 20, 2018\n2011\n78\nWhen an army recruit is found dead, his fellow...\nHorror Movies\n\n\n3\ns4\nMovie\n9\nShane Acker\nElijah Wood, John C. Reilly, Jennifer Connelly...\nUnited States\nNovember 16, 2017\n2009\n80\nIn a postapocalyptic world, rag-doll robots hi...\nAction\n\n\n4\ns5\nMovie\n21\nRobert Luketic\nJim Sturgess, Kevin Spacey, Kate Bosworth, Aar...\nUnited States\nJanuary 1, 2020\n2008\n123\nA brilliant group of students become card-coun...\nDramas\n\n\n6\ns7\nMovie\n122\nYasir Al Yasiri\nAmina Khalil, Ahmed Dawood, Tarek Lotfy, Ahmed...\nEgypt\nJune 1, 2020\n2019\n95\nAfter an awful accident, a couple admitted to ...\nHorror Movies\n\n\n\n\n\n\n\n\n\n\nInvestigate the Netflix movie data, keeping only the columns “title”, “country”, “genre”, “release_year”, “duration”, and saving this into a new DataFrame called netflix_movies.\n\n#Investigate the Netflix movie data, keeping only the columns \"title\", \"country\", \"genre\", \"release_year\", \"duration\", and saving this into a new DataFrame called netflix_movies.\nnetflix_movies = netflix_subset[[\"title\", \"country\", \"genre\", \"release_year\", \"duration\"]]\nnetflix_movies.head(10)\n\n\n\n\n\n\n\n\n\ntitle\ncountry\ngenre\nrelease_year\nduration\n\n\n\n\n1\n7:19\nMexico\nDramas\n2016\n93\n\n\n2\n23:59\nSingapore\nHorror Movies\n2011\n78\n\n\n3\n9\nUnited States\nAction\n2009\n80\n\n\n4\n21\nUnited States\nDramas\n2008\n123\n\n\n6\n122\nEgypt\nHorror Movies\n2019\n95\n\n\n7\n187\nUnited States\nDramas\n1997\n119\n\n\n8\n706\nIndia\nHorror Movies\n2019\n118\n\n\n9\n1920\nIndia\nHorror Movies\n2008\n143\n\n\n10\n1922\nUnited States\nDramas\n2017\n103\n\n\n13\n2,215\nThailand\nDocumentaries\n2018\n89\n\n\n\n\n\n\n\n\n\n\nFilter netflix_movies to find the movies that are shorter than 60 minutes, saving the resulting DataFrame as short_movies; inspect the result to find possible contributing factors.\n\n#Filter netflix_movies to find the movies that are shorter than 60 minutes, saving the resulting DataFrame as short_movies; inspect the result to find possible contributing factors.\nshort_movies = netflix_movies[netflix_movies.duration&lt;60]\nshort_movies.head(2)\nprint(short_movies.shape)\n\n(420, 5)\n\n\n\nshort_movies.columns\n\nIndex(['title', 'country', 'genre', 'release_year', 'duration'], dtype='object')\n\n\n\nsns.countplot(y='genre', data=short_movies)\n\n\n\n\n\n\n\n\nGenre of the top four movies are Documentaries, Children, Standup and Uncategorized\n\n\nUsing a for loop and if/elif statements, iterate through the rows of netflix_movies and assign colors of your choice to four genre groups (“Children”, “Documentaries”, “Stand-Up”, and “Other” for everything else). Save the results in a colors list.\n\n#Using a for loop and if/elif statements, iterate through the rows of netflix_movies and assign colors of your choice to four genre groups (\"Children\", \"Documentaries\", \"Stand-Up\", and \"Other\" for everything else). Save the results in a colors list. \"\ncolors = []\n\nfor index, row in netflix_movies.iterrows():\n    genre = row['genre']\n    if genre == \"Children\":\n        colors.append(\"blue\")  # Assigning blue for Children genre\n    elif genre == \"Documentaries\":\n        colors.append(\"green\")  # Assigning green for Documentaries genre\n    elif genre == \"Stand-Up\":\n        colors.append(\"yellow\")  # Assigning yellow for Stand-Up genre\n    else:\n        colors.append(\"red\")  # Assigning red for Other genres\n\n#print(colors)\n\n\n\nInitialize a figure object called fig and create a scatter plot for movie duration by release year using the colors list to color the points and using the labels “Release year” for the x-axis, “Duration (min)” for the y-axis, and the title “Movie Duration by Year of Release\n\n#Initialize a figure object called fig and create a scatter plot for movie duration by release year using the colors list to color the points and using the labels \"Release year\" for the x-axis, \"Duration (min)\" for the y-axis, and the title \"Movie Duration by Year of Release\nfig, ax = plt.subplots()\nax.scatter(netflix_movies['release_year'],netflix_movies['duration'],color=colors)\nax.set_xlabel(\"Release year\")\nax.set_ylabel(\"Duration (min)\")\nax.set_title(\"Movie Duration by Year of Release\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nanswer=\"maybe\"",
    "crumbs": [
      "Project: Investigating Netflix Movies"
    ]
  },
  {
    "objectID": "Project-Exploring_NYC_Public_School_Test_Result_Scores.html",
    "href": "Project-Exploring_NYC_Public_School_Test_Result_Scores.html",
    "title": "Project: Exploring NYC Public School Test Result Scores",
    "section": "",
    "text": "Task 1\nCreate a pandas DataFrame called best_math_schools containing the “school_name” and “average_math” score for all schools where the results are at least 80% of the maximum possible score, sorted by “average_math” in descending order.",
    "crumbs": [
      "Project: Exploring NYC Public School Test Result Scores"
    ]
  },
  {
    "objectID": "Project-Exploring_NYC_Public_School_Test_Result_Scores.html#task-1",
    "href": "Project-Exploring_NYC_Public_School_Test_Result_Scores.html#task-1",
    "title": "Project: Exploring NYC Public School Test Result Scores",
    "section": "",
    "text": "math_scores = schools[[\"school_name\",\"average_math\"]]\nmath_scores = math_scores[math_scores[\"average_math\"]&gt;=(800*.8)]\nprint(math_scores.shape)\nbest_math_schools = math_scores.sort_values(\"average_math\",ascending=False)\nprint(best_math_schools)\n(10, 2)\n                                           school_name  average_math\n88                              Stuyvesant High School           754\n170                       Bronx High School of Science           714\n93                 Staten Island Technical High School           711\n365  Queens High School for the Sciences at York Co...           701\n68   High School for Mathematics, Science, and Engi...           683\n280                     Brooklyn Technical High School           682\n333                        Townsend Harris High School           680\n174  High School of American Studies at Lehman College           669\n0    New Explorations into Science, Technology and ...           657\n45                       Eleanor Roosevelt High School           641",
    "crumbs": [
      "Project: Exploring NYC Public School Test Result Scores"
    ]
  },
  {
    "objectID": "Project-Exploring_NYC_Public_School_Test_Result_Scores.html#task-2",
    "href": "Project-Exploring_NYC_Public_School_Test_Result_Scores.html#task-2",
    "title": "Project: Exploring NYC Public School Test Result Scores",
    "section": "Task 2",
    "text": "Task 2\nIdentify the top 10 performing schools based on scores across the three SAT sections, storing as a pandas DataFrame called top_10_schools containing the school name and a column named “total_SAT”, with results sorted by total_SAT in descending order.\nschools['total_SAT'] = schools.loc[:,[\"average_math\",\"average_reading\",\"average_writing\"]].sum(axis=1)\nschools.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nschool_name\nborough\nbuilding_code\naverage_math\naverage_reading\naverage_writing\npercent_tested\ntotal_SAT\n\n\n\n\n0\nNew Explorations into Science, Technology and ...\nManhattan\nM022\n657\n601\n601\nNaN\n1859\n\n\n1\nEssex Street Academy\nManhattan\nM445\n395\n411\n387\n78.9\n1193\n\n\n2\nLower Manhattan Arts Academy\nManhattan\nM445\n418\n428\n415\n65.1\n1261\n\n\n3\nHigh School for Dual Language and Asian Studies\nManhattan\nM445\n613\n453\n463\n95.9\n1529\n\n\n4\nHenry Street School for International Studies\nManhattan\nM056\n410\n406\n381\n59.7\n1197\n\n\n\n\ntop_10_schools = schools.sort_values(\"total_SAT\",ascending=False)\ntop_10_schools = top_10_schools.iloc[0:10][[\"school_name\",\"total_SAT\"]]\ntop_10_schools.shape\n(10, 2)",
    "crumbs": [
      "Project: Exploring NYC Public School Test Result Scores"
    ]
  },
  {
    "objectID": "Project-Exploring_NYC_Public_School_Test_Result_Scores.html#task-3",
    "href": "Project-Exploring_NYC_Public_School_Test_Result_Scores.html#task-3",
    "title": "Project: Exploring NYC Public School Test Result Scores",
    "section": "Task 3",
    "text": "Task 3\nLocate the NYC borough with the largest standard deviation for “total_SAT”, storing as a DataFrame called largest_std_dev with “borough” as the index and three columns: “num_schools” for the number of schools in the borough, “average_SAT” for the mean of “total_SAT”, and “std_SAT” for the standard deviation of “total_SAT”. Round all numeric values to two decimal places.\nschools.borough.unique()\narray(['Manhattan', 'Staten Island', 'Bronx', 'Queens', 'Brooklyn'],\n      dtype=object)\n\nschools_std_dev = schools.groupby(\"borough\")[\"total_SAT\"].agg(['count','mean','std']).round(2).rename(columns={\"count\":\"num_schools\",\"mean\":\"average_SAT\",\"std\":\"std_SAT\"})\nschools_std_dev\n\n\n\n\n\n\nnum_schools\naverage_SAT\nstd_SAT\n\n\n\n\nborough\n\n\n\n\n\nBronx\n98\n1202.72\n150.39\n\n\nBrooklyn\n109\n1230.26\n154.87\n\n\nManhattan\n89\n1340.13\n230.29\n\n\nQueens\n69\n1345.48\n195.25\n\n\nStaten Island\n10\n1439.00\n222.30\n\n\n\n\nlargest_std_dev = schools_std_dev.sort_values(\"std_SAT\",ascending=False).iloc[[0]]\nlargest_std_dev\n\n\n\n\n\n\nnum_schools\naverage_SAT\nstd_SAT\n\n\n\n\nborough\n\n\n\n\n\nManhattan\n89\n1340.13\n230.29",
    "crumbs": [
      "Project: Exploring NYC Public School Test Result Scores"
    ]
  },
  {
    "objectID": "Project-Hypothesis_Testing_in_Healthcare.html",
    "href": "Project-Hypothesis_Testing_in_Healthcare.html",
    "title": "Project: Hypothesis Testing in Healthcare- Drug Safety",
    "section": "",
    "text": "Your task",
    "crumbs": [
      "Project: Hypothesis Testing in Healthcare- Drug Safety"
    ]
  },
  {
    "objectID": "Project-Hypothesis_Testing_in_Healthcare.html#your-task",
    "href": "Project-Hypothesis_Testing_in_Healthcare.html#your-task",
    "title": "Project: Hypothesis Testing in Healthcare- Drug Safety",
    "section": "",
    "text": "Determine if the proportion of adverse effects differs significantly between the Drug and Placebo groups, saving the p-value as a variable called two_sample_p_value.\nFind out if the number of adverse effects is independent of the treatment and control groups, saving as a variable called num_effects_p_value containing a p-value.\nExamine if there is a significant difference between the ages of the Drug and Placebo groups, storing the p-value of your test in a variable called age_group_effects_p_value.\n\n# Import packages\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.proportion import proportions_ztest\nimport pingouin\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndrug_safety = pd.read_csv(\"drug_safety.csv\")\n\n# Start coding here...\ndrug_safety.tail(20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\ntrx\nweek\nwbc\nrbc\nadverse_effects\nnum_effects\n\n\n\n\n16083\n62\nfemale\nPlacebo\n8\n4.7\n3.6\nNo\n0\n\n\n16084\n58\nmale\nPlacebo\n0\n9.1\n5.1\nNo\n0\n\n\n16085\n58\nmale\nPlacebo\n1\nNaN\nNaN\nYes\n1\n\n\n16086\n58\nmale\nPlacebo\n12\n7.2\n5.2\nNo\n0\n\n\n16087\n58\nmale\nPlacebo\n16\nNaN\nNaN\nYes\n1\n\n\n16088\n58\nmale\nPlacebo\n2\n6.9\n5.1\nNo\n0\n\n\n16089\n58\nmale\nPlacebo\n20\nNaN\nNaN\nYes\n1\n\n\n16090\n58\nmale\nPlacebo\n4\n10.3\n5.3\nYes\n1\n\n\n16091\n58\nmale\nPlacebo\n8\n6.4\n5.1\nNo\n0\n\n\n16092\n68\nmale\nDrug\n0\n5.7\n4.6\nNo\n0\n\n\n16093\n68\nmale\nDrug\n1\nNaN\nNaN\nYes\n1\n\n\n16094\n68\nmale\nDrug\n2\nNaN\nNaN\nNo\n0\n\n\n16095\n78\nmale\nPlacebo\n0\n7.2\n5.0\nNo\n0\n\n\n16096\n78\nmale\nPlacebo\n1\nNaN\nNaN\nYes\n1\n\n\n16097\n78\nmale\nPlacebo\n12\n6.5\n4.9\nNo\n0\n\n\n16098\n78\nmale\nPlacebo\n16\nNaN\nNaN\nYes\n1\n\n\n16099\n78\nmale\nPlacebo\n2\n7.5\n4.9\nNo\n0\n\n\n16100\n78\nmale\nPlacebo\n20\nNaN\nNaN\nYes\n1\n\n\n16101\n78\nmale\nPlacebo\n4\n6.4\n4.8\nNo\n0\n\n\n16102\n78\nmale\nPlacebo\n8\n7.8\n4.8\nNo\n0\n\n\n\n\ndrug_safety.columns\nIndex(['age', 'sex', 'trx', 'week', 'wbc', 'rbc', 'adverse_effects',\n       'num_effects'],\n      dtype='object')\ndrug_safety.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16103 entries, 0 to 16102\nData columns (total 8 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   age              16103 non-null  int64  \n 1   sex              16103 non-null  object \n 2   trx              16103 non-null  object \n 3   week             16103 non-null  int64  \n 4   wbc              9128 non-null   float64\n 5   rbc              9127 non-null   float64\n 6   adverse_effects  16103 non-null  object \n 7   num_effects      16103 non-null  int64  \ndtypes: float64(2), int64(3), object(3)\nmemory usage: 1006.6+ KB\ndrug_safety.describe()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nage\nweek\nwbc\nrbc\nnum_effects\n\n\n\n\ncount\n16103.000000\n16103.00000\n9128.000000\n9127.000000\n16103.000000\n\n\nmean\n64.117556\n7.74098\n7.340557\n4.672784\n0.101596\n\n\nstd\n8.783207\n6.94350\n1.996652\n0.458520\n0.323181\n\n\nmin\n39.000000\n0.00000\n1.800000\n2.100000\n0.000000\n\n\n25%\n58.000000\n1.00000\n6.000000\n4.400000\n0.000000\n\n\n50%\n65.000000\n4.00000\n7.100000\n4.700000\n0.000000\n\n\n75%\n71.000000\n12.00000\n8.400000\n5.000000\n0.000000\n\n\nmax\n84.000000\n20.00000\n26.500000\n7.600000\n3.000000\n\n\n\n\n\nDetermine if the proportion of adverse effects differs significantly between the Drug and Placebo groups, saving the p-value as a variable called two_sample_p_value.\nimport pandas as pd\nfrom statsmodels.stats.proportion import proportions_ztest\n\n# Creating a contingency table of adverse effects by treatment groups\ncontingency_table = pd.crosstab(drug_safety['adverse_effects'], drug_safety['trx'])\n#print(contingency_table)\n# Extracting the counts of adverse effects for Drug and Placebo groups\nadverse_effects_drug = contingency_table.loc['Yes', 'Drug']  # Assuming '1' represents presence of adverse effects\nadverse_effects_placebo = contingency_table.loc['Yes', 'Placebo']\n# Extracting the total counts for Drug and Placebo groups\ntotal_drug = contingency_table['Drug'].sum()\ntotal_placebo = contingency_table['Placebo'].sum()\n# Performing the two-sample proportions z-test\ncount = [adverse_effects_drug, adverse_effects_placebo]\nnobs = [total_drug, total_placebo]\n\ntwo_sample_z_stat, two_sample_p_value = proportions_ztest(count, nobs)\n\n# Display the p-value\nprint(f\"The p-value for the two-sample proportions z-test is: {two_sample_p_value}\")\nThe p-value for the two-sample proportions z-test is: 0.9639333330262475\ntwo_sample_p_value\n0.9639333330262475\n\n\nFind out if the number of adverse effects is independent of the treatment and control groups, saving as a variable called num_effects_p_value containing a p-value.\nexpected,observed,stats = pingouin.chi2_independence(data=drug_safety,x='num_effects',y='trx')\nnum_effects_p_value = stats[\"pval\"][0]\n#print(stats)",
    "crumbs": [
      "Project: Hypothesis Testing in Healthcare- Drug Safety"
    ]
  },
  {
    "objectID": "Project-Hypothesis_Testing_in_Healthcare.html#inspecting-whether-age-is-normally-distributed",
    "href": "Project-Hypothesis_Testing_in_Healthcare.html#inspecting-whether-age-is-normally-distributed",
    "title": "Project: Hypothesis Testing in Healthcare- Drug Safety",
    "section": "Inspecting whether age is normally distributed",
    "text": "Inspecting whether age is normally distributed\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Creating a histogram to visualize age distribution by treatment groups\nplt.figure(figsize=(8, 6))\nsns.histplot(data=drug_safety, x='age', hue='trx', kde=True)\nplt.title('Distribution of Age by Treatment Groups')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.legend(title='Treatment')\nplt.show()\n\n\n\npng\n\n\n# Performing Shapiro-Wilk test for normality by treatment groups (trx)\nshapiro_results = pg.normality(data=drug_safety, dv='age', group='trx')\n\n# Displaying Shapiro-Wilk test results\nprint(shapiro_results)\n                W          pval  normal\ntrx                                    \nDrug     0.976785  2.189152e-38   False\nPlacebo  0.975595  2.224950e-29   False\n\nExamine if there is a significant difference between the ages of the Drug and Placebo groups, storing the p-value of your test in a variable called age_group_effects_p_value.\nSignificant difference between the ages of both groups\nTo ensure age wasn’t a confounder, conduct a Mann-Whitney test to determine if age differed significantly between the trx groups.\ndata = drug_safety\n# Filtering ages for 'Drug' group\nage_drug = data.loc[data['trx'] == 'Drug', 'age']\n\n# Filtering ages for 'Placebo' group\nage_placebo = data.loc[data['trx'] == 'Placebo', 'age']\n\n# Performing Mann-Whitney U test for age between 'Drug' and 'Placebo' groups\nmann_whitney_result = pg.mwu(age_drug, age_placebo)\n\n# Extracting p-value from the Mann-Whitney U test result DataFrame\nage_group_effects_p_value = mann_whitney_result['p-val']\n\n# Displaying the p-value\nprint(f\"P-value for Mann-Whitney U test between 'Drug' and 'Placebo' groups: {age_group_effects_p_value}\")\nP-value for Mann-Whitney U test between 'Drug' and 'Placebo' groups: MWU    0.256963\nName: p-val, dtype: float64",
    "crumbs": [
      "Project: Hypothesis Testing in Healthcare- Drug Safety"
    ]
  },
  {
    "objectID": "Project-Predictive_Modeling_for_Agriculture.html",
    "href": "Project-Predictive_Modeling_for_Agriculture.html",
    "title": "Project: Sowing Success - How Machine Learning Helps Farmers Select the Best Crops",
    "section": "",
    "text": "Instructions\nBuild a multi-class Logistic Regression model to predict categories of “crop” with a F1 score of more than 0.5.\nSplit the data into training and test sets, setting test_size equal to 20% and using a random_state of 42.\nPredict the “crop” type using each feature individually by looping over all the features, and, for each feature, fit a Logistic Regression model and calculate f1_score(). When creating the model, set max_iter to 2000 so the model can converge, and pass an appropriate string value to the multi_class keyword argument.\nIn order to avoid selecting two features that are highly correlated, perform a correlation analysis for each pair of features, enabling you to build a final model without the presence of multicollinearity.\nFeature ‘K’ and ‘P’ are highly co-related. Dropping ‘P’",
    "crumbs": [
      "Project: Sowing Success - How Machine Learning Helps Farmers Select the Best Crops"
    ]
  },
  {
    "objectID": "Project-Predictive_Modeling_for_Agriculture.html#instructions",
    "href": "Project-Predictive_Modeling_for_Agriculture.html#instructions",
    "title": "Project: Sowing Success - How Machine Learning Helps Farmers Select the Best Crops",
    "section": "",
    "text": "Read in soil_measures.csv as a pandas DataFrame and perform some data checks, such as determining the number of crops, checking for missing values, and verifying that the data in each potential feature column is numeric.\nSplit the data into training and test sets, setting test_size equal to 20% and using a random_state of 42.\nPredict the “crop” type using each feature individually by looping over all the features, and, for each feature, fit a Logistic Regression model and calculate f1_score(). When creating the model, set max_iter to 2000 so the model can converge, and pass an appropriate string value to the multi_class keyword argument.\nIn order to avoid selecting two features that are highly correlated, perform a correlation analysis for each pair of features, enabling you to build a final model without the presence of multicollinearity.\nOnce you have your final features, train and test a new Logistic Regression model called log_reg, then evaluate performance using f1_score(), saving the metric as a variable called model_performance.\n\n# All required libraries are imported here for you.\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\n\n# Load the dataset\ncrops = pd.read_csv(\"datasets/soil_measures.csv\")\n\n# Write your code here\n#Individual crops\ncrop=crops['crop'].unique()\nprint(len(crop))\n22\n\ny = crops['crop']\nX = crops.drop(\"crop\", axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\nlogreg = LogisticRegression(multi_class='multinomial', max_iter=2000)\nfor c in X_train.columns:\n    dt_train = X_train[[c]]  # Selecting the current feature for training\n    dt_test = X_test[[c]]    # Selecting the current feature for testing\n    \n    logreg.fit(dt_train, y_train)\n    y_pred = logreg.predict(dt_test)\n    \n    f1 = f1_score(y_test, y_pred, average='weighted')\n    print(f\"F1 score using feature '{c}': {f1}\")\nF1 score using feature 'N': 0.10507916708090527\nF1 score using feature 'P': 0.10457380486654515\nF1 score using feature 'K': 0.2007873036107074\nF1 score using feature 'ph': 0.04532731061152114\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate the correlation matrix\ncorrelation_matrix = X.corr()\n\n# Plotting the correlation heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap of Features')\nplt.show()\n\n\n\npng\n\n\n\nfinal_features = ['N', 'K', 'ph']\nX = X[final_features]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n\nlog_reg = LogisticRegression(multi_class='multinomial', max_iter=2000)\nlog_reg.fit(X_train, y_train)\ny_pred = log_reg.predict(X_test)\nmodel_performance = f1_score(y_test, y_pred, average='weighted')\nmodel_performance\n0.558010495235685",
    "crumbs": [
      "Project: Sowing Success - How Machine Learning Helps Farmers Select the Best Crops"
    ]
  },
  {
    "objectID": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html",
    "href": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html",
    "title": "Project: Customer Analytics- Preparing Data for Modeling",
    "section": "",
    "text": "Task 1\nColumns containing integers must be stored as 32-bit integers (int32).",
    "crumbs": [
      "Project: Customer Analytics- Preparing Data for Modeling"
    ]
  },
  {
    "objectID": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html#task-1",
    "href": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html#task-1",
    "title": "Project: Customer Analytics- Preparing Data for Modeling",
    "section": "",
    "text": "int_columns = ds_jobs.select_dtypes(include='int64').columns.tolist()\nint_columns\n['student_id', 'training_hours', 'job_change']\nds_jobs[int_columns]=ds_jobs[int_columns].astype('int32')\nds_jobs.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19158 entries, 0 to 19157\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   student_id              19158 non-null  int32  \n 1   city                    19158 non-null  object \n 2   city_development_index  19158 non-null  float64\n 3   gender                  14650 non-null  object \n 4   relevant_experience     19158 non-null  object \n 5   enrolled_university     18772 non-null  object \n 6   education_level         18698 non-null  object \n 7   major_discipline        16345 non-null  object \n 8   experience              19093 non-null  object \n 9   company_size            13220 non-null  object \n 10  company_type            13018 non-null  object \n 11  last_new_job            18735 non-null  object \n 12  training_hours          19158 non-null  int32  \n 13  job_change              19158 non-null  int32  \ndtypes: float64(1), int32(3), object(10)\nmemory usage: 1.8+ MB",
    "crumbs": [
      "Project: Customer Analytics- Preparing Data for Modeling"
    ]
  },
  {
    "objectID": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html#task-2",
    "href": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html#task-2",
    "title": "Project: Customer Analytics- Preparing Data for Modeling",
    "section": "Task 2",
    "text": "Task 2\nColumns containing floats must be stored as 16-bit floats (float16).\nfloat_columns = ds_jobs.select_dtypes(include='float64').columns.tolist()\nds_jobs[float_columns]=ds_jobs[float_columns].astype(\"float16\")\nds_jobs.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19158 entries, 0 to 19157\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   student_id              19158 non-null  int32  \n 1   city                    19158 non-null  object \n 2   city_development_index  19158 non-null  float16\n 3   gender                  14650 non-null  object \n 4   relevant_experience     19158 non-null  object \n 5   enrolled_university     18772 non-null  object \n 6   education_level         18698 non-null  object \n 7   major_discipline        16345 non-null  object \n 8   experience              19093 non-null  object \n 9   company_size            13220 non-null  object \n 10  company_type            13018 non-null  object \n 11  last_new_job            18735 non-null  object \n 12  training_hours          19158 non-null  int32  \n 13  job_change              19158 non-null  int32  \ndtypes: float16(1), int32(3), object(10)\nmemory usage: 1.7+ MB",
    "crumbs": [
      "Project: Customer Analytics- Preparing Data for Modeling"
    ]
  },
  {
    "objectID": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html#task-3",
    "href": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html#task-3",
    "title": "Project: Customer Analytics- Preparing Data for Modeling",
    "section": "Task 3",
    "text": "Task 3\nColumns containing nominal categorical data must be stored as the category data type.\nds_jobs.columns\nIndex(['student_id', 'city', 'city_development_index', 'gender',\n       'relevant_experience', 'enrolled_university', 'education_level',\n       'major_discipline', 'experience', 'company_size', 'company_type',\n       'last_new_job', 'training_hours', 'job_change'],\n      dtype='object')\nds_jobs.head(6)\n\n\n\n\n\n\n\n\n\n\n\nstudent_id\n\n\n\n\ncity\n\n\n\n\ncity_development_index\n\n\n\n\ngender\n\n\n\n\nrelevant_experience\n\n\n\n\nenrolled_university\n\n\n\n\neducation_level\n\n\n\n\nmajor_discipline\n\n\n\n\nexperience\n\n\n\n\ncompany_size\n\n\n\n\ncompany_type\n\n\n\n\nlast_new_job\n\n\n\n\ntraining_hours\n\n\n\n\njob_change\n\n\n\n\n\n\n\n\n0\n\n\n\n\n8949\n\n\n\n\ncity_103\n\n\n\n\n0.919922\n\n\n\n\nMale\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n&gt;20\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n1\n\n\n\n\n36\n\n\n\n\n1\n\n\n\n\n\n\n1\n\n\n\n\n29725\n\n\n\n\ncity_40\n\n\n\n\n0.775879\n\n\n\n\nMale\n\n\n\n\nNo relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n15\n\n\n\n\n50-99\n\n\n\n\nPvt Ltd\n\n\n\n\n&gt;4\n\n\n\n\n47\n\n\n\n\n0\n\n\n\n\n\n\n2\n\n\n\n\n11561\n\n\n\n\ncity_21\n\n\n\n\n0.624023\n\n\n\n\nNaN\n\n\n\n\nNo relevant experience\n\n\n\n\nFull time course\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n5\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nnever\n\n\n\n\n83\n\n\n\n\n0\n\n\n\n\n\n\n3\n\n\n\n\n33241\n\n\n\n\ncity_115\n\n\n\n\n0.789062\n\n\n\n\nNaN\n\n\n\n\nNo relevant experience\n\n\n\n\nNaN\n\n\n\n\nGraduate\n\n\n\n\nBusiness Degree\n\n\n\n\n&lt;1\n\n\n\n\nNaN\n\n\n\n\nPvt Ltd\n\n\n\n\nnever\n\n\n\n\n52\n\n\n\n\n1\n\n\n\n\n\n\n4\n\n\n\n\n666\n\n\n\n\ncity_162\n\n\n\n\n0.767090\n\n\n\n\nMale\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nMasters\n\n\n\n\nSTEM\n\n\n\n\n&gt;20\n\n\n\n\n50-99\n\n\n\n\nFunded Startup\n\n\n\n\n4\n\n\n\n\n8\n\n\n\n\n0\n\n\n\n\n\n\n5\n\n\n\n\n21651\n\n\n\n\ncity_176\n\n\n\n\n0.764160\n\n\n\n\nNaN\n\n\n\n\nHas relevant experience\n\n\n\n\nPart time course\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n11\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\n1\n\n\n\n\n24\n\n\n\n\n1\n\n\n\n\n\n\nnominal_cat_columns=['city', 'gender','enrolled_university', 'major_discipline', 'company_type']\nds_jobs[nominal_cat_columns]=ds_jobs[nominal_cat_columns].astype(\"category\")\nds_jobs.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19158 entries, 0 to 19157\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype   \n---  ------                  --------------  -----   \n 0   student_id              19158 non-null  int32   \n 1   city                    19158 non-null  category\n 2   city_development_index  19158 non-null  float16 \n 3   gender                  14650 non-null  category\n 4   relevant_experience     19158 non-null  object  \n 5   enrolled_university     18772 non-null  category\n 6   education_level         18698 non-null  object  \n 7   major_discipline        16345 non-null  category\n 8   experience              19093 non-null  object  \n 9   company_size            13220 non-null  object  \n 10  company_type            13018 non-null  category\n 11  last_new_job            18735 non-null  object  \n 12  training_hours          19158 non-null  int32   \n 13  job_change              19158 non-null  int32   \ndtypes: category(5), float16(1), int32(3), object(5)\nmemory usage: 1.1+ MB",
    "crumbs": [
      "Project: Customer Analytics- Preparing Data for Modeling"
    ]
  },
  {
    "objectID": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html#task-4",
    "href": "Project-Customer_Analytics_Preparing_Data_for_Modeling.html#task-4",
    "title": "Project: Customer Analytics- Preparing Data for Modeling",
    "section": "Task 4",
    "text": "Task 4\nColumns containing ordinal categorical data must be stored as ordered categories, and not mapped to numerical values, with an order that reflects the natural order of the column.\nds_jobs[\"education_level\"].unique()\narray(['Graduate', 'Masters', 'High School', nan, 'Phd', 'Primary School'],\n      dtype=object)\neducation_order=['Primary School','High School','Graduate', 'Masters',  'Phd', 'nan' ]\nds_jobs[\"education_level\"]=pd.Categorical(ds_jobs[\"education_level\"],ordered=True,categories=education_order)\nds_jobs.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19158 entries, 0 to 19157\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype   \n---  ------                  --------------  -----   \n 0   student_id              19158 non-null  int32   \n 1   city                    19158 non-null  category\n 2   city_development_index  19158 non-null  float16 \n 3   gender                  14650 non-null  category\n 4   relevant_experience     19158 non-null  object  \n 5   enrolled_university     18772 non-null  category\n 6   education_level         18698 non-null  category\n 7   major_discipline        16345 non-null  category\n 8   experience              19093 non-null  object  \n 9   company_size            13220 non-null  object  \n 10  company_type            13018 non-null  category\n 11  last_new_job            18735 non-null  object  \n 12  training_hours          19158 non-null  int32   \n 13  job_change              19158 non-null  int32   \ndtypes: category(6), float16(1), int32(3), object(4)\nmemory usage: 978.9+ KB\nds_jobs[\"relevant_experience\"].unique()\norder=['No relevant experience','Has relevant experience']\nds_jobs[\"relevant_experience\"]=pd.Categorical(ds_jobs[\"relevant_experience\"],ordered=True,categories=order)\nds_jobs[\"enrolled_university\"].unique()\norder=['no_enrollment','Part time course','Full time course','NaN']\nds_jobs[\"enrolled_university\"]=pd.Categorical(ds_jobs[\"enrolled_university\"],ordered=True,categories=order)\norder=['&lt;1','1','2', '3',  '4','5','6', '7',  '8','9','10','11','12', '13',  '14','15','16', '17',  '18','19', '20','&gt;20','nan' ]\nds_jobs[\"experience\"].unique()\narray(['&gt;20', '15', '5', '&lt;1', '11', '13', '7', '17', '2', '16', '1', '4',\n       '10', '14', '18', '19', '12', '3', '6', '9', '8', '20', nan],\n      dtype=object)\norder=['&lt;1','1','2', '3',  '4','5','6', '7',  '8','9','10','11','12', '13',  '14','15','16', '17',  '18','19', '20','&gt;20','nan' ]\nds_jobs[\"experience\"]=pd.Categorical(ds_jobs[\"experience\"],ordered=True,categories=order)\nds_jobs.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19158 entries, 0 to 19157\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype   \n---  ------                  --------------  -----   \n 0   student_id              19158 non-null  int32   \n 1   city                    19158 non-null  category\n 2   city_development_index  19158 non-null  float16 \n 3   gender                  14650 non-null  category\n 4   relevant_experience     19158 non-null  category\n 5   enrolled_university     18772 non-null  category\n 6   education_level         18698 non-null  category\n 7   major_discipline        16345 non-null  category\n 8   experience              19093 non-null  category\n 9   company_size            13220 non-null  object  \n 10  company_type            13018 non-null  category\n 11  last_new_job            18735 non-null  object  \n 12  training_hours          19158 non-null  int32   \n 13  job_change              19158 non-null  int32   \ndtypes: category(8), float16(1), int32(3), object(2)\nmemory usage: 717.9+ KB\nds_jobs[\"company_size\"].unique()\narray([nan, '50-99', '&lt;10', '10000+', '5000-9999', '1000-4999', '10-49',\n       '100-499', '500-999'], dtype=object)\norder=['&lt;10','10-49','50-99', '100-499',  '500-999','1000-4999','5000-9999','10000+','nan']\nds_jobs[\"company_size\"]=pd.Categorical(ds_jobs[\"company_size\"],ordered=True,categories=order)\nds_jobs.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19158 entries, 0 to 19157\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype   \n---  ------                  --------------  -----   \n 0   student_id              19158 non-null  int32   \n 1   city                    19158 non-null  category\n 2   city_development_index  19158 non-null  float16 \n 3   gender                  14650 non-null  category\n 4   relevant_experience     19158 non-null  category\n 5   enrolled_university     18772 non-null  category\n 6   education_level         18698 non-null  category\n 7   major_discipline        16345 non-null  category\n 8   experience              19093 non-null  category\n 9   company_size            13220 non-null  category\n 10  company_type            13018 non-null  category\n 11  last_new_job            18735 non-null  object  \n 12  training_hours          19158 non-null  int32   \n 13  job_change              19158 non-null  int32   \ndtypes: category(9), float16(1), int32(3), object(1)\nmemory usage: 587.3+ KB\nds_jobs[\"last_new_job\"].unique()\narray(['1', '&gt;4', 'never', '4', '3', '2', nan], dtype=object)\norder=['never','1','2', '3',  '4','&gt;4','nan']\nds_jobs[\"last_new_job\"]=pd.Categorical(ds_jobs[\"last_new_job\"],ordered=True,categories=order)\nds_jobs.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19158 entries, 0 to 19157\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype   \n---  ------                  --------------  -----   \n 0   student_id              19158 non-null  int32   \n 1   city                    19158 non-null  category\n 2   city_development_index  19158 non-null  float16 \n 3   gender                  14650 non-null  category\n 4   relevant_experience     19158 non-null  category\n 5   enrolled_university     18772 non-null  category\n 6   education_level         18698 non-null  category\n 7   major_discipline        16345 non-null  category\n 8   experience              19093 non-null  category\n 9   company_size            13220 non-null  category\n 10  company_type            13018 non-null  category\n 11  last_new_job            18735 non-null  category\n 12  training_hours          19158 non-null  int32   \n 13  job_change              19158 non-null  int32   \ndtypes: category(10), float16(1), int32(3)\nmemory usage: 456.7 KB\nds_jobs['company_size'].cat.categories[5]\n'1000-4999'\nx=ds_jobs['experience']&gt;=ds_jobs['experience'].cat.categories[10]\ny = ds_jobs['company_size']&gt;=ds_jobs['company_size'].cat.categories[5]\nds_jobs_clean = ds_jobs[x & y]\nds_jobs_clean.head(10)\n\n\n\n\n\n\n\n\n\n\n\nstudent_id\n\n\n\n\ncity\n\n\n\n\ncity_development_index\n\n\n\n\ngender\n\n\n\n\nrelevant_experience\n\n\n\n\nenrolled_university\n\n\n\n\neducation_level\n\n\n\n\nmajor_discipline\n\n\n\n\nexperience\n\n\n\n\ncompany_size\n\n\n\n\ncompany_type\n\n\n\n\nlast_new_job\n\n\n\n\ntraining_hours\n\n\n\n\njob_change\n\n\n\n\n\n\n\n\n9\n\n\n\n\n699\n\n\n\n\ncity_103\n\n\n\n\n0.919922\n\n\n\n\nNaN\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n17\n\n\n\n\n10000+\n\n\n\n\nPvt Ltd\n\n\n\n\n&gt;4\n\n\n\n\n123\n\n\n\n\n0\n\n\n\n\n\n\n12\n\n\n\n\n25619\n\n\n\n\ncity_61\n\n\n\n\n0.913086\n\n\n\n\nMale\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n&gt;20\n\n\n\n\n1000-4999\n\n\n\n\nPvt Ltd\n\n\n\n\n3\n\n\n\n\n23\n\n\n\n\n0\n\n\n\n\n\n\n31\n\n\n\n\n22293\n\n\n\n\ncity_103\n\n\n\n\n0.919922\n\n\n\n\nMale\n\n\n\n\nHas relevant experience\n\n\n\n\nPart time course\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n19\n\n\n\n\n5000-9999\n\n\n\n\nPvt Ltd\n\n\n\n\n&gt;4\n\n\n\n\n141\n\n\n\n\n0\n\n\n\n\n\n\n34\n\n\n\n\n26494\n\n\n\n\ncity_16\n\n\n\n\n0.910156\n\n\n\n\nMale\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nBusiness Degree\n\n\n\n\n12\n\n\n\n\n5000-9999\n\n\n\n\nPvt Ltd\n\n\n\n\n3\n\n\n\n\n145\n\n\n\n\n0\n\n\n\n\n\n\n40\n\n\n\n\n2547\n\n\n\n\ncity_114\n\n\n\n\n0.925781\n\n\n\n\nFemale\n\n\n\n\nHas relevant experience\n\n\n\n\nFull time course\n\n\n\n\nMasters\n\n\n\n\nSTEM\n\n\n\n\n16\n\n\n\n\n1000-4999\n\n\n\n\nPublic Sector\n\n\n\n\n2\n\n\n\n\n14\n\n\n\n\n0\n\n\n\n\n\n\n47\n\n\n\n\n25987\n\n\n\n\ncity_103\n\n\n\n\n0.919922\n\n\n\n\nOther\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n19\n\n\n\n\n10000+\n\n\n\n\nPublic Sector\n\n\n\n\n4\n\n\n\n\n52\n\n\n\n\n1\n\n\n\n\n\n\n104\n\n\n\n\n1180\n\n\n\n\ncity_16\n\n\n\n\n0.910156\n\n\n\n\nMale\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n12\n\n\n\n\n5000-9999\n\n\n\n\nPvt Ltd\n\n\n\n\n1\n\n\n\n\n52\n\n\n\n\n0\n\n\n\n\n\n\n108\n\n\n\n\n25349\n\n\n\n\ncity_16\n\n\n\n\n0.910156\n\n\n\n\nMale\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n&gt;20\n\n\n\n\n1000-4999\n\n\n\n\nPvt Ltd\n\n\n\n\n&gt;4\n\n\n\n\n28\n\n\n\n\n0\n\n\n\n\n\n\n115\n\n\n\n\n20576\n\n\n\n\ncity_97\n\n\n\n\n0.924805\n\n\n\n\nMale\n\n\n\n\nHas relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nGraduate\n\n\n\n\nSTEM\n\n\n\n\n&gt;20\n\n\n\n\n1000-4999\n\n\n\n\nPvt Ltd\n\n\n\n\n&gt;4\n\n\n\n\n19\n\n\n\n\n0\n\n\n\n\n\n\n130\n\n\n\n\n3921\n\n\n\n\ncity_36\n\n\n\n\n0.893066\n\n\n\n\nNaN\n\n\n\n\nNo relevant experience\n\n\n\n\nno_enrollment\n\n\n\n\nPhd\n\n\n\n\nSTEM\n\n\n\n\n&gt;20\n\n\n\n\n1000-4999\n\n\n\n\nPublic Sector\n\n\n\n\n&gt;4\n\n\n\n\n4\n\n\n\n\n0\n\n\n\n\n\n\nds_jobs_clean.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 2201 entries, 9 to 19143\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype   \n---  ------                  --------------  -----   \n 0   student_id              2201 non-null   int32   \n 1   city                    2201 non-null   category\n 2   city_development_index  2201 non-null   float16 \n 3   gender                  1821 non-null   category\n 4   relevant_experience     2201 non-null   category\n 5   enrolled_university     2185 non-null   category\n 6   education_level         2184 non-null   category\n 7   major_discipline        2097 non-null   category\n 8   experience              2201 non-null   category\n 9   company_size            2201 non-null   category\n 10  company_type            2144 non-null   category\n 11  last_new_job            2184 non-null   category\n 12  training_hours          2201 non-null   int32   \n 13  job_change              2201 non-null   int32   \ndtypes: category(10), float16(1), int32(3)\nmemory usage: 76.3 KB",
    "crumbs": [
      "Project: Customer Analytics- Preparing Data for Modeling"
    ]
  },
  {
    "objectID": "Project-Analyzing_Crime_in_Los_Angeles.html",
    "href": "Project-Analyzing_Crime_in_Los_Angeles.html",
    "title": "Project: Analyzing Crime in Los Angeles",
    "section": "",
    "text": "The Data\nThey have provided you with a single dataset to use. A summary and preview are provided below.\nIt is a modified version of the original data, which is publicly available from Los Angeles Open Data.",
    "crumbs": [
      "Project: Analyzing Crime in Los Angeles"
    ]
  },
  {
    "objectID": "Project-Analyzing_Crime_in_Los_Angeles.html#task-1",
    "href": "Project-Analyzing_Crime_in_Los_Angeles.html#task-1",
    "title": "Project: Analyzing Crime in Los Angeles",
    "section": "Task 1",
    "text": "Task 1\nWhich hour has the highest frequency of crimes? Store as an integer variable called peak_crime_hour.\n# Start coding here\n# Use as many cells as you need\npeak_crime_hour=(crimes.value_counts(\"TIME OCC\").idxmax())\npeak_crime_hour=int(peak_crime_hour[:2])",
    "crumbs": [
      "Project: Analyzing Crime in Los Angeles"
    ]
  },
  {
    "objectID": "Project-Analyzing_Crime_in_Los_Angeles.html#task-2",
    "href": "Project-Analyzing_Crime_in_Los_Angeles.html#task-2",
    "title": "Project: Analyzing Crime in Los Angeles",
    "section": "Task 2",
    "text": "Task 2\nWhich area has the largest frequency of night crimes (crimes committed between 10pm and 3:59am)? Save as a string variable called peak_night_crime_location.\n# Convert TIME OCC as date time\ncrimes[\"TimeOcc_int\"] = crimes[\"TIME OCC\"].astype(\"int\")\ncrimes.head(5)\n\n\n\n\n\n\n\n\n\n\n\nDR_NO\n\n\n\n\nDate Rptd\n\n\n\n\nDATE OCC\n\n\n\n\nTIME OCC\n\n\n\n\nAREA NAME\n\n\n\n\nCrm Cd Desc\n\n\n\n\nVict Age\n\n\n\n\nVict Sex\n\n\n\n\nVict Descent\n\n\n\n\nWeapon Desc\n\n\n\n\nStatus Desc\n\n\n\n\nLOCATION\n\n\n\n\nTimeOcc_int\n\n\n\n\n\n\n\n\n0\n\n\n\n\n221412410\n\n\n\n\n2022-06-15\n\n\n\n\n2020-11-12\n\n\n\n\n1700\n\n\n\n\nPacific\n\n\n\n\nTHEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n13600 MARINA POINT DR\n\n\n\n\n1700\n\n\n\n\n\n\n1\n\n\n\n\n220314085\n\n\n\n\n2022-07-22\n\n\n\n\n2020-05-12\n\n\n\n\n1110\n\n\n\n\nSouthwest\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n27\n\n\n\n\nF\n\n\n\n\nB\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n2500 S SYCAMORE AV\n\n\n\n\n1110\n\n\n\n\n\n\n2\n\n\n\n\n222013040\n\n\n\n\n2022-08-06\n\n\n\n\n2020-06-04\n\n\n\n\n1620\n\n\n\n\nOlympic\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n60\n\n\n\n\nM\n\n\n\n\nH\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n3300 SAN MARINO ST\n\n\n\n\n1620\n\n\n\n\n\n\n3\n\n\n\n\n220614831\n\n\n\n\n2022-08-18\n\n\n\n\n2020-08-17\n\n\n\n\n1200\n\n\n\n\nHollywood\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n28\n\n\n\n\nM\n\n\n\n\nH\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n1900 TRANSIENT\n\n\n\n\n1200\n\n\n\n\n\n\n4\n\n\n\n\n231207725\n\n\n\n\n2023-02-27\n\n\n\n\n2020-01-27\n\n\n\n\n0635\n\n\n\n\n77th Street\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n37\n\n\n\n\nM\n\n\n\n\nH\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n6200 4TH AV\n\n\n\n\n635\n\n\n\n\n\n\ncrimes_10pm_to_4am = crimes[((crimes[\"TimeOcc_int\"]&gt;=2200) & (crimes[\"TimeOcc_int\"]&lt;=2400))|((crimes[\"TimeOcc_int\"]&gt;=0) & (crimes[\"TimeOcc_int\"]&lt;=400))]\ncrimes_10pm_to_4am.columns\nIndex(['DR_NO', 'Date Rptd', 'DATE OCC', 'TIME OCC', 'AREA NAME',\n       'Crm Cd Desc', 'Vict Age', 'Vict Sex', 'Vict Descent', 'Weapon Desc',\n       'Status Desc', 'LOCATION', 'TimeOcc_int'],\n      dtype='object')\npeak_night_crime_location = crimes_10pm_to_4am.value_counts(\"AREA NAME\").idxmax()\npeak_night_crime_location\n'Central'\ncrimes_10pm_to_4am.value_counts(\"AREA NAME\").max()\n4211",
    "crumbs": [
      "Project: Analyzing Crime in Los Angeles"
    ]
  },
  {
    "objectID": "Project-Analyzing_Crime_in_Los_Angeles.html#task-3",
    "href": "Project-Analyzing_Crime_in_Los_Angeles.html#task-3",
    "title": "Project: Analyzing Crime in Los Angeles",
    "section": "Task 3",
    "text": "Task 3\nIdentify the number of crimes committed against victims by age group (0-18, 18-25, 26-34, 35-44, 45-54, 55-64, 65+). Save as a pandas Series called victim_ages\n# Define the age bins\nbins = [0, 17, 25, 34, 44, 54, 64, float('inf')]\nlabels = ['0-18', '18-25', '26-34', '35-44', '45-54', '55-64', '65+']\n\n# Create a new column based on the age bins\ncrimes['Age Group'] = pd.cut(crimes['Vict Age'], bins=age_bins, labels=age_labels)\ncrimes.head(10)\n\n\n\n\n\n\n\n\n\n\n\nDR_NO\n\n\n\n\nDate Rptd\n\n\n\n\nDATE OCC\n\n\n\n\nTIME OCC\n\n\n\n\nAREA NAME\n\n\n\n\nCrm Cd Desc\n\n\n\n\nVict Age\n\n\n\n\nVict Sex\n\n\n\n\nVict Descent\n\n\n\n\nWeapon Desc\n\n\n\n\nStatus Desc\n\n\n\n\nLOCATION\n\n\n\n\nTimeOcc_int\n\n\n\n\nAge Group\n\n\n\n\n\n\n\n\n0\n\n\n\n\n221412410\n\n\n\n\n2022-06-15\n\n\n\n\n2020-11-12\n\n\n\n\n1700\n\n\n\n\nPacific\n\n\n\n\nTHEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)\n\n\n\n\n0\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n13600 MARINA POINT DR\n\n\n\n\n1700\n\n\n\n\nNaN\n\n\n\n\n\n\n1\n\n\n\n\n220314085\n\n\n\n\n2022-07-22\n\n\n\n\n2020-05-12\n\n\n\n\n1110\n\n\n\n\nSouthwest\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n27\n\n\n\n\nF\n\n\n\n\nB\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n2500 S SYCAMORE AV\n\n\n\n\n1110\n\n\n\n\n26-34\n\n\n\n\n\n\n2\n\n\n\n\n222013040\n\n\n\n\n2022-08-06\n\n\n\n\n2020-06-04\n\n\n\n\n1620\n\n\n\n\nOlympic\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n60\n\n\n\n\nM\n\n\n\n\nH\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n3300 SAN MARINO ST\n\n\n\n\n1620\n\n\n\n\n55-64\n\n\n\n\n\n\n3\n\n\n\n\n220614831\n\n\n\n\n2022-08-18\n\n\n\n\n2020-08-17\n\n\n\n\n1200\n\n\n\n\nHollywood\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n28\n\n\n\n\nM\n\n\n\n\nH\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n1900 TRANSIENT\n\n\n\n\n1200\n\n\n\n\n26-34\n\n\n\n\n\n\n4\n\n\n\n\n231207725\n\n\n\n\n2023-02-27\n\n\n\n\n2020-01-27\n\n\n\n\n0635\n\n\n\n\n77th Street\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n37\n\n\n\n\nM\n\n\n\n\nH\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n6200 4TH AV\n\n\n\n\n635\n\n\n\n\n35-44\n\n\n\n\n\n\n5\n\n\n\n\n220213256\n\n\n\n\n2022-07-14\n\n\n\n\n2020-07-14\n\n\n\n\n0900\n\n\n\n\nRampart\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n79\n\n\n\n\nM\n\n\n\n\nB\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n1200 W 7TH ST\n\n\n\n\n900\n\n\n\n\n65+\n\n\n\n\n\n\n6\n\n\n\n\n221216052\n\n\n\n\n2022-07-07\n\n\n\n\n2020-02-23\n\n\n\n\n1000\n\n\n\n\n77th Street\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n28\n\n\n\n\nF\n\n\n\n\nB\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n500 W 75TH ST\n\n\n\n\n1000\n\n\n\n\n26-34\n\n\n\n\n\n\n7\n\n\n\n\n221515929\n\n\n\n\n2022-10-10\n\n\n\n\n2020-04-01\n\n\n\n\n1200\n\n\n\n\nN Hollywood\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n33\n\n\n\n\nM\n\n\n\n\nW\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n5700 CARTWRIGHT AV\n\n\n\n\n1200\n\n\n\n\n26-34\n\n\n\n\n\n\n8\n\n\n\n\n231906599\n\n\n\n\n2023-03-03\n\n\n\n\n2020-01-14\n\n\n\n\n1335\n\n\n\n\nMission\n\n\n\n\nTHEFT OF IDENTITY\n\n\n\n\n35\n\n\n\n\nM\n\n\n\n\nO\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n14500 WILLOWGREEN LN\n\n\n\n\n1335\n\n\n\n\n35-44\n\n\n\n\n\n\n9\n\n\n\n\n231207476\n\n\n\n\n2023-02-27\n\n\n\n\n2020-08-15\n\n\n\n\n0001\n\n\n\n\n77th Street\n\n\n\n\nBURGLARY\n\n\n\n\n72\n\n\n\n\nM\n\n\n\n\nB\n\n\n\n\nNaN\n\n\n\n\nInvest Cont\n\n\n\n\n8800 HAAS AV\n\n\n\n\n1\n\n\n\n\n65+\n\n\n\n\n\n\nvictim_ages = crimes.value_counts(\"Age Group\")\nvictim_ages\nAge Group\n26-34    47470\n35-44    42157\n45-54    28353\n18-25    28291\n55-64    20169\n65+      14747\n0-18      4528\ndtype: int64",
    "crumbs": [
      "Project: Analyzing Crime in Los Angeles"
    ]
  }
]